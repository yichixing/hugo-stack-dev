[{"content":"题目： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 如果我修改了baseURL 的https://yichixing.github.io/hugo-Fixlt-dev/，那github pag估计就用不了，也就是github page 和 cloudflare page二选一： 好的，用户现在遇到了一个问题，他们想同时在GitHub Pages和Cloudflare Pages上部署Hugo博客，但修改baseURL后导致其中一个平台无法正常使用。用户可能希望两个平台都能正确显示，但目前只能二选一。我需要分析问题根源，并提供解决方案。 首先，用户提到修改baseURL为GitHub Pages的路径后，Cloudflare Pages出现问题。这是因为baseURL决定了生成静态文件中资源（如CSS、JS）的路径。如果baseURL设置为GitHub Pages的子路径，那么在Cloudflare Pages上，这些资源路径会指向错误的URL，导致样式加载失败。 用户的需求是让两个平台都能正常工作，但当前的配置只能支持一个。这可能是因为Hugo的baseURL是硬编码的，无法同时适应两个不同的部署路径。需要找到一种方法，让Hugo在不同环境下生成不同的baseURL，或者使用相对路径来避免绝对路径的问题。 接下来，我需要考虑可能的解决方案。例如，使用环境变量在构建时动态设置baseURL，或者在Cloudflare Pages中配置自定义域名，使其路径与GitHub Pages一致。另外，检查是否启用了相对URLs选项，这可以让资源使用相对路径，而不是绝对路径，从而适应不同的部署路径。 用户可能对Hugo的配置和持续集成/持续部署（CI/CD）流程不太熟悉，所以需要提供详细的步骤，比如如何在GitHub Actions中设置不同的环境变量，或者在Cloudflare Pages中配置构建命令。同时，需要确保用户理解每个步骤的作用，避免混淆。 还要考虑用户是否希望通过自定义域名来统一访问路径，这样无论部署到哪个平台，baseURL都可以指向同一个自定义域名，从而避免路径冲突。这可能需要配置DNS和SSL证书，但可以从根本上解决问题。 最后，总结解决方案，分步骤说明如何配置，确保用户能够逐步操作，并验证每个步骤的效果，确保两个平台都能正常显示。 方案 2：动态设置 baseURL（通过环境变量） 在构建时动态注入 baseURL，实现不同平台的差异化配置。 GitHub Pages 配置 toml 复制 1 baseURL = \u0026#34;https://yichixing.github.io/hugo-Fixlt-dev/\u0026#34; Cloudflare Pages 配置 在 Cloudflare Pages 的构建设置中添加环境变量 ： 变量名 ：HUGO_BASEURL 值 ：https://your-cloudflare-domain.com/ 修改 config.toml ： toml 复制 1 baseURL = env.HUGO_BASEURL # 优先使用环境变量 正确写法 toml 复制 1 baseURL = \u0026#34;env.HUGO_BASEURL\u0026#34; # ✅ 使用双引号包裹 工作流名称 name: deploy 触发工作流的事件 on: push: branches: # 仅在 main 分支被推送时触发 - main 定义工作流中的作业 jobs: 定义一个名为 deploy 的作业 deploy: # 指定作业运行的环境为最新的 Ubuntu 系统 runs-on: ubuntu-latest # 定义作业中的步骤 steps: # 步骤1：检出代码 - name: Checkout # 使用官方的 actions/checkout 动作来获取仓库代码 uses: actions/checkout@v4 with: # 获取完整的 Git 历史记录（对于部署工具 peaceiris/actions-gh-pages 可能需要） # 0 表示获取所有历史，而不是默认的浅克隆（只获取最后一次提交） fetch-depth: 0 # 步骤2：设置 Hugo 环境 - name: Setup Hugo # 使用社区维护的 peaceiris/actions-hugo 动作来安装 Hugo uses: peaceiris/actions-hugo@v3 with: # 指定要安装的 Hugo 版本为最新版 hugo-version: \u0026#39;latest\u0026#39; # 明确指定安装 Hugo 的 Extended 版本（支持 SASS/SCSS 等特性） extended: true # 步骤3：构建网站 - name: Build Web # 运行 Hugo 命令来生成静态网站文件 # -D 参数表示包含草稿内容（如果本地预览用过，这里保持一致，或按需移除） run: hugo -D # 步骤4：部署网站到 GitHub Pages - name: Deploy Web # 使用社区维护的 peaceiris/actions-gh-pages 动作来部署 uses: peaceiris/actions-gh-pages@v4 with: # 指定用于认证的 Personal Access Token (PAT) # ${{ secrets.TOKEN }} 会从仓库的 Secrets 中读取名为 TOKEN 的值 # （视频中占位符是 你的token变量名，这里假设你设置为 TOKEN） PERSONAL_TOKEN: ${{ secrets.TOKEN }} # 指定要将静态文件部署到的目标仓库 # 格式为：你的GitHub用户名/目标仓库名 # （视频中占位符是 你的github名/你的仓库名，这里假设目标仓库是 letere-gz/hugo-dev） EXTERNAL_REPOSITORY: yichixing/hugo-Fixlt-dev # 指定将静态文件推送到目标仓库的哪个分支（通常是 gh-pages 或 main） PUBLISH_BRANCH: main # 指定包含构建好的静态网站文件的目录（Hugo 默认生成到 public） PUBLISH_DIR: ./public # 自定义部署时 Git 提交的信息 commit_message: auto deploy env: HUGO_BASEURL: \u0026#34;https://yichixing.github.io/hugo-Fixlt-dev/\u0026#34; 这个加在哪里？ name: deploy on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 with: fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v3 with: hugo-version: \u0026#39;latest\u0026#39; extended: true # 关键修改：在 Build Web 步骤中添加环境变量 - name: Build Web run: hugo -D env: HUGO_BASEURL: \u0026#34;https://yichixing.github.io/hugo-Fixlt-dev/\u0026#34; # 添加到这里 - name: Deploy Web uses: peaceiris/actions-gh-pages@v4 with: PERSONAL_TOKEN: ${{ secrets.TOKEN }} EXTERNAL_REPOSITORY: yichixing/hugo-Fixlt-dev PUBLISH_BRANCH: main PUBLISH_DIR: ./public commit_message: auto deploy IGNORE_WHEN_COPYING_START content_copy download Use code with caution. IGNORE_WHEN_COPYING_END 上面是前因后果，下面是问题所在： main仓库使用action生成静态程序给dev仓库时，action不能修改环境变量啊，否则环境变量覆盖了dev仓库的baseURL，导致cloudflare page的环境变量用不了，需要dev仓库自己使用aciton才行，你给我个dev仓库的完整的action代码吧 这是main仓库的工作流 工作流名称 name: deploy 触发工作流的事件 on: push: branches: # 仅在 main 分支被推送时触发 - main 定义工作流中的作业 jobs: 定义一个名为 deploy 的作业 deploy: # 指定作业运行的环境为最新的 Ubuntu 系统 runs-on: ubuntu-latest # 定义作业中的步骤 steps: # 步骤1：检出代码 - name: Checkout # 使用官方的 actions/checkout 动作来获取仓库代码 uses: actions/checkout@v4 with: # 获取完整的 Git 历史记录（对于部署工具 peaceiris/actions-gh-pages 可能需要） # 0 表示获取所有历史，而不是默认的浅克隆（只获取最后一次提交） fetch-depth: 0 # 步骤2：设置 Hugo 环境 - name: Setup Hugo # 使用社区维护的 peaceiris/actions-hugo 动作来安装 Hugo uses: peaceiris/actions-hugo@v3 with: # 指定要安装的 Hugo 版本为最新版 hugo-version: \u0026#39;latest\u0026#39; # 明确指定安装 Hugo 的 Extended 版本（支持 SASS/SCSS 等特性） extended: true # 步骤3：构建网站 - name: Build Web # 运行 Hugo 命令来生成静态网站文件 # -D 参数表示包含草稿内容（如果本地预览用过，这里保持一致，或按需移除） run: hugo -D # 步骤4：部署网站到 GitHub Pages - name: Deploy Web # 使用社区维护的 peaceiris/actions-gh-pages 动作来部署 uses: peaceiris/actions-gh-pages@v4 with: # 指定用于认证的 Personal Access Token (PAT) # ${{ secrets.TOKEN }} 会从仓库的 Secrets 中读取名为 TOKEN 的值 # （视频中占位符是 你的token变量名，这里假设你设置为 TOKEN） PERSONAL_TOKEN: ${{ secrets.TOKEN }} # 指定要将静态文件部署到的目标仓库 # 格式为：你的GitHub用户名/目标仓库名 # （视频中占位符是 你的github名/你的仓库名，这里假设目标仓库是 letere-gz/hugo-dev） EXTERNAL_REPOSITORY: yichixing/hugo-Fixlt-dev # 指定将静态文件推送到目标仓库的哪个分支（通常是 gh-pages 或 main） PUBLISH_BRANCH: main # 指定包含构建好的静态网站文件的目录（Hugo 默认生成到 public） PUBLISH_DIR: ./public # 自定义部署时 Git 提交的信息 commit_message: auto deploy IGNORE_WHEN_COPYING_START content_copy download Use code with caution. IGNORE_WHEN_COPYING_END 你先回答我，你看懂了前因后果没，回答懂了还是不懂，不懂的我会补充；然后再解决我的疑问；另一个疑问是cloudflare page的环境变量的疑问，我怀疑cloudflare page连接了dev库后得到了完整的静态资源，此时cloudflare page的环境变量baseURL就无法作用在静态资源里面了，你觉得是还是不是，回答一下 ","date":"2025-04-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/ai%E9%80%BB%E8%BE%91%E6%B5%8B%E8%AF%95%E9%A2%98%E9%A2%98%E7%9B%AE%E6%9C%AC%E8%BA%AB%E5%8F%AA%E6%9C%89%E5%B0%91%E6%95%B0%E6%A8%A1%E5%9E%8B%E8%83%BD%E7%AD%94%E5%AF%B9/","title":"AI逻辑测试题题目本身（只有少数模型能答对） "},{"content":" \u0026laquo;\u0026lt;传输工具filepizza：只要在地址输入分享链接就可以传输，因此手机端也能用； https://github.com/kern/filepizza；在线版：https://file.pizza/\u0026gt;\n《♓《虽然只支持传输单个文件，但我可以打包啊，因此相当于无论传输什么都没限制；》 《♐《由于只需要拍个网址下来，所以使用便易程度因人而异，但比起那种软件流畅度低的localsend，我还是更接受用流畅的网页》 \u0026lt;C盘清理工具：Windows Cleaner ；https://github.com/darkmatter2048/WindowsCleaner\u0026gt;\n\u0026lt;视频字幕、水印清除工具：video-subtitle-remover；这个作者的另一款软件就是大名鼎鼎的硬字幕提取软件 ；https://github.com/YaoFANGUK/video-subtitle-remover\u0026gt;\n\u0026lt;AI提示词手册：system-prompts-and-models-of-ai-tools（）；https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools \u0026gt;\n\u0026lt;一站式模型系统；aigcpanel https://github.com/modstart-lib/aigcpanel\u003e\n\u0026lt;批量重命名工具：rename-photos ；https://github.com/Arman19941113/rename-photos\u0026gt;\n","date":"2025-04-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E5%B7%A5%E5%85%B7%E9%9B%86%E5%90%88/","title":"工具集合"},{"content":"题目： [[[[[[[模型测试]\n《《《编程能力 gemini2.5pro \u0026gt; qwen2.5max vs Claude-3.7-Sonnet》\n【测试题目：想同时在GitHub Pages和Cloudflare Pages上部署Hugo博客，因此动态设置 baseUR】 《《能行的方案：分离构建，各自为政；既然问题根源在于“一次构建，两个不同 baseURL 的目标”，最清晰的解决方案就是让两个平台各自独立构建；简单来说就是github page和cloudflare page单独分别用main仓库源码部署》 《该方案可行的原因在于【cloudflare page真的有执行hugo源码的能力 cloudflare page能独立构建hugo项目得到hugo静态资源】，因此两个平台各自独立构建是成立的，让cloudflare page独立构建hugo项目得到hugo静态资源即可，然后github page也独立用action构建hugo项目得到hugo静态资源》 \u0026lt;用该编程题一测就知道模型编程能力的优劣\u0026gt; \u0026laquo;\u0026lt;【除了gemini2.5pro以外 ，其它模型都认为应该创建dev仓库的action代码 而且这个action还在run hugo -D】，这种说法是错误的，dev仓库都是从main仓库转换来的静态资源，因此dev仓库哪里来的源码让你执行hugo命令啊，能说出在dev仓库的action程序中执行run hugo -D 的全都是代码分析能力不行的，我把action源码给到这些模型，这些模型早就该分析出dev仓库都是静态资源，不可能在dev仓库执行aciton程序了\u0026gt; 《《《另外有一个重要的认知就是【配置文件的参数的参数值仅在项目构建过程正被读取】，这意味着项目构建好后不会调用参数，比如 配置文件的baseURL必须在项目构建过程中把变量设置好，项目构建完成后再设置变量就没用了》 《编程能力和推理能力一般的模型》 《《《测试结论：gemini2.5pro是唯一能成功解决我问题的模型，因此gemini2.5pro成为我能免费用到的最强模型，不但可以帮助我用视频学习，还有免费使用的平台Trae，编程能力还是免费用到中的最强》 \u0026lt;本地开源模型比较 千问QWQ vsGemma 3、千问QWQ视觉推理能力测试\u0026gt; https://www.bilibili.com/video/BV1LcdaYZEN2/ https://aistudio.google.com/prompts/1wToupoSnKb31mh4-ZZ535NZNrK_7knrk\n[1 千问QWQ vsGemma 3] [1 推理能力肯定有思维链的千问QWQ强] \u0026lt;内置了思维链的QWQ在推理时性能是远高于没有推理能力的模型的，使用更长的时间和更长的token输出来获取更高的性能\u0026gt; [2 翻译能力] \u0026lt;在使用推理能力下，内置了思维链的QWQ在花费思考时间的前提下能稍微领先gemma3，但翻译也很看重速度，不用思维链时gemma3胜，所以综合来看，翻译还是gemma3强\u0026gt; [3 长文本总结能力] \u0026lt;思维链对长上下文的总结也是有一定帮助的，还是QWQ强\u0026gt; [4 角色扮演能力（文学能力）] \u0026lt;gemma3在角色扮演这块很强，模型能完全理解提示词里面设置好的角色，由于gemma3的指令跟随能力更强，所以角色扮演也很容易跟随我们的引导进行变化\u0026gt; \u0026lt;QWQ的强大推理能力导致了逻辑性太强，表达会倾向于书面化，也就是AI味道太浓\u0026gt; \u0026lt;推理模型的特殊性也导致QWQ无法再长周期指令下保质指令性能，在多轮角色扮演后会表现出一些非常不理智的行为\u0026gt; [5 编程能力] 《编程能力方面推普通模型不可能打得过推理模型，肯定是QWQ胜，当然时间消耗和上下文消耗只能自己取舍了，有思维链的QWQ在编程能力可是能解决R1的存在》 [6 指令跟随能力] [6.1 注意力测试] \u0026lt;指令跟随能力检验的是 模型在多轮对话中理解和持续遵循复杂指令的能力，可以理解成上下文记忆和处理以及理解能力，其中最重要的当然是模型对上下文的记忆能力，因为推理分析太占用注意力了》 《单次规则下，指令跟随能力方面 gemma3更强，推理模型无法在长周期指令下保质指令性能，因为推理模型注意力比不过普通模型，推理模型更容易因为随着对话次数的增加而导致注意力衰减，导致输出环节出现规则遗忘等“掉链子”情况》 《总结起来就是在使用推理模型的时候，一定要在提示词那里时刻提醒模型规则，否则推理模型把注意力都集中在了推理分析，容易遗忘规则的》 《《多轮对话弥补方案：每轮对话都提起规则，这种情况下就是QWQ更强，它只是注意力不集中在以前的规则，但只要规则在现在，推理模型就会把注意力分配到现在的规则，再凭借强大的推理能力去遵循规则》 [[[6.2 复杂规则的遵循能力] 《《其实指令跟随能力更多的是要求模型按我的规则输出，也就是提示词里面的“同时满足所有这些要求，并且这个分析本身也要遵循上述所有规则”，输入的规则越多越复杂就越是考验模型的指令跟随能力；》 《《复杂指令的遵循肯定推理模型强，推理模型只是多轮对话不行，因为多轮多话会导致模型注意力不集中；但单次输出的话，推理模型本身能把强逻辑和推理能力注意力集中在规则上，输出的格式肯定更准确\u0026gt; \u0026lt;比如同样的例子和格式，gemin2.0无论怎么提醒它格式，它都无法输出正确的格式，但gemini2.5pro凭借它的推理能力能做到更容易做到以复杂的规则输出\u0026gt; 《续，在指令跟随能力上 其实推理模型gemini2.5pro也不太行，对提示词又要求 比如给出了例子和格式还要我再把格式解释一遍，字体修改也是两次的提起，尤其是中文输出更是要重复提起 https://aistudio.google.com/prompts/17frt2TEoNuFHOBzK-lF4EDJmr-XYYCzD》 [2 推理模型vs基础语言模型] [1 结论] \u0026lt;推理模型和基础语音模型没有绝对的谁强谁弱，它们各有优劣，适用于不同的应用场景 \u0026gt; [2 推理模型虽然强大，但缺点不能视而不见] \u0026laquo;1 推理模型不适合需要快速交互的场景，因为优异的性能，然而这种优势也伴随着显著的代价，及每一次提问都需要更额外的思考过程，甚至可能长达五分钟以上\u0026gt; \u0026lt;2 对于算力有限的本地电脑而言，这种额外的运算负担会直接导致输出速度显著地下降，无疑是一个不容忽视的缺点\u0026gt; \u0026lt;3 注意力机制问题导致指令跟随能力不强，角色扮演、多轮对话的场景容易出错，需要时刻提醒规则\u0026gt; [3 gemma3的优点] \u0026lt;1 综合性能强，除了编程以外几乎什么都做\u0026gt; \u0026lt;2 性能表现则展现出另一种优势，无需冗长的思考过程，响应迅速，能够更快速地交付结果，提升了使用效率\u0026gt; \u0026lt;3 指令跟随能力更强，适合多轮多话或晚角色扮演\u0026gt; \u0026lt;4 文学能力更强，不像推理模型因为逻辑性太强导致输出的文学AI味太浓\u0026gt; [3 gemma3视觉推理能力测试] [1 OCR能力] \u0026lt;完美地将整个表格OCR， 带颜色的数据也能标注出来\u0026gt; [2 复杂图像分析（比如折线图）] \u0026lt;能对所有关键数据都进行提取，分析了表格的走向， 其对图表的分析还是非常到位的，分析的结论也是正确的\u0026gt; [3 故事情节和故事逻辑理解能力] \u0026lt;gemma3能理解多张图片的逻辑关系，能把多张图片拼接成一个连续的故事情节\u0026gt; \u0026lt;比如给几张猫和老鼠的图片，gemma3把这几张图片拼接成一个连续的故事情节：Tom打字 -\u0026gt; Jerry拔插头 -\u0026gt; Tom触电\u0026gt; [4 对复杂或抽象的构图的理解（比如视觉错位）] \u0026lt;gemma3始终不是推理模型，肯定无法处理视觉元素的扭曲、错位、抽象表达\u0026gt; \u0026laquo;模型需要更深入的推理（例如，判断这些扭曲的元素是什么、它们代表什么意义）才能理解复杂构图，同时有推理能力和推理能力的千问2.5max应该会做得更好\u0026raquo; \u0026laquo;\u0026lt;GPT-4.1测试\u0026gt; https://www.bilibili.com/video/BV14AdZYpEDk https://aistudio.google.com/prompts/1woqAsn54QZrpy8CoeyzMsiFfTMPg_u1B （获取测试题） https://aistudio.google.com/prompts/1woqAsn54QZrpy8CoeyzMsiFfTMPg_u1B\n[gpt4.1模型能力测试《《《可以要提示词 这些提示词用来测试模型的能力很好》] [1 幻觉识别能力] \u0026lt;对于虚构的人物、历史事件、地理位置识别较好，不易产生幻觉；但对于虚构的科学理论、技术、化学物质等容易产生幻觉，会一本正经地编造信息。\u0026gt; [2 指令遵循能力] \u0026lt;基本能遵循复杂的多重指令，比如用【测试模型是否能遵循指令的复杂指令测试的提示词】测试，输出结果成功实现 包括否定性约束（如禁用特定字母）和格式要求（如JSON），能力较之前版本有显著提升，但在细节处理上（如 答案标签 格式）可能存在小瑕疵。\u0026gt; https://aistudio.google.com/prompts/1KyZa9d9Vd3D4dItzp3wgkDRe6frLgT5C [3 逻辑推理能力] \u0026lt;在解决增加了复杂约束条件的农夫过河问题时失败，第一步即出错，表明其在复杂逻辑推理方面仍有不足。\u0026gt; [4 模式识别能力] \u0026lt;使用【找规律测试（序列推理测试）的提示词】中，gpt4.1未能识别基于字母的序列规律，但成功识别了两种不同的数字序列规律（交替模式和二进制模式）。\u0026gt; [5 编程能力] \u0026lt;编程能力强大，尤其在SwiftUI和前端（HTML/Tailwind CSS）方面表现出色，能生成复杂且有效的代码和界面；但在Three.js可视化方面生成的代码未能成功渲染；通过Cline插件也能完成较复杂的macOS应用开发（如水印应用），但可能需要错误修复环节。\u0026gt; [6 API调用成本] \u0026lt;API调用成本尚可，完成较复杂的macOS应用开发（如水印应用）消耗了约 $0.56 的 API 调用费用，对于这样一个相对复杂的项目来说，成本尚可接受\u0026gt; ","date":"2025-04-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95%E9%A2%98%E5%A4%A7%E5%85%A8ai%E8%B6%85%E5%85%83%E9%A2%86%E5%9F%9F%E7%89%88/","title":"模型测试题大全（AI超元领域版） "},{"content":"༼面包购买小技巧 4——法国和德国黑麦面包有什么区别 ༽\npart1: Gang Gang面包店初体验与黑麦面包 (00:00-00:09) 1.1: 介绍Gang Gang面包店\n今天去了一家叫 Gang Gang 的面包店。 感觉还挺好，所以大肆的进货，把店里所有值得买的东西都买了。 (00:09-00:21) 1.2: 展示黑麦面包与第一印象\n评价标准至少包括名字、质感、颜色做得比较好的。 进店时看到了这款黑麦面包。 看到这款面包时，当时就觉得稳了。 (00:21-00:32) 1.3: 黑麦面包外观与T系列面粉\n面包表面撒的面粉，烤焦后有特殊味道。 解释了T45 T55面粉的概念，指面粉烤焦后留下的灰分的含量。 (00:32-00:38) 1.4: 与宜家杰森面包对比\n这款面包是宜家那款杰森面包（像床头柜一样的面包）的亲戚，是其高配版本。 (00:38-00:48) 1.5: 黑麦面包内部组织特点\n这款面包不会有大气孔。 原因是其面粉的属性与平时做法棍的不同，即使长时间发酵也发不出大气泡。 (00:48-00:57) 1.6: 黑麦面包食用建议 (德国面包)\n建议这款面包最适合搭配牛窝骨、大肘子和酸菜。 指出德国人也是这么吃德国面包的。 part2: 法国面包对比与评价标准 (00:57-01:13) 2.1: 展示法国面包并与黑麦面包对比\n展示了另一款法国面包。 两款面包长得很像，但有区别。 名字不一样，内部组织不一样，面粉比例是不同的。 闻起来味道是差不多的，但口感上面区别会比较大。 (01:14-01:28) 2.2: 评价面包好坏的标准 (裂口与皮)\n提供了判断面包好坏的方法： 一看裂口情况（要炸开）。 二看皮，皮不能太厚。做得不好的大面包，皮烤的特别的厚，或者表面烤焦了。 (01:28-01:31) 2.3: 面包价格\n一条面包大概是 56块钱，视频中展示的是半条。 part3: 全麦吐司 (Graham面包) 介绍 (01:31-01:44) 3.1: 展示全麦吐司及其名称由来\n第三个点评的是全麦吐司，两天的保质期。 中文名叫全麦吐司，但包装上写了英文和日语叫 Graham。 Graham是个美国人，一位农学家。 (01:44-01:51) 3.2: Graham面包与早期全麦吐司\nGraham 曾推广的全麦面包。 早期的全麦吐司是在他的基础上做的。 (01:51-02:12) 3.3: Graham面包造型特点 (One Loaf)\nGraham面包 首先是全麦面包。 其造型特点是 one Loaf，与普通吐司卷成三个卷不同，它的圆圈（卷的纹路）应该在侧面。 (02:12-02:18) 3.4: Graham面包制作特点\n它是一次卷一个很大的卷，然后一次发酵起来，这是 Grandham这款面包的特点。 (02:18-02:25) 3.5: 评价全麦吐司的标准\n判断方法还是看皮的厚度，以及四周和中间的质地的不同。 (02:25-02:39) 3.6: 美式全麦吐司的口感特点\n这是一款很经典的美式全麦吐司。 口感不会像日本人做的那么细腻，就是这种粗糙的，不加添加剂就是这个样子。 (02:39-02:43) 3.7: 麦香味评价\n因为全麦含量不多，所以有麦香，但是跟刚才两款肯定没得比。 (02:43-03:05) 3.8: 切片厚度问题\n指出这款面包有个问题是切太薄了。 不知道切这么薄怎么吃，一般只有工厂的（如罗森、seven Eleven的三明治）才会切这么薄。 正常的切法是半条吐司切4片到6片，这款明显超过6片，已经8片了，是8切的吐司。 part4: 红豆包评价 (03:05-03:17) 4.1: 红豆包外观评价 (上色与膨胀)\n展示红豆包。 指出这条线，说明蛋液刷到这里，然后先烤到是这里。 底下偏白的部分是膨胀的地方。 (03:17-03:26) 4.2: 红豆包底部评价\n检查底下，没有明显的裂口。 解释底部褶皱了是因为运输时面包被压了，拿出来时是刚出炉的。 (03:26-03:37) 4.3: 综合评价标准\n顶部有黑芝麻。 检查底下是否很圆，这说明受力很均匀。 看里面红豆馅有多少。 看底下有没有死（面没发起来）。这些都是常见判断的方法。 (03:37-03:48) 4.4: 更多外观细节\n再次看底部的线，如果边上有一圈线，说明鸡蛋刷多了。 这款整体控制都很好。 (03:48-04:19) 4.5: 内部馅料与制作分析\n切开展示，馅料是满的。 分析：很多时候用低糖的红豆沙或者自制红豆馅时，如果红豆馅炒的不够干、糖给的不够多，会导致红豆馅会很重，把底下压死，或者上面的水蒸气会把面包顶起来（形成空洞）。 这款面包整体就比较好，红豆馅看起来像是自己煮的。 底部的还好，组织也是对的，颜色也是对的。 (04:19-04:32) 4.6: 水平评价\n这款红豆包的水平，在日本是属于中游。 比顶级店的话差一点，但是绝对比路边店要好，已经很高级了。 part5: 黄油卷评价 (04:32-04:46) 5.1: 展示黄油卷及问题分析\n展示之前做的小的面包（黄油卷）。 指出小面包底下它就不圆，有点像扇形。 (04:46-05:15) 5.2: 制作难点与日式做法\n解释原因：面包太小了，手用力整形会比较困难。 这是标准的日式黄油卷的做法。 指出虽然叫黄油卷，但和黄油的关系真的不是很大，黄油给的不多，应该说是很少，里面一般还会混一部分的起酥油。 这种面包是日本人最喜欢的早餐，多年未变。 part6: 可颂评价 (05:15-05:27) 6.1: 可颂味道评价\n展示可颂，评价味道对了。 这是标准的黄油可颂的味道，但吃不到发酵黄油的味道，只是黄油的味道。 (05:27-05:57) 6.2: 提升可颂风味的技巧\n揭示一些手艺不太行的店如何让可颂变得好吃的方法：在可颂出炉时趁热喷一点糖水。 散热的时候水分蒸发后，表面就会有层糖，吃起来甜甜的会比较好吃。 将此比作卖橙子的常见套路（用一瓶糖水让橙子更甜），称之为一点不值一提的小套路。 (05:57-06:01) 6.3: 可颂价格与评价\n这个可颂不打开就知道是重磅的产品。 价格是 20块。 part7: 杏仁可颂评价与横评 (06:01-06:16) 7.1: 展示杏仁可颂与制作分析\n做横评，展示一款杏仁可颂。 指出好的杏仁可颂，上面的杏仁酱要铺到你那个杏仁都看不到。 从底面看，这款的酱已经把这个整个的那个可颂都已经压扁了。 (06:16-06:33) 7.2: 杏仁可颂的日法对比\n这款里面还挤了杏仁酱，这是小日本做那个杏仁可颂的做法。 之前法国老师过来上课时都说日本人的杏仁可颂做的好。 原因是法国人没有那么大方，而这款杏仁酱给的多，日本人给的多，卖得贵一点。 (06:33-06:40) 7.3: 杏仁可颂的法式做法特点\n法国人的话就比较做的会比较像这个（指上一个普通可颂），杏仁片，杏仁酱它就会稀疏很多。 (06:40-06:47) 7.4: 杏仁可颂内部与价格评价\n切开展示，这款中间有薄薄的一层酱的。 这款卖 20块钱，觉得是值得的。 (06:47-07:03) 7.5: 横向对比与“不讲武德”\n主要原因是这敌人太狡猾了，旁边居然还有高手。 如果没有这款，那个杏仁可颂（指上一个）已经超过了几乎所有上海的杏仁可颂，就它的堆料程度。 但碰到了这款，里面还加了一层巧克力，评价这顶得住啊，看这个酱有多厚。 评价这款简直不讲武德。 (07:03-07:10) 7.6: 最终评价\n切开展示。 总结：这款其实做的不错，他卖20出头，我觉得也没有太大的问题。 自嘲：本来我觉得我已经够良心了，结果别人比我还卷的很。 (07:10-07:19) 7.7: 巧克力杏仁可颂\n再次强调这款中间还加了一层巧克力。 展示其酱有多厚。 (07:19-07:37) 7.8: 杏仁可颂的起源与现状\n解释杏仁可颂它本身存在的意义：有点像是头一天没有卖完的米饭，第二天变成了炒饭；头一天没卖完的可颂，就变成了杏仁可颂（或巧克力可颂，这款是巧克力杏仁可颂）。 但现在不是这样，因为杏仁可颂卖的太好了，爱吃的人太多了，所以很多店是专门多烤可颂来做杏仁可颂。 指出杏仁可颂必须是让可颂要放一天才能做，它跟那个炒饭是一样的，必须要把它放在外面，相当于把它放干了，可颂放干了以后再去刷糖水，再去放酱。 (07:37-08:16) 7.9: 制作细节差异\n如果你用刚烤出来的可颂做，首先它的质地不稳定，它太软了； 其次它的含水量太多，它中间的酱不能挤太多，不然的话它会有点稀。 (08:16-08:22) 7.10: 台湾面包店对比\n举例：在台湾的法国面包店里面，可颂基本上是没有人买的，但是卖的最好的产品就是杏仁可颂。 ","date":"2025-04-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E9%9D%A2%E5%8C%85%E8%B4%AD%E4%B9%B0%E5%B0%8F%E6%8A%80%E5%B7%A7-4%E6%B3%95%E5%9B%BD%E5%92%8C%E5%BE%B7%E5%9B%BD%E9%BB%91%E9%BA%A6%E9%9D%A2%E5%8C%85%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","title":"面包购买小技巧 4——法国和德国黑麦面包有什么区别"},{"content":"༼ MCP+智能体，开发AI版“你画我猜”效率翻倍༽\npart1: 自定义智能体 (MCP Git \u0026amp; Github 配置) (02:42-03:04) 1.1: 添加Git MCP Server\n介绍如何添加 Git 的 MCP Server。 作者展示了在 MCP 市场 中找到 Git 工具，并点击添加。 添加过程需要参考 介绍页面，将 JSON 配置 复制过来。作者展示了从 Github 上的 modelcontextprotocol/servers 仓库中找到 mcp-server-git 的配置，特别是 uvx 的配置方式。 关键操作：复制了使用 uvx 命令启动 mcp-server-git 的 JSON 配置。 作者建议：使用 uvx 之前需要在电脑上先安装 uv（一个 Python 版本和包管理工具），可以使用官网的一键安装命令进行安装。 画面内容 (JSON 配置)： 1 2 3 4 5 6 7 8 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;git\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;mcp-server-git\u0026#34;, \u0026#34;--repository-path\u0026#34;, \u0026#34;.\u0026#34;] } } } 作者将复制的 JSON 粘贴到 Trae 的 MCP Server 添加窗口中，并点击确认完成添加。现在 MCP Servers 列表中同时有了 git 和 GitHub。 (03:04-03:09) 1.2: 创建Git专家智能体\n目的：将已配置的 git 和 GitHub MCP Server 工具组合成一个专门处理 Git 相关任务的智能体。 关键操作： 点击“智能体” -\u0026gt; “+ 创建智能体”。 将智能体命名为 “Git 专家”。 在“工具” -\u0026gt; “工具 - MCP”部分，勾选上刚刚配置好的 git 和 GitHub 两个工具。 编写提示词，定义该智能体的角色：你是一个Git专家，擅长使用MCP Server操作Git与Github。 点击“创建”完成智能体配置。 part2: Git专家智能体测试 (初始化与提交) (03:10-03:21) 2.1: 使用Git专家初始化项目\n目的：测试新创建的 “Git 专家” 智能体的 Git 初始化能力。 关键操作： 新建一个对话窗口。 使用 @ 符号调用“Git 专家”智能体。 ♏输入指令：把这个文件夹初始化成一个git工程，名字用文件夹名字即可。 AI 开始执行任务，依次调用了 git MCP 工具执行了 git init 命令，并创建了 .gitignore 文件，然后执行了 git add .gitignore 和 git commit -m \u0026quot;初始初始化仓库：添加.gitignore文件\u0026quot; 命令。 用户一路点击确认（或设置为自动执行），AI 成功完成了初始化。 (03:22-03:37) 2.2: 使用Git专家提交到Github\n目的：测试 “Git 专家” 智能体与 Github 的交互能力，将本地仓库推送到远程。 关键操作： ♏输入指令：把这个仓库，提交到Github上面。 AI 开始执行任务，调用 GitHub MCP 工具创建了一个新的 Github 仓库 (tech-shrimp/trae_test)。 接着调用 git MCP 工具执行了 git remote add origin ... 和 git push -u origin main 命令，将本地仓库与远程仓库关联并推送代码。 结果：操作成功完成，代码被推送到新创建的 Github 仓库。作者切换到浏览器验证，确认 Github 上已成功创建 trae_test 仓库。 part3: Git专家智能体测试 (分支合并与冲突解决) (03:37-03:48) 3.1: 模拟分支冲突\n目的：创建一个更复杂的场景来测试 “Git 专家” 智能体处理分支合并冲突的能力。 关键操作： 作者在 Github 仓库 trae_test 中手动操作： 创建了一个 colors.txt 文件。 创建了两个分支：feature 和 main。 ♋- 在 feature 分支的 colors.txt 文件末尾添加了一行 \u0026ldquo;pink\u0026rdquo;。 ♋- 在 main 分支的 colors.txt 文件末尾添加了一行 \u0026ldquo;black\u0026rdquo;。 结果：这样就在两个分支的同一个文件 (colors.txt) 的同一位置（末尾）产生了修改冲突。 作者观点：这是开发过程中经常遇到的问题。 (03:48-04:12) 3.2: 指示Git专家合并分支并解决冲突\n目的：让 “Git 专家” 智能体自动合并 feature 分支到 main 分支，并解决期间产生的冲突。 关键操作： ♏输入指令：把远程的feature分支合并到main分支，并且帮我解决冲突。 AI 开始执行任务： 调用 git MCP 工具执行 git status 检查状态，然后 git fetch origin 获取远程更新。 尝试执行 git merge origin/feature 进行合并，此时检测到冲突。 AI 分析冲突，决定保留两个分支的修改（即同时保留 \u0026ldquo;pink\u0026rdquo; 和 \u0026ldquo;black\u0026rdquo;）。 AI 修改 colors.txt 文件以解决冲突，然后执行 git add colors.txt 和 git commit -m \u0026quot;Merge feature branch: 合并 colors.txt 中的颜色列表\u0026quot;。 最后执行 git push 将解决冲突后的结果推送到远程 main 分支。 结果：colors.txt 文件现在包含 6 种颜色，AI 流畅丝滑地完成了合并和冲突解决。 part4: 规则 (Rule) 功能介绍与项目规则配置 (04:13-04:29) 4.1: 规则功能介绍\n规则 (Rule) 是 Trae 的另一个重磅更新。 它的作用类似于系统级的提示词 (System Prompt)。 可以告诉 AI 当前项目的具体是做什么的、使用的技术框架以及开发者的开发习惯等。 目的：让 AI 更懂项目，也更懂你。 (04:29-04:36) 4.2: 项目设定：AI你画我猜\n本次演示项目：使用 Trae 完成一个 “AI 你画我猜” 的小游戏。 核心特点：使用 AI (OpenAI Vision API) 而不是真人作为裁判来猜测玩家画的内容。 准备工作：作者删除了之前测试用的 colors.txt 文件。 (04:36-04:55) 4.3: 配置项目规则\n操作路径：点击右上角用户头像 -\u0026gt; AI 功能管理 -\u0026gt; 规则。 规则类型：分为个人规则（全局生效）和项目规则（仅对当前项目生效）。 关键操作：作者选择添加项目规则，点击 “+ 创建 project_rules.md” 按钮。 结果：在项目根目录下创建了一个名为 .trae/rules/project_rules.md 的文件。 (04:44-04:55) 4.4: 编写项目规则内容\n目的：在 project_rules.md 文件中定义项目的基本信息和技术选型，供 AI 参考。\n关键操作：使用 Markdown 格式编写规则内容。\n♏画面内容 (技术栈选型)：\n1 2 3 4 5 6 7 8 9 10 11 12 # AI 你画我猜网页游戏 ## 项目概述 创建一个在线你画我猜游戏，玩家可以在画布上作画，AI 系统负责猜测画的是什么内容。 ## 技术栈选型 - 脚手架：**Vite** - 前端：**React + TypeScript** - 画布：**HTML5 Canvas** - 后端：**Node.js + Express** - AI 集成：**OpenAI Vision API** -《“你画我猜”的应用，可能只需要调用 Vision API 这一个功能，这个功能的作用就是 直接通过 HTTP 请求与 OpenAI API 交互》 这些信息将帮助 AI 更好地理解项目需求并生成符合技术栈的代码。\npart5: 使用规则拆解任务与AI执行 (前端部分) (04:55-05:10) 5.1: 指示AI根据规则拆解任务\n目的：利用配置好的项目规则 (Rule)，让 AI 自动将项目需求拆解成可执行的任务清单。 关键操作： 切换回 Trae 的聊天界面。 调用 @Builder 智能体。 ♋输入指令：请根据 #rules 里面的项目介绍，拆解任务，任务可以跟踪状态，把所有任务写到 task.md 文件。 #rules 指的是引用刚才创建的 project_rules.md 文件。 #task.md 指的是让 AI 将拆解结果输出到 task.md 文件。 (05:08-05:10) 5.2: AI生成任务清单\n结果：AI 根据 project_rules.md 的内容，成功生成了一个包含详细步骤和状态追踪标记的任务清单 (task.md 文件)。 (05:10-05:18) 5.3: 简化任务清单\n原因：为了演示方便，AI 生成的原始任务清单可能过于复杂。 关键操作：作者手动编辑 task.md 文件，删掉了一些复杂的功能，只保留了实现核心功能所需的基础任务。 (05:18-05:27) 5.4: 指示AI进行逐项开发\n目的：让 AI 根据简化后的任务清单，开始逐步执行开发任务。 关键操作： 再次调用 @Builder 智能体。 输入指令：根据 #task.md 进行逐项开发，并且标记状态。 AI 开始分析 task.md 文件并准备执行第一个任务。 (05:27-05:43) 5.5: AI执行前端任务 (环境搭建与画布开发)\n执行过程： AI 首先执行环境搭建任务：调用系统命令执行 npm create vite@latest . --template react-ts 来创建 React + TypeScript 项目。 接着执行 npm install 安装依赖。 然后开始执行画布功能开发任务，创建和修改相关组件文件 (DrawingCanvas.tsx, DrawingCanvas.css 等)。 用户交互：AI 每生成一段代码或执行一个命令，都会展示给用户，用户只需点击“接受”（或“全部接受”）即可。 作者评价：因为有任务列表的存在，AI 干活非常的有条理，用户操作非常简单。 (05:46-05:59) 5.6: AI执行前端任务 (用户界面开发与测试)\n执行过程： AI 继续根据 task.md 执行后续的前端任务，包括用户界面布局 (App.tsx, App.css)、结果显示区域 (ResultDisplay.tsx)、单元测试等。 AI 逐步创建、修改代码，并更新 task.md 中对应任务的状态为已完成。 结果：一次调用 @Builder 智能体，AI 就直接创建了 26 个文件，前端相关的工作已经完全完成。 part6: AI执行后端任务 (06:01-06:08) 6.1: 指示AI进行后端开发\n目的：让 AI 继续执行任务清单中剩余的后端开发任务。 关键操作： 新建一个会话。 调用 @Builder 智能体。 输入指令：请根据 #task.md 这些未完成的任务，继续完成后端的开发。 (06:08-06:13) 6.2: AI执行后端任务 (环境搭建与API开发)\n执行过程： AI 开始执行后端任务： 环境搭建：创建 server 目录，生成 package.json, tsconfig.json 等配置文件，安装 Node.js 和 Express 相关依赖 (npm install express cors dotenv @types/node @types/express @types/cors ts-node nodemon typescript)。 API 开发：创建 Express 服务器入口文件 (index.ts)，配置路由 (routes/drawing.ts)，实现 /api/drawing/recognize 接口用于接收图片数据并调用 OpenAI Vision API 进行图像识别。 创建 .env 文件模板用于存放 OpenAI API Key。 (06:13-06:16) 6.3: AI完成所有任务\n结果：AI 成功执行了所有后端开发任务，并将 task.md 中所有任务的状态标记为已完成（绿色对勾）。AI 的开发工作至此完成。 (06:16-06:27) 6.4: 修复后端代码小问题\n问题：在 drawing.ts 文件中，AI 错误地导入了 openai 的 npm 包，但项目规则指定的是直接调用 OpenAI Vision API。 关键操作： 作者发现了这个小错误。 输入指令：修改 drawing.ts 文件，优先使用 fetch 调用 openai，不要引入 openai 的包。 AI 理解指令，修改了 drawing.ts 中的代码，将原本导入 openai 包的方式改为了使用 Node.js 内置的 fetch 直接调用 OpenAI Vision API 端点 (https://api.openai.com/v1/chat/completions)。 结果：代码被成功修正，消除了不必要的依赖。 (06:25-06:27) 6.5: 配置环境变量\n关键操作：作者手动编辑 .env 文件，填入自己的 OpenAI API Key，并配置了 CORS 允许的来源（前端地址 http://localhost:5173）。 (06:27-06:41) 6.6: 启动前后端服务\n目的：运行刚刚由 AI 开发完成的前后端代码。 关键操作： 启动后端：在 Trae 的终端中，cd ./server 进入后端目录，然后执行 npm run dev 启动 Node.js (Express) 服务。后端运行在 http://localhost:3000。 启动前端：新建一个终端，在项目根目录执行 npm run dev 启动 Vite 开发服务器。前端运行在 http://localhost:5173。 part7: 智能体 Debug (前端UI问题) (06:42-06:48) 7.1: 发现前端UI问题\n测试场景：在浏览器中打开前端页面 http://localhost:5173。 问题描述：页面上成功显示了画板 (Canvas)，可以作画，也有“清空画布”按钮，但是缺少了用于触发 AI 猜测的按钮。 (06:48-07:01) 7.2: 使用截图和多模态能力请求修复\n目的：利用 Trae 的多模态能力和 Claude 3.5 模型，通过截图让 AI 理解并修复 UI 问题。 关键操作： 对当前缺少按钮的前端页面进行截图。 回到 Trae 的聊天界面，将截图粘贴进去。 选择 Claude 3.5 Sonnet 模型（因为它支持多模态）。 调用 @Builder 智能体，并引用 #Workspace (整个项目代码)。 输入指令：没有让 AI 猜测的按钮，帮我修复一下。 (07:00-07:12) 7.3: AI修复前端UI\n执行过程： AI (Claude 3.5) 分析了截图和代码 (#Workspace)，理解了问题所在。 AI 识别出需要在 DrawingCanvas.tsx 组件中添加一个按钮，并为其绑定触发 AI 猜测的事件处理函数。 AI 生成了修改后的代码，在画板下方添加了一个 “AI 猜测” 按钮。 用户交互：用户检查 AI 生成的代码修改，确认无误后点击接受。 结果：前端页面刷新后，“AI 猜测”按钮成功出现。 part8: 智能体 Debug (后端API与浏览器工具) (07:02-07:09) 8.1: 发现后端API调用错误\n测试场景：在添加了按钮的前端页面上，画一个图形（例如字母 A），然后点击 “AI 猜测” 按钮。 问题描述：点击按钮后，页面提示 “抱歉，AI 猜测失败，请重试”。 ♋Debug 操作：打开浏览器的开发者工具 (F12)，切换到控制台 (Console)。 错误信息：控制台显示了两个关键错误： POST http://localhost:5174/api/guess 404 (Not Found)：表明前端尝试请求的 API 地址错误（端口号 5174 不对，且路径 /api/guess 可能与后端不符）。 AI猜测出错: Error: AI猜测请求失败：这是前端代码中捕获到的网络请求错误。 (07:07-07:16) 8.2: 手动Debug的局限性\n传统方式：需要手动复制控制台的错误信息，然后粘贴到 Trae 中，让 AI 分析并修复。 Trae 的优势：通过 MCP (Model Context Protocol) 功能，提供了更进阶、更自动化的 Debug 方式。 (07:16-08:13) 8.3: ♏配置Browser Tools MCP Server\n目的：设置一个 MCP Server，让 Trae 能够直接读取浏览器的控制台信息、网络请求等状态，实现更智能的 Debug。 关键步骤： 安装浏览器插件： 在 Trae 的 MCP 市场中搜索 browser，找到 Browser Tools。 点击其介绍页面中的链接，下载 Chrome 扩展 (BrowserToolsMCP Chrome Extension) 的压缩包。 解压压缩包。 ♏打开浏览器的扩展管理页面 (edge://extensions/ 或 chrome://extensions/)。 启用开发者模式。 将解压后的插件文件夹拖拽到扩展管理页面进行安装。安装成功后会显示 BrowserTools MCP 插件。 配置 Trae 中的 MCP Server： 在 Trae 的 MCP 市场中添加 Browser Tools。 参考其介绍页面，复制 npx 启动命令的配置 JSON。 修改 JSON：将示例的 example-server 名称改为 browser-tools，将 mcp-server-example 参数改为 @agentdeskai/browser-tools-mcp@latest。 点击确认保存配置。 启动 MCP Server 进程： 打开一个新的命令行窗口（不是 Trae 内置终端）。 运行 Browser Tools 介绍页面提供的第三步命令：npx @agentdeskai/browser-tools-server@latest。 保持此命令行窗口运行。 结果：成功配置并启动了 Browser Tools MCP Server，打通了 Trae 与浏览器之间的通信。 (08:13-08:19) 8.4: 创建浏览器专家智能体\n目的：创建一个专门利用 Browser Tools MCP 进行 Debug 的智能体。 关键操作： 点击“智能体” -\u0026gt; “+ 创建智能体”。 命名为 “浏览器专家”。 在“工具 - MCP”中只勾选 browser-tools。 点击“创建”。 (08:19-08:42) 8.5: 使用浏览器专家Debug\n准备工作：重新打开要 Debug 的浏览器页面 (localhost:5174)。 连接确认：打开开发者工具 (F12)，页面顶部会出现提示条：\u0026ldquo;BrowserTools MCP\u0026rdquo; 已开始调试此浏览器。 触发错误：在页面上画图并点击 “AI 猜测” 按钮，再次触发之前的 404 错误。 关键操作： 回到 Trae。 调用新创建的 @浏览器专家 智能体。 ♏输入指令：浏览器控制台有哪些报错。 (08:35-08:42) 8.6: AI分析错误并定位问题\n执行过程： “浏览器专家” 智能体调用了其绑定的 browser-tools MCP 工具中的 getConsoleErrors 函数。 MCP Server 从浏览器获取了控制台的错误日志，并返回给 Trae 中的 AI。 AI 分析了返回的错误信息（包含详细的错误类型、消息、堆栈等）。 结果：AI 成功获取并理解了浏览器端的报错信息，准备进行下一步分析。 (08:41-08:55) 8.7: 指示AI修复API路径问题\n目的：让 AI 结合浏览器报错信息和项目代码，定位并修复 API 路径不匹配的问题。 AI 分析： AI 检查了浏览器报错信息，确认是 /api/guess 接口返回 404。 ♏AI 检查了后端代码 (server/src/routes/drawing.ts 和 server/src/index.ts)，发现后端实际提供的接口路径是 /api/drawing/recognize。 AI 检查了前端代码 (src/App.tsx 和 src/components/Canvas/index.tsx)，确认前端调用的是 /api/guess。 结论：AI 准确地指出了前后端 API 调用的路径不匹配是导致 404 错误的原因。 关键操作： 调用 @Builder 智能体（需要它来修改代码）。 输入指令（结合浏览器专家的分析结果）：因为前端API调用的是 /api/guess，但服务器配置的路由是 /api/drawing/recognize。帮我修复问题。 AI 修复：AI 修改了前端代码 (src/App.tsx 或相关调用处)，将 API 请求路径从 /api/guess 更正为 /api/drawing/recognize。 part9: 最终测试与效果展示 (09:08-09:10) 9.1: 再次测试 - 依然报错\n测试场景：修复 API 路径后，再次在前端页面画图并点击 “AI 猜测”。 结果：仍然报错，控制台可能显示新的错误（视频中未明确显示具体新错误，但暗示问题未完全解决）。 (09:10-09:13) 9.2: 结合控制台报错再次请求修复\n目的：让 AI 结合最新的控制台错误信息，进行进一步的 Debug 和修复。 关键操作： 调用 @浏览器专家 智能体。 输入指令：结合控制台报错修复问题。 AI 分析： 浏览器专家再次通过 browser-tools 获取控制台错误。 AI 分析后发现，这次的问题是前端在调用 fetch 时，没有指定后端的端口号，导致请求仍然发向了前端的 5173 端口，而非后端的 3000 端口。 (09:13-09:16) 9.3: AI最终修复\n关键操作： AI 识别到需要修改前端发送请求时的 URL。 ♏AI 修改了 src/components/Canvas/guessDrawing.tsx (或其他实际发送请求的文件) 中的 fetch 调用，将 URL 明确指定为后端的地址和端口：http://localhost:3000/api/drawing/recognize。 用户交互：用户接受 AI 的代码修改。 (09:16-09:20) 9.4: 成功测试 - 画A\n测试场景：进行最终测试，在画板上画一个字母 \u0026ldquo;A\u0026rdquo;。 结果：点击 “AI 猜测” 后，请求成功，AI 返回结果：“这张图片是一个手绘的字母 \u0026lsquo;A\u0026rsquo;，也就是英文字母表中的第一个字母。” (09:20-09:25) 9.5: 成功测试 - 画蜜蜂 (AI误认)\n测试场景：画一个蝴蝶的简笔画。 结果：AI 返回结果：“这是一张手绘的蜜蜂简笔画。你可以看到它有四只翅膀、分节的身体，还有两根触角。” 作者评论：AI 把蝴蝶认成了蜜蜂。 (09:25-09:31) 9.6: 成功测试 - 画房子\n测试场景：画一个带烟囱的房子的简笔画。 结果：AI 返回结果：“这是一幅简笔画，画的是一间房子。可以看到有屋顶、门和窗户，还有一个像是烟囱的结构。整体是一个简单的立体房屋造型。” (09:31-09:38) 9.7: 成功测试 - 画大象\n测试场景：画一个大象的简笔画。 结果：AI 返回结果：“这是一幅简笔画，看起来像是一只大象。可以看到大象的长鼻子、耳朵、四条腿和尾巴。画面风格简洁，但特征比较明显。” 作者总结：整个过程还是挺有意思的。 ","date":"2025-04-27T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/mcp-%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%80%E5%8F%91ai%E7%89%88%E4%BD%A0%E7%94%BB%E6%88%91%E7%8C%9C%E6%95%88%E7%8E%87%E7%BF%BB%E5%80%8D/","title":"MCP+智能体，开发AI版“你画我猜”效率翻倍"},{"content":"༼没钱做推广的冷门AI好用吗？AI工具大合集！【Git源宝】 ༽\npart1: AI工具合集推荐导览 (00:00-00:07) 1.1: 思维导图介绍\n视频开头展示了一个思维导图，介绍其囊括了许多没有足够预算做推广但很好用却少有人发掘的小众AI。 (00:07-00:15) 1.2: 整理与分类\n作者表示花费了一个多月整理这些好玩的小众AI。 将AI工具按照学习类、职场类、自媒体类三个大类进行了分类，并制作成思维导图供大家查阅使用。 (00:15-00:25) 1.3: 思维导图价值与声明\n展示了思维导图的部分内容，包含学生、新媒体、职场等分类下的具体AI工具名称。 这个导图旨在帮助用户在特定领域找到合适的AI，避免盲目寻找（一头雾水）。 作者声明本期视频全程无广告，观众可以放心收藏和点赞。 part2: jpgHD - 人工智能老照片无损修复 (00:25-00:35) 2.1: ♏jpgHD 功能介绍与演示\n介绍 jpgHD，一个修复AI。 适用场景：当制作视频搜集的素材比较模糊时。 核心功能：可以免费修复和画质增强。 演示：将一张模糊的初音未来毛绒玩具图片修复并增强为清晰的二次元画风图片。 评价：对于修复电子包浆的照片非常好用。 (00:35-00:43) 2.2: jpgHD 更多功能展示\n展示了 jpgHD 的效果演示界面。 除了超分辨率外，还支持： 给黑白照片上色（示例：林肯照片）。 让模糊的二次元图片变得更加清晰（示例：戴帽子的奥黛丽·赫本与马、另一个二次元女孩）。 修复破损照片、老照片（示例：女孩和男孩的老照片对比）。 魔法动态照片。 part3: ElevenLabs - AI配音与声音克隆 (00:43-00:54) 3.1: ♏ElevenLabs 功能与应用场景\n介绍 ElevenLabs，一个配音AI。 适用场景： 不方便给视频录音时。 想拥有女孩的声音时。 核心功能： ♈克隆甜美的声音 (Voice Cloning)。 ♈将文本直接转换成语音 (Text to Speech)。 界面展示：显示了 Text to Speech 界面，左侧有 Voices (声音)、Playground (游乐场)、Voice Lab (声音实验室)、Voice Changer (变声器) 等选项。 评价：效果比剪辑软件自带的要好很多。 part4: Suno - AI 作曲 (00:54-01:07) 4.1: Suno 功能与效果试听\n介绍 Suno，一个作曲AI。 适用场景：当找不到合适的视频BGM时，可以用它来制作。 界面展示：显示了 Suno 的网站界面，包含 Trending (趋势)、Hot in April (四月热门) 等板块。 效果试听：播放了一段由 Suno 生成的英文歌曲片段。 (01:07-01:24) 4.2: Suno 使用方法与免费额度\n强调最重要的优势：每天都有几次免费的使用额度。 使用方法： 在 Lyrics (歌词) 区域输入好歌词。 选择 Styles (风格)。 点击生成 (Create) 即可。 演示：输入了中文古诗词作为歌词，选择了 hiphop 风格，并播放了生成的中文说唱片段。 part5: Viggle AI - 视频人物动作替换 (01:24-01:37) 5.1:♏ Viggle AI 功能与演示\n介绍 Viggle AI，一个可以用来整活 (制作创意/搞笑内容) 的AI。 演示：展示了一个由两个跳舞的人组成的抽象视频。 核心功能：可以用它免费来制作这类视频。 界面展示：显示了 Viggle AI 的操作界面。 操作流程（多人场景）： 选择 Multi 功能。 左上角上传动作视频 (Motion)。 右上角上传你的照片 (Character)。 点击 Generate。 评价：上手简单，值得推荐。 part6: 白日梦 - AI短剧生成 (01:37-01:51) 6.1: 白日梦 功能与制作流程\n介绍 白日梦 (Bairimeng)，一个可以用来做 AI短视频 的AI。 制作流程： 写故事：首先可以根据你的设定写一个故事。 选形象：然后确定好主角。 生成图片：根据分镜生成每一个场景的图片。 视频编辑：AI 会将图片组合成一部抽象短片。 附加功能：可以配上旁白和对话。 评价：非常方便。 界面展示：显示了其包含“写故事”、“选形象”、“生成图片”、“视频编辑”四个步骤的界面，并展示了生成的图片序列和旁白输入框。 (01:51-02:00) 6.2: 白日梦 生成效果展示\n展示了一段由白日梦生成的动画短片效果。 内容为宫崎骏风格的动画，一个女孩和一个猪形态的角色在对话。 part7: Whisper - AI字幕识别 (02:00-02:12) 7.1: Whisper 功能与应用\n介绍 Whisper，一个识别字幕的AI (自动语音识别 ASR 系统)。 主要用途：给视频配上字幕。 对比：平时使用剪映，但其自动识别字幕功能需要会员。 推荐理由：Whisper 是一个免费 (白嫖) 的选择。 开源信息：已经在GitHub开源了。 评价：十分推荐。 界面展示：显示了 Whisper 的介绍页面和 GitHub 仓库页面。 part8: 朱雀大模型 - AI内容检测 (02:12-02:28) 8.1: 朱雀大模型 功能与使用\n介绍 朱雀大模型 的在线 AI检测器。 功能：可以检测文本中的 AI率 (疑似AI生成内容占比)。 作用：帮助识别文章的AI生成率。 适用场景： 当拿不准自己写的文章 AI 味重不重时。 老师可以用它来检测学生的作业是不是AI生成的。 界面演示：展示了粘贴文本、点击检测后，显示 100% 疑似AI 的结果。 语言支持：中文英文都可以检测。 part9: AI Short - 提示词AI (02:28-02:43) 9.1: AI Short 功能与特点\n介绍 AI Short，一个提示词AI (AI Short 效率革命)。 特点：其提供的提示词非常离谱。 示例1 (学术)：可以通过提示词将文章润色成Nature风格，方便成为学术大牛。 示例2 (娱乐)：甚至提供一些魅魔扮演的提示词。 适用人群：很适合学生党在学习之余玩乐使用。 界面展示：显示了网站 (aishort.top) 及其分类筛选和提示词列表。 part10: 沉浸式翻译 - 网页翻译AI插件 (02:43-03:03) 10.1: 网页翻译痛点与沉浸式翻译介绍\n介绍 网页翻译AI，特指沉浸式翻译 (Immersive Translate) 这个小插件。 适用场景：看一些英文论坛来学习英语。 痛点：浏览器自带的机翻不仅经常翻译错误，还会覆盖掉原来的原始英文。 推荐：沉浸式翻译插件。 优点： 小巧性能好。 能形成对照双语翻译。 评价：非常方便我们去看一些英文网站。 界面演示：展示了在 Reddit 和 Wikipedia 页面上使用沉浸式翻译实现双语对照的效果。 part11: Trae - 免费AI IDE (03:03-03:22) 11.1: Trae 功能与优势\n介绍 Trae，一个免费的AI IDE。 对比：相比 VS Code 里的 AI 插件，它的体验会更加的整体一些。 适用场景：如果专业里会涉及到编程，在遇到很难的题时，可以用它来辅助写代码。 集成模型：里面也接入了 Deepseek 和 豆包 的模型。 扩展性：也能通过 API 添加其他的模型。 评价：十分建议收藏。 界面演示：展示了 Trae 的安装配置过程（导入 VS Code 配置、安装命令行）和 IDE 主界面、模型设置（可添加 API）。 part12: 稿定AI - 设计类AI (03:22-03:37) 12.1: 稿定AI 功能与免费使用注意\n介绍 稿定AI，一个设计类AI。 功能：可以制作 LOGO、科普攻略、商品图、海报等。 特定应用：制作在某红薯 (小红书) 上常见的显眼的大字封面。 免费使用注意：生成结果中只有不带皇冠的模板才可以免费白嫖，需要避开带皇冠的。 界面演示：展示了稿定AI的模板库，特别是大字封面模板，并指出了皇冠标识。 part13: iSlide - AI PPT制作 (03:37-03:47) 13.1: iSlide 功能与应用\n介绍 iSlide，一个能白嫖制作PPT的AI。 集成方式：作为 PowerPoint 的插件 (iSlide AI 助手)。 适用场景：当工作中需要制作PPT的时候。 特点：里面内置了很多模板。 评价：还是挺方便应急的。 界面演示：展示了在 PowerPoint 中通过 iSlide AI 助手输入主题，AI 自动生成 PPT 大纲和内容的过程。 part14: 通义法睿 - 法律AI助手 (03:47-03:59) 14.1: 通义法睿 功能与应用场景\n介绍 通义法睿，一个法律AI。 适用场景：当在职场中受到法律纠纷，比如恶意裁员等情况。 作用：可以使用这个 AI法律助手 来查询相关的法条，保护自己的权益。 界面演示：展示了通义法睿的问答界面，并输入了“面对被公司解雇的情况，我有哪些法律权利和赔偿可以争取？”的问题，AI 正在生成回答。 part15: 总结与后续 (03:59-04:16) 15.1: 视频总结与资源获取\n以上就是本期视频介绍的所有小众AI。 为了方便大家访问，视频中提到的东西都放在了老地方，大家可以自行免费获取。 呼吁观众点赞、收藏、加关注。 作者自我介绍：源宝，一台AI和黑科技的挖掘机。 结束语：我们下期见。 ","date":"2025-04-11T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E5%9C%A8%E7%BA%BFai%E5%B7%A5%E5%85%B7%E5%90%88%E9%9B%86%E6%B2%A1%E9%92%B1%E5%81%9A%E6%8E%A8%E5%B9%BF%E7%9A%84%E5%86%B7%E9%97%A8ai%E5%A5%BD%E7%94%A8%E5%90%97ai%E5%B7%A5%E5%85%B7%E5%A4%A7%E5%90%88%E9%9B%86/","title":"在线AI工具合集（没钱做推广的冷门AI好用吗？AI工具大合集）"},{"content":" https://www.youtube.com/watch?v=A52BrmvZvtU\u0026t=141s：PotPlayer+DeepSeek自动识别翻译字幕\nhttps://www.youtube.com/watch?v=4906t5zygAE\u0026t=161s：sub-check linux版，本地我已有sub_check_win_GUI，而该linux版的意义是部署在外网的其它设备，而且是docker或docker compose方式部署程序\n《这里也提供了一个比较值得参考的GUI转WebUI的思路，就是单纯地将GUI界面转WebUI界面而已，我相信AI能做得到的》 ","date":"2025-04-11T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E8%A7%86%E9%A2%91%E6%87%92%E5%BE%97%E6%88%AA%E5%9B%BE%E4%BA%86/","title":"视频懒得截图了"},{"content":" 《《《编程能力 qwen3比gemini2.5pro强的》 测试题目：多平台的baseURL如何解决 gemini2.5pro能用的方案是不同平台独立部署 qwen3比gemini2.5pro给出了更多的方案，比如main仓库的aciton只管推送，由dev仓库的action独立部署等 ","date":"2025-03-29T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%BC%96%E7%A8%8B%E8%83%BD%E5%8A%9B-gemini2.5pro-vs-qwen3%E5%8D%83%E9%97%AE3%E5%BC%BA%E6%9E%9C%E7%84%B6%E8%B6%8A%E5%90%8E%E9%9D%A2%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%B6%8A%E5%BC%BA/","title":"编程能力 gemini2.5pro vs  qwen3，千问3强，果然越后面的模型越强 "},{"content":" 《《《编程能力 gemini2.5pro \u0026gt; qwen2.5max vs Claude-3.7-Sonnet》 【测试题目：想同时在GitHub Pages和Cloudflare Pages上部署Hugo博客，因此动态设置 baseURL】 《《能行的方案：分离构建，各自为政；既然问题根源在于“一次构建，两个不同 baseURL 的目标”，最清晰的解决方案就是让两个平台各自独立构建；简单来说就是github page和cloudflare page单独分别用main仓库源码部署》 《该方案可行的原因在于【cloudflare page真的有执行hugo源码的能力 cloudflare page能独立构建hugo项目得到hugo静态资源】，因此两个平台各自独立构建是成立的，让cloudflare page独立构建hugo项目得到hugo静态资源即可，然后github page也独立用action构建hugo项目得到hugo静态资源》 \u0026lt;用该编程题一测就知道模型编程能力的优劣\u0026gt; \u0026laquo;\u0026lt;【除了gemini2.5pro以外 ，其它模型都认为应该创建dev仓库的action代码 而且这个action还在run hugo -D】，这种说法是错误的，dev仓库都是从main仓库转换来的静态资源，因此dev仓库哪里来的源码让你执行hugo命令啊，能说出在dev仓库的action程序中执行run hugo -D 的全都是代码分析能力不行的，我把action源码给到这些模型，这些模型早就该分析出dev仓库都是静态资源，不可能在dev仓库执行aciton程序了\u0026gt; 《《《另外有一个重要的认知就是【配置文件的参数的参数值仅在项目构建过程正被读取】，这意味着项目构建好后不会调用参数，比如 配置文件的baseURL必须在项目构建过程中把变量设置好，项目构建完成后再设置变量就没用了》 《编程能力和推理能力一般的模型只会给你dev仓库的action代码，明明dev仓库的任何action的没意义》 《《《测试结论：gemini2.5pro是唯一能成功解决我问题的模型，因此gemini2.5pro成为我能免费用到的最强模型，不但可以帮助我用视频学习，还有免费使用的平台Trae或无限邮箱cursor，编程能力还是免费用到中的最强》 《♒《逻辑推理是gemini2.5pro比claude强，gemini2.5pro逻辑推理能力很强 文件索引能力在有明确提示词要求的情况下也非常强，所以单纯的编程的使用一定是gemini2.5pro\u0026gt;claude》 《♒《《因此gemini2.5pro适合开发 前后端 代码，而claude适合开发样式（UI）；♒claude用来开发UI的效果实在太好，本来我要求加个高光，claude还帮我加了个动画过渡效果》 《图片识别能力也是gemini2.5pro强，如果有图片想让AI按图片要求来，也是用gemini2.5pro；claude我把分格图片给它看，它居然看成了是卡片；♒而gemini2.5pro是真的能照着图片来写代码》 ","date":"2025-03-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%BC%96%E7%A8%8B%E8%83%BD%E5%8A%9B%E5%AF%B9%E6%AF%94gemini2.5pro-vsqwen2.5max%E5%92%8Cclaude-3.7-sonnet/","title":"编程能力对比gemini2.5pro vsqwen2.5max和Claude-3.7-Sonnet "},{"content":"༼全流程项目构建到部署_穷鬼套餐版 ༽ ༺https://aistudio.google.com/prompts/111ysUKgf9_PR3y4WpkPTJiuEsCHn5DcG ༻\npart1: 独立开发者穷鬼套餐介绍 (0:00-0:06) 1.1: 论坛热门话题：独立开发者穷鬼套餐 视频开头提到，在技术论坛中经常看到一个有趣的话题，即**\u0026ldquo;独立开发者省钱之穷鬼套餐\u0026rdquo;**。\n(0:06-0:13) 1.2: 穷鬼套餐覆盖范围 这些免费工具涵盖了开发、代码托管、前后端部署、数据库以及用户行为分析等一系列功能。\n(0:13-0:17) 1.3: 适用人群 这种套餐尤其适合学生党、独立开发者以及初创公司。\n(0:17-0:24) 1.4: 零成本启动与迭代 开发者可以利用这些免费工具，零成本做出产品并发布上线，等到开始盈利以后再慢慢迭代扩容。\n(0:24-0:34) 1.5: 本期视频目标 本期视频准备用一个小项目（贪吃蛇游戏）把这些免费的工具全部串联起来，展示如何不花一分钱就能上线一个可以赚钱的业务应用。\npart2: 技术选型 (0:34-0:40) 2.1: 后端部署成本最低方案：Node.js 作者认为，对于后端，部署成本最低的方案是 Node.js。\n(0:40-0:51) 2.2: Node.js 优势：Serverless Function 原因是 ♋Node.js 可以很轻松地改造成 Serverless Function（无服务器函数）。现在很多平台提供免费的云函数部署服务，♐例如 Cloudflare Worker、Vercel 和 Netlify 等。\n(0:51-0:56) 2.3: 前端选型：影响不大 对于前端选型，作者认为其对于部署成本几乎没有影响，选择自己喜欢的就好。\n(0:56-1:04) 2.4: 前端推荐：React 与 React Native 推荐使用 React，因为后续还可以使用 React Native 改造成跨端的手机应用。\n(1:04-1:10) 2.5: ♈框架选择：Next.js ♋结合后端 Node.js 和前端 React，很自然会想到 Next.js，这是一个在海外十分流行的开发框架。\n(1:10-1:18) 2.6: Next.js 优势：SEO 友好 Next.js 具有 SEO 友好的特点，使得网站更容易被搜索引擎收录。它结合了客户端（Client）渲染和服务器端（Server）渲染（SSR/Static HTML）的能力。\n(1:18-1:24) 2.7: 本期项目选择与 AI 辅助 本期视频决定使用 Next.js 来搭建工程。由于作者对 Next.js 不太熟悉，计划将主要的开发任务交给 AI 来完成。\npart3: AI开发工具与项目初始化 (1:24-1:35) 3.1: 推荐免费AI编程方案 (VSCode + Cline + OpenRouter) 推荐一个完全免费的 AI 编程方案：在 VS Code 上使用 Cline 插件，并接入 OpenRouter 的免费 API，以达到非常好的 AI 编程效果。\n(1:35-1:44) 3.2: 安装VSCode 首先访问 code.visualstudio.com，下载并安装 VS Code。安装过程十分简单，一路点击下一步即可。\n(1:44-1:47) 3.3: VSCode 基础使用参考 作者提到之前有一个完整视频介绍 VS Code 的使用。\n(1:47-1:56) 3.4: 安装与配置Cline插件 进入 VS Code 后，在扩展市场搜索 Cline 插件，点击 Install 进行安装。\n(1:56-2:05) 3.5: 获取 OpenRouter API Key 打开 Cline 插件界面，选择 \u0026ldquo;Use your own API key\u0026rdquo;，API Provider 选择 OpenRouter，然后点击 \u0026ldquo;Get OpenRouter API Key\u0026rdquo; 按钮。\n(2:05-2:10) 3.6: OpenRouter授权 跳转到 OpenRouter 网站进行授权（该网站在国内可以直连），如果没有账号需要先注册。点击 Authorize。\n(2:10-2:23) 3.7: 完成Cline模型选择 (DeepSeek) 回到 VS Code，点击 Open 允许插件打开 URI。这样 OpenRouter API key 就配置好了。在 Model 选择处，搜索并选择免费的 DeepSeek free 模型（deepseek/deepseek-chat-v3-0324-free）。AI 编程软件配置完成。\n(2:23-2:30) 3.8: 安装Node.js ♈访问 Node.js 官网 (nodejs.org)，下载并安装 Node.js（LTS 版本）。安装过程同样是一路点击下一步。\n(2:30-2:54) 3.9: ♈ 创建Next.js项目 (Snake) ♓安装完 Node.js 后，♓重启 VS Code。打开一个新的终端（Terminal），切换到目标项目文件夹（例如 D:\\Project2025）。♋输入命令 npx create-next-app 并按回车。\n项目命名： 输入 snake（贪吃蛇）。 配置选项： 对于 TypeScript, ESLint, Tailwind CSS, src/ directory, App Router, import alias 等选项，♋全部一路回车使用默认配置。 (2:54-3:06) 3.10: 启动初始Next.js项目 项目创建成功后，使用 VS Code 打开新创建的 snake 目录，点击信任。♋在终端中输入 npm run dev 启动项目。♋浏览器访问 http://localhost:3000，可以看到一个空白的 Next.js 工程已成功运行。\n(3:07-3:18) 3.11: 使用Cline生成贪吃蛇游戏页面 接下来，准备使用AI工具（Cline）将Next.js的初始页面改造。\n目标： 将页面改造成一个贪吃蛇的网页小游戏。 操作： 在Cline中输入改造需求的prompt。（♋⌊把这个项目⌋改造成一个含吃蛇的网页版小游戏。） 结果： AI（Cline）成功生成了贪吃蛇游戏的代码，并在浏览器中演示了基本的游戏界面。 part4: 代码管理 (3:18-3:24) 4.1: 个人开发者代码管理最佳选择：GitHub 对于个人开发者来说，代码管理的最佳选择是 GitHub。\n(3:24-3:38) 4.2: GitHub 功能与优势 GitHub 是全球最大的代码仓库托管与协作平台，可以在其中存储、分享并与他人一起编写代码。它支持创建私有（private）仓库，只有自己和被授权的协作者可以访问，不用担心源代码泄露。\n(3:38-3:50) 4.3: 解决GitHub访问慢的问题 如果访问 GitHub 速度很慢，可以在搜索栏搜索\u0026quot;商店\u0026quot;，来到微软的应用商店（Microsoft Store），下载 Watt Toolkit 这个工具。\n(3:50-3:53) 4.4: GitHub 账号注册 如果没有 GitHub 账号，需要先注册一个。\n(3:53-4:07) 4.5: 安装 Git 为了将 Next.js 工程上传到 GitHub，需要先安装 Git。在 VS Code 的源代码管理（Source Control）面板，点击 \u0026ldquo;Download Git for Windows\u0026rdquo; 按钮，跳转到下载页面。下载 64 位的安装包并安装，安装过程同样是一路点击下一步。\n(4:07-4:18) 4.6: 配置 Git 并发布到 GitHub 安装完 Git 后，在 VS Code 终端中配置 Git 用户名和邮箱： git config --global user.name \u0026quot;your_username\u0026quot; git config --global user.email \u0026quot;your_email@example.com\u0026quot; 然后，在 VS Code 的源代码管理面板♋点击 \u0026ldquo;Publish Branch\u0026rdquo; 按钮。选择 \u0026ldquo;Publish to GitHub private repository\u0026rdquo;♋⌊将代码发⌋ 布为私有仓库。\n(4:18-4:23) 4.7: 确认代码备份 现在，项目代码已经提交备份到 GitHub 上面了，拥有了一个远程的 Git 仓库。\npart5: 数据库 (4:23-4:33) 5.1: 免费数据库服务推荐：Neon 与 Supabase 对于免费的数据库服务，这里推荐两个：Neon 和 Supabase。它们都提供 500 MB 的免费存储空间。本视频选择使用 Neon。\n(4:33-4:46) 5.2: 创建 Neon 数据库 来到 Neon 的官网 (neon.tech)，♋点击 \u0026ldquo;Start for free\u0026rdquo;。选择使用 GitHub 登录。\n数据库配置： Neon 是基于 PostgreSQL 的免费数据库。进入后，选择 PostgreSQL 版本（如 17），给 Project name 起个名字（如 tech-shrimp-db）。 结果： 瞬间就拥有了自己的免费数据库。 (4:46-4:50) 5.3: 查看数据库连接信息 在 Neon 的 ♈Project Dashboard，♋点击右上角的 Connect 按钮，可以查看数据库连接的相关信息（如连接字符串、用户名、密码、主机地址、数据库名）。\npart6: 数据库工具与集成 (4:50-4:57) 6.1: 数据库管理工具推荐：DBeaver 推荐使用开源免费的数据库管理工具 DBeaver。先来到其官网 (dbeaver.io)，下载 DBeaver Community 版本。\n(4:57-5:20) 6.2: ♈连接 DBeaver 到 Neon 数据库 打开 DBeaver 软件，新建数据库连接。\n选择数据库类型： 选择 PostgreSQL。 填写连接信息： 回到 Neon 的♎connect连接信息界面，将对应的主机（Host）地址、数据库（Database）名、用户（User）名、密码（Password） 复制并粘贴到 DBeaver 的相应字段中。 测试连接： 点击 \u0026ldquo;测试连接\u0026rdquo;。DBeaver 会自动下载 PostgreSQL 驱动。 结果： 连接成功。点击完成。 (5:20-5:35) 6.3: 在 DBeaver 中创建数据表 在 DBeaver 左侧导航栏可以打开数据库。♋接下来准备创建一个表，用来保存玩家分数，做一个分数排行榜。\n操作： 在 public 模式下的**表（Tables）**上右键，选择 \u0026ldquo;新建 表\u0026rdquo;。 表结构： 表名： player_score 列： 添加两列：player_name (类型 varchar) 和 score (类型 int)。 保存： 按 Ctrl+S 保存，点击执行（Persist）。 (5:35-5:58) 6.4: ♎使用 AI (Cline) 生成数据库操作代码 在 DBeaver 中右键点击刚创建的♈ player_score 表，选择 \u0026ldquo;生成 SQL\u0026rdquo; -\u0026gt; DDL，♐复制 CREATE TABLE 语句。\nPrompt 构建： ♐回到 VS Code 的 Cline，粘贴 CREATE TABLE 语句，并提出需求： ♐请把玩家的数据保存到 PostgreSQL 数据库里面，以下是建表语句。 ♋游戏开始前弄一个输入框，让玩家自己填名字。 (5:58-6:10) 6.5: 向 AI 提供数据库连接信息 ♈⌊AI（Cline） 询问 ⌋PostgreSQL 数据库的连接信息。回到 Neon 复制完整的连接字符串（Connection string）。 ♈它的主要目的是获取这些信息，以便生成正确配置的、能够连接数据库的应用程序代码 ♋因为它需要这些信息来生成正确的应用程序代码\n(6:10-6:16) 6.6: AI 生成代码并测试 将连接字符串粘贴给 AI（Cline）。AI 修改了代码。点击 Save 保存更改。在终端运行 npm run dev 启动项目进行测试。\n(6:16-6:41) 6.7: 发现并分析错误 启动时出现构建错误：Module not found: Can't resolve 'dns'。\n定位原因： 查看 AI 写的代码，发现 page.tsx（前端页面）直接 import 并调用了 lib/db.ts 中的 saveScore 方法。而 saveScore 方法内部使用了 pg 包进行数据库连接和操作 (await pool.connect(), client.query())，并读取了环境变量 process.env.DATABASE_URL。 问题分析： Next.js 默认会将 page.tsx 作为前端组件处理。在前端（浏览器）环境中直接调用包含后端数据库操作（需要 pg 包）和敏感信息（数据库连接字符串）的代码是错误的，并且存在严重的安全隐患，会导致敏感信息暴露。 (6:41-6:56) 6.8: 向 AI 反馈错误并寻求解决方案 与 AI（Cline） 交流，指出保存数据库的逻辑似乎不正常，询问♒⌊保存数据库这块的逻辑正常么？这是后台操作还是前端操作？⌋，♒AI肯定没有意识到自己的错误，让 AI 检查其生成的代码，特别是 db.ts 和 page.tsx。\n(6:56-7:04) 6.9: AI 提出正确解决方案 (API Route) AI（Cline） 识别到错误，并提出了正确的解决方案：\n创建 API 路由： 新建一个 API 端点（例如 /api/saveScore）。 前后端分离： 前端页面通过发送 POST 请求到这个 API 端点来触发后端的数据库保存操作。 (7:04-7:19) 7.1: 更简洁的 Next.js 解决方案 (use server) 作者指出，其实还有一种更简单的 Next.js 改法：\n在 lib/db.ts 文件的顶部加上一行 'use server'; 指令。 作用： 这会告诉 Next.js，这个文件里的函数是服务器后端方法。♋Next.js 会自动处理，♐生成相应的 API 端点，♋将前后端的逻辑分割开，无需手动创建 API 路由。 (7:19-7:51) 7.2: 对 AI 编程能力的思考 这个例子体现了作者对 AI 编程的看法：\nAI 的角色： 现阶段的 AI 水平相当于一个知识面广的初级程序员。 人类的角色： 人类依旧是高级程序员、项目架构师，需要负责系统架构设计、理解编程原理（如♈前后端交互、保护安全信息等系统架构的知识）。 协作方式： 不能完全依赖 AI，人类需要审查和指导 AI 的工作，将更多精力投入到架构设计和业务需求理解上。作为人类，掌握系统架构、编程原理等知识仍然是必学必会的。 part7: 部署 (7:51-8:00) 8.1: Next.js 项目最佳部署平台：Vercel 作为一个 Next.js 项目，最方便的部署方式就是部署到 Vercel 里面，♋因为 Vercel 目前是 Next.js 的母公司。\n(8:00-8:08) 8.2: 开始 Vercel 部署流程 来到 Vercel 官网 (vercel.com)，♋点击 \u0026ldquo;Start Deploying\u0026rdquo;。选择 Continue with GitHub 登录。♈选择之前创建的 snake GitHub 仓库，点击 Import。\n(8:08-8:18) 8.3: 配置环境变量并部署 项目设置基本保持默认。♐唯一需要修改的是在 Environment Variables（环境变量） 中添加数据库连接信息。\nKey: DATABASE_URL Value: 粘贴从 Neon 获取的连接字符串。 点击 Deploy 开始部署。 (8:18-8:32) 8.4: 部署完成与访问 等待大约 1 分钟左右，项目就部署完成了。点击 \u0026ldquo;Continue to Dashboard\u0026rdquo;。Vercel 会分配一个默认域名（例如 snake-bice-three.vercel.app）。\n测试： 访问该域名，功能与本地一致，可以正常游戏并保存分数到 Neon 数据库。 ♋注意： Vercel 的默认域名♋只能在国外的上网环境访问到。 (8:32-8:45) 8.5: Vercel 免费额度限制 在♋ Vercel 的 Dashboard -\u0026gt; Usage 页面，♈可以看到免费用户的用量限制。作者认为 Edge Requests（每月 100 万次）可能比较瓶颈。如果升级到 Pro 模式，这些用量限制可以提高 10 倍。\npart8: 域名 (8:45-8:55) 9.1: 域名的重要性与选择 域名作为一个产品的排面，不建议用免费的，因为不稳定且非常折腾。\n(8:55-9:05) 9.2: 推荐廉价域名注册商：NameSilo 可以来到 NameSilo (namesilo.com) 这个地址，以最低成本获得一个域名。\n示例： 搜索一个6 位数字，发现 6位数字.xyz 的域名只需要 0.99 美元一年。 (9:05-9:18) 9.3: 使用 Cloudflare 托管域名 有了域名以后，可以把它♐托管到 Cloudflare 上面。作者提到，关于如何把域名托管到 Cloudflare，在之前的视频里有过详细介绍，这里不再赘述。\n(9:18-9:34) 9.4: Vercel 绑定自定义域名 回到 Vercel 的项目♍设置（Settings） -\u0026gt; Domains 页面，点击 Add。\n输入域名： 输入自己购买并托管到 Cloudflare 的域名（或子域名），♋例如 snake.tech-shrimp.com。 自动配置： 点击 Add 后，♋选择 Configure Automatically。Vercel 会自动通过 Cloudflare API 配置 DNS 记录。 (9:34-9:40) 9.5: 测试自定义域名 配置完成后，页面上会显示两个蓝色的对勾，表示配置完成。使用这个自定义域名（snake.tech-shrimp.com）同样可以访问到刚才部署的网站，并且♉这个域名在国内是可以直连的。\npart9: 用户行为分析 (9:40-9:51) 10.1: 免费用户行为监控工具：Microsoft Clarity 推荐使用 Microsoft Clarity (clarity.microsoft.com)，这是微软推出的免费用户行为监控工具。它提供了热点地图（Heatmaps）、♈用户行为录像（Session Recordings）、**报表（Dashboard）**等一系列功能。\n(9:51-10:02) 10.2: 集成 Clarity 到 Next.js 项目 登录 Clarity（可以使用 Google 账号）。♋在 Clarity 项目设置中，♉选择手动安装（Manual install），♈获取跟踪代码（Tracking Code）。\n操作： 回到 VS Code，打开 ♋⌊app/layout.tsx 文件⌋。在 \u0026lt;body\u0026gt; 标签内，定义一个 \u0026lt;Script\u0026gt; 组件（从 next/script 导入），。将 Clarity 的跟踪代码粘贴到 \u0026lt;Script\u0026gt; 组件内部。 (10:02-10:10) 10.3: 验证 Clarity 集成 保存 layout.tsx 文件。在本地重新启动项目 (npm run dev) 并进行一些操作。♈回到 Clarity 网站，在录制（Recordings）页面，♐可以看到作为用户的操作已经被录制下来了。\n(10:10-10:17) 10.4: Clarity 其他功能说明 Clarity 的仪表板（Dashboard）和热点地图（Heatmaps）需要过 24 小时才能呈现数据。至此，♉已将网站轻松接入了 Clarity。\npart10: 邮件 (10:17-10:39) 11.1: 免费邮件收发方案 作者之前有一期视频介绍如何使用 Cloudflare 获取无限多个免费邮箱。\n收邮件： 主要使用 **Cloudflare 的电子邮箱路由（Email Routing）**功能。 ♉发邮件「白Cloudflare无限多企业邮箱，可收发邮件」： 借助 Resend (resend.com) 的免费 API。Resend 的免费套餐每天可以发送 100 封邮件，每个月最多 3000 封。 说明： 详细用法在之前的视频里，本期不再赘述。 part11: DDoS 防御 (10:39-10:55) 12.1: 免费 DDoS 防御方案 之前也有一期完整视频介绍使用 Cloudflare 防御 DDoS 攻击。\n主要方法： 配置 CDN。 配置 IP 白名单。 使用 Cloudflare 防火墙（Firewall）。 说明： 这三种方法可以有效防御 DDoS 攻击，详细内容在那期视频里，这里不再赘述。 part12: 其他免费工具与总结 (10:55-1:01) 13.1: 其他推荐的免费工具\n♎UI 设计： 可以选择 Figma (figma.com)。 文档工具： 可以选择 Notion (notion.so)。 (11:01-1:04) 13.2: 工单管理工具 ♎工单管理可以使用 GitHub 的 Project 功能。\n(11:04-11:13) 13.3: 作者开发习惯与视频总结 作者自嘲写代码风格是**\u0026ldquo;一把梭\u0026rdquo;，通常不做设计**、不写文档、也不创建 ticket。视频到此结束。\n","date":"2025-03-11T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E5%85%A8%E6%B5%81%E7%A8%8B%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA%E5%88%B0%E9%83%A8%E7%BD%B2_%E7%A9%B7%E9%AC%BC%E5%A5%97%E9%A4%90%E7%89%88/","title":"全流程项目构建到部署_穷鬼套餐版"},{"content":"","date":"2025-03-11T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E6%A8%A1%E6%9D%BF/","title":"模板"},{"content":"༼ 面包购买小技巧 2——崎本生吐司什么水平？༽ 00:00:00 强者发型最让人难过的不是强者发型，而是我秃了，但是没变强。就给你们介绍一下怎么样来看面包餐包们，这餐包就很好看。这个的话，他因为他没有刷鸡蛋，所以说他这个地方不会有那种明显的线。然后这底下看到没有裂纹，这就是中间没有收紧，这就是做工不是太好的实习生做的，但是做多了就不会。然后是这个面包，这个面包我们可以看得出来，就这个地方有一条线能明显的看得出来，就是有光泽的地方是刷了蛋的，没光泽的地方是没有刷弹的。底下白色的这一部分就是进了烤箱以后膨胀的部分。\n00:00:36 ♈很多时候大家对那个发酵本身它是有误解的，就是只有我要你发酵声你才发酵，怎么可能？♈酵母它不听你的话，好吧？♈那个酵母它想发就发是吧？冷藏的时候也可以发，高温的时候也可以发，♐直到50多度它死了。|他们把这个面包发酵到自己觉得好了，然后再开烤箱。烤箱预热要十几分钟，面包就又发了十几分钟。♈然后面包你把它放进去的时候，面包只有三十多度，三十多度升温到50多度对吧？然后慢慢的再烤熟，这个过程当中它还在发酵，|♋所以说为什么底下会有这一圈白的这就是这个面包它在烤箱里面发酵的痕迹。\n00:01:19 你懂怎么样做面包，你自然就懂怎么样买面包。知道了这些面包和甜点他做的时候有哪些环节是如何判断他做的好坏，自然买的时候也会知道它的好坏。还有一个点就是这个面包怎么样好吃。你看它这个面包明显是不圆的这面包从底部可以看得出来，它不是圆形的，♈不是圆形的说明什么问题？说明这个面包它搓的时候用力不均匀，用力不均匀，这个面包它的力道就不一样。♋这样的话你吃着就是一口软一口硬，这就是普通面包店的水平。好面包店的话它做出来就是圆的，每一个面包它都受力很匀。\n00:02:02 ♈就是我评价面包一直以来都是统一的标准，首先皮薄上色好，然后再就是里面的组织比较均1，看到没有？上下左右还是中间，它的那个气泡的大小是几乎一致的。如果能够看得到那种漩涡状的纹理的话，那就说明整形特别好。♈它的气泡均匀就说明这个面团受力均匀，受力均匀就说明吃起来口感比较一致，我从中间抽一片出来，他这个样子，这个地方就是属于整形的时候排气没有排好，但是♈整体的面团就是很松软。然后你看这撕的时候，他也没有给广告费，♋看们撕起来像棉絮一样就是比较好。\n00:02:50 ♈最近我发现一个比较有意思就是chagt，它可以点评这个面包的好坏，以后你们去买的时候可以用它。但是我跟你们说第好目前我感觉他点评的不是太好，我现在看GPT怎么来评价这个面包。稍等，见证奇迹的时候到看看他的分析，它是从几个维度说的。♋首先说优缺点，这个面包的组织怎么样，发酵的怎么样？然后还有不足和可以优化的点，包括还有表皮分析，上色还有什么之类的。我觉得他把这个面包的评价维度基本上都考虑到了。\n00:03:25 ♈如果你去一家面包店，你不知道这个面包做的好坏，你就拍张照片发给gp t♈你看AI认不认可他的面包。对于不懂的人来说，AI应该比你们懂，♋他评价不了太高级的吐司，但是作为基本的吐司，绝对能够评价得了你做出来的东西。你让AI帮你点评一下，以目前来看是可以的。♋就觉得自己烘焙做的好的，♋或者比如说你们如果在小红书上面看到哪家想买的，♈你可以把这个图也发给gt看看g怎么样评价他们。这拿起来它应该是这么软的。目前来说我觉得它已经是最接近日本圣图斯的产品，它和人字传做的比较方便。我在盒马买了，非常失望，然后我在卢布中买了它不是生吐司，我觉得它是一个没有蛋的。不留兄再给大家看下七氛，刚才我们已经看过了，它的深吐司还是做的不错。\n00:04:16 我上海的、北京的，然后天津的七本都看过了，都买过了。这一家是第一次买，但是其他的我都买了不止一次。他们一贯表现都不错，而且都是去年表现不错，今年在新的地方也能表现不错。我觉得他们整体的管理是不错，肯定在设备上面下了大功。因为没听说哪家店能够培养这么多人才，主要还是靠设备，这个应该很贵。\n00:04:42 给大家看一下，现在正儿八经这么做的已经很少了。大家有没有发现一个现象，就是♋现在那些吐司，特别是那种小的吐司，上面搞的花里胡哨，有很多肉松什么东西的，让你很有购买欲望。然后你一切开发现所有的料都在表面|，看见没有？这就是典型的老式吐司。它的做法是什么样的？这种的做法就是先把那个面包的面团擀成这么大一张大饼，然后抹上酱，卷起来以后切切了以后再扭成麻花。这样的话它就是特别均匀的，你每一口都能吃到馅料吃起来就会很爽。但是给的料就要很多，你料给的多，你就要卖的贵。\n00:05:20 别人的做法是我把那个吐司变成一了辫子以后，♈把所有的材料撒在表面，它只需要撒不到泥，一般的材料它也可以做出跟你一样的样子。看起来大家椰蓉都是一样多，但是他就可以比你卖的便宜。♋这个第一印象就已经决定了大多数的客人会往把所有料撒在表面的点，这就是一种降本增效的方法。\n00:05:41 我做椰蓉吐司的时候也是这样做的，因为馅料都比较好吃。尝一口它的椰蓉没有偷工减料，就是黄油椰蓉往里面怼。但我觉得的话，如果在外里面加点奶粉会更猛，威力会更加加强。主要是这样做的店太少了，一般都是上面给一些椰蓉，然后底下就还是白吐司，说实话吃的有点意思。\n00:06:01 这是一个卡仕达吐司，这也是一个在上海已经阔别十 年之久的产品了。因为没有人干这种蠢事，你知道吧？就是又花功夫又花材料，卖的稍微贵一点，然后就被一群人追上骂。我上次去棋本的时候还没有看到这个。对了，棋本要跟你们说些什么东西，我太适合买。\n00:06:20 就是他家的背部，他家的贝果，实际上就是把那个软面包做成了贝果的形状，他是那种特别软的。我提前告诉你们，有人喜欢吃这种日式软贝果，有人喜欢吃美式的硬背果，这是个个人偏好的问题，对吧？这就有点像是你喜欢吃济南的煎饼果子还是天津的煎饼果子，那谁知道呢？特别是你要对于一个武汉人来说的话，其实我觉得两边都挺好吃。\n00:06:46 这是五大道的另外一家甜点店，进去以后就觉得他做的不错。就是从外表上面来看，我觉得这做的不错。全麦吐司这么看还是看皮，然后看四周土司，它周围有模具，它不能自由发展，它发展到旁边的话，适当的沉积是正常的。你里面其实能够♈看得到它的那个漩涡状，♏还有就是这个地方的白边，这个地方能看到明显浅一些，就说明它的发酵控制也还可以|，这个地方说明它的整形还可以，♈然后他的这个气泡说明他揉的面也还行。\n00:07:18 然后再就是闻香味，当然一般的店里面是不会给你闻香味的，你只能说是我前面已经看到这些东西都ok的情况下，买了以后第一件事情是闻香味。我在里面闻到了糖的味道，它是属于一个偏甜一点的。而且他店里面我还发现一个特别有意思的东西，这个我已经好久没有见到了，不留修肉吐司，我一看就乐了。\n00:07:37 因为我以前不是跟大家讲过一个超象故事，我们语言学校有个同学就是偷偷吃这个面包，胖了十几斤。因为我觉得太好吃了，天天去吃。但他吃的是附中的那家，这家没有这个水，怎么看他是不留奥消弱吐司就是这么黄，它的第一配版在山姆里面，山姆里面那个黄油吐司多乐之日的就是这个的简配。它有一点我觉得我比较喜欢的一点，就是他做的这个造型，布流香吐司的造型应该是搓成一个一个的小球，然后烤出来以后都是这一个一个的这种小山包。闻起来的话，它的那个发酵风味和黄油风味也都还可以。但是我感觉它是它发酵时间偏长了一点，就从这中间这个树脂的这个气孔，我觉得它稍微偏长了一点，结果黄油的味道就散了一些。因为这个面包我觉得最重要的点就是黄油的香味。那你为了把这个面包发的大一点，然后黄油香味变少了，其实我觉得有点可惜，希望这个面包可以越做越。 00:08:34 对了，还有一点就是不灵头削肉吐司比一般的吐司都要小，一对比就可以看出来它就小。为什么？♋因为它第一它的黄油比较多，第二的话就是这个面团它比较弱，它膨胀不了那么大。一般来说不留休是250克的吐司，而普通的吐司白吐司的话是450克的吐司|。\n00:08:53 还有一种250克吐司是什么呢？就中间加了馅料的吐司，就比如这个，♋因为你里面如果加了馅料以后，你想把面包发的450克那么大，你是在做梦。所以说这个东西它都是有基本原理在里面的。\n00:09:07 当你看到形状、颜色或者是什么模具不匹配的东西，就赶紧快跑，就不要觉得舔舔石头不干活。我们是随便在选模具，什么东西该用，什么样的模具该做多大都是前人留下来的经验，而不是什么我脑袋一拍，我去换个新造型，然后再来点评一下他们的泡芙，就是泡芙这个东西在法语里面叫包菜。长得像包菜，就是做的好，就是正常的包菜应该是这个样子这个样子。那这个泡芙它就有问题，你就比如说我们这横着看，他就有点横向发展，横向发展不是什么好事，这个就是稍微好一些，但它整个面团其实也是偏稀的。真正好的他应该是这个形状包，它应该是这个形状，这块的就不错。你看上面它是花瓣形的裂开，而且它的那个高度和宽度的比例就比较正常。\n","date":"2025-03-11T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E9%9D%A2%E5%8C%85%E8%B4%AD%E4%B9%B0%E5%B0%8F%E6%8A%80%E5%B7%A7-2%E5%B4%8E%E6%9C%AC%E7%94%9F%E5%90%90%E5%8F%B8%E4%BB%80%E4%B9%88%E6%B0%B4%E5%B9%B3/","title":"面包购买小技巧 2——崎本生吐司什么水平？"},{"content":" [让AI解析github项目]\n《1 直接把项目克隆下来，然后用AI客户端打开即可，反正项目说明README.md是markdown，图片什么的都有，再复杂的项目AI都能用README.md解析》\n\u0026lt;2 使用github MCP解析自己克隆下来的仓库\u0026gt;\n\u0026lt;3 给AI客户端添加URL文档集 直接把github项目地址作为文档集数据\n","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/ai%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%A6%82%E4%BD%95%E8%A7%A3%E6%9E%90github%E9%A1%B9%E7%9B%AE/","title":"AI客户端如何解析github项目"},{"content":" [让AI解析github项目]\n\u0026lt;1 给AI客户端添加URL文档集 直接把项目文档的地址作为文档集数据》\n\u0026lt;2 把在线项目文档的网页crlt+s保存下来后加入到当前项目\u0026gt;\n\u0026lt;3 使用联网 MCP服务 然后AI客户端联网去解析在线项目文档\u0026gt;\n","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/ai%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%B7%BB%E5%8A%A0%E9%A1%B9%E7%9B%AE%E6%96%87%E6%A1%A3%E4%B8%89%E7%A7%8D%E6%96%B9%E6%A1%88/","title":"AI客户端添加项目文档（三种方案）"},{"content":"part1: 创建新的 GitHub 仓库 (00:04-00:23) 1.1: 推荐仓库命名 对于首次部署博客的用户，视频推荐将 GitHub 仓库名称设置为 **用户名.github.io** 的格式。视频中演示了输入 **letere-gg.github.io** 作为示例。\n(00:23-00:30) 1.2: 处理命名冲突 视频指出 **letere-gg.github.io** 这个特定名称在其账户下已被占用，因为它已被用于部署另一个项目（提及了 Hexo，但后续操作基于 Hugo）。\npart2: 理解 GitHub Pages 的 URL 结构 (00:30-00:47) 2.1: 用户名.github.io 仓库的 URL 视频展示了名为 **letere-gg.github.io** 的仓库，在其 Settings -\u0026gt; Pages 中，生成的 GitHub Pages 网站 URL 是根域名 **https://letere-gg.github.io**。\n(00:48-01:02) 2.2: 普通仓库的 URL 视频接着展示了另一个名为 **hugo-stack** 的仓库，在其 Settings -\u0026gt; Pages 中，生成的 GitHub Pages 网站 URL 带有仓库名称作为子路径，即 **https://letere-gg.github.io/hugo-stack**。\n(01:02-01:18) 2.3: 部署建议 基于以上对比，视频建议首次部署时使用 用户名.github.io 格式命名仓库，这样生成的网站 URL 不带子路径，可以避免一些潜在的 bug。\npart3: 设置新仓库与本地项目配置 (01:21-01:35) 3.1: 创建示例仓库 由于推荐的仓库名已被占用，视频演示创建了一个新的示例仓库，命名为 **hugo-dev**，并确保其设置为 Public（公开），因为只有公开仓库才能部署 GitHub Pages 静态页面。\n(01:35-01:46) 3.2: 准备上传代码 仓库创建成功后，页面显示了用于上传代码的 Git 命令。视频提到下一步是上传代码，但在此之前需要修改一个配置文件。\n(01:46-02:16) 3.3: 修改 hugo.yaml 文件 视频打开本地项目中的 **hugo.yaml** 文件，指出需要修改 **baseURL** 配置项。将其值修改为与 GitHub Pages 部署后期望的 URL 一致。因为仓库名是 hugo-dev，所以 baseURL 被设置为 **https://letere-gg.github.io/hugo-dev/**，必须包含末尾的仓库名子路径。\npart4: 生成静态网站并上传至 GitHub (02:18-02:46) 4.1: 使用 Hugo 生成站点 视频演示在本地项目文件夹中♈删除旧的 public 文件夹（如果存在）。然后在项目根目录打开命令提示符 (cmd)，♈运行 **hugo -D** 命令。此命令使用 Hugo 编译项目，并♈将生成的静态网站文件输出到 public 文件夹中。\n(02:46-03:18) 4.2: 初始化 Git 仓库 ♈进入新生成的 public 文件夹，确认里面是静态网站内容。在此 public 文件夹内打开命令提示符 (cmd)，并♈执行 **git init** 命令，将其初始化为一个 Git 仓库。\n(03:18-03:49) 4.3: 添加文件、提交并关联远程仓库 ♈按照 GitHub 页面提供的指南（略作修改）：\n使用 **git add .** 命令将 public 文件夹内所有生成的静态文件添加到暂存区（视频强调是 . 而不是 README.md）。 使用 **git commit -m \u0026quot;first commit\u0026quot;** 提交更改。 使用 **git branch -M main** 将当前分支重命名为 main。 使用 **git remote add origin https://github.com/letere-gg/hugo-dev.git** 关联到之前创建的远程 GitHub 仓库。 (03:49-04:08) 4.4: 推送代码到 GitHub ♈执行 **git push -u origin main** 命令，将本地 main 分支的代码推送到 GitHub 远程仓库。视频提到在国内环境可能需要开启代理才能成功推送。推送完成后，刷新 GitHub 仓库页面，可以看到静态文件已成功上传。\npart5: 启用 GitHub Pages 并验证部署 (04:09-04:23) 5.1: 配置 Pages 源 返回 GitHub 上的 hugo-dev 仓库页面，进入 Settings -\u0026gt; Pages 选项卡。在 \u0026ldquo;Build and deployment\u0026rdquo; 部分下的 \u0026ldquo;Source\u0026rdquo; 设置中，选择 Deploy from a branch，然后从 \u0026ldquo;Branch\u0026rdquo; 下拉菜单中选择 **main** 分支，点击 Save。\n(04:23-05:24) 5.2: 等待部署完成 保存设置后，GitHub Pages 开始部署过程。视频指出这需要一些时间。等待页面刷新，直到显示 \u0026ldquo;Your site is live at \u0026hellip;\u0026rdquo; 并给出最终的网站 URL：**https://letere-gg.github.io/hugo-dev/**。\n(05:24-05:35) 5.3: 访问并验证站点 点击页面上提供的网站链接。浏览器成功加载了部署好的博客网站，显示内容正常，证明部署已成功。视频总结这个过程很简单。\n","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/github-page%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2/","title":"github page部署hugo博客"},{"content":"Hugo博客的action代码：解析用aciton配置hugo博客的action代码 好的，这是视频中展示的、由社区贡献者 (peaceiris) 维护的用于通过 GitHub Actions 部署 Hugo 站点的 Actions 工作流 YAML 代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # 工作流名称 name: deploy # 触发工作流的事件 on: push: branches: # 仅在 main 分支被推送时触发 - main # 定义工作流中的作业 jobs: # 定义一个名为 deploy 的作业 deploy: # 指定作业运行的环境为最新的 Ubuntu 系统 runs-on: ubuntu-latest # 定义作业中的步骤 steps: # 步骤1：检出代码 - name: Checkout # 使用官方的 actions/checkout 动作来获取仓库代码 uses: actions/checkout@v4 with: # 获取完整的 Git 历史记录（对于部署工具 peaceiris/actions-gh-pages 可能需要） # 0 表示获取所有历史，而不是默认的浅克隆（只获取最后一次提交） fetch-depth: 0 # 步骤2：设置 Hugo 环境 - name: Setup Hugo # 使用社区维护的 peaceiris/actions-hugo 动作来安装 Hugo uses: peaceiris/actions-hugo@v3 with: # 指定要安装的 Hugo 版本为最新版 hugo-version: \u0026#39;latest\u0026#39; # 明确指定安装 Hugo 的 Extended 版本（支持 SASS/SCSS 等特性） extended: true # 步骤3：构建网站 - name: Build Web # 运行 Hugo 命令来生成静态网站文件 # -D 参数表示包含草稿内容（如果本地预览用过，这里保持一致，或按需移除） run: hugo -D # 步骤4：部署网站到 GitHub Pages - name: Deploy Web # 使用社区维护的 peaceiris/actions-gh-pages 动作来部署 uses: peaceiris/actions-gh-pages@v4 with: # 指定用于认证的 Personal Access Token (PAT) # ${{ secrets.TOKEN }} 会从仓库的 Secrets 中读取名为 TOKEN 的值 # （视频中占位符是 你的token变量名，这里假设你设置为 TOKEN） PERSONAL_TOKEN: ${{ secrets.TOKEN }} # 指定要将静态文件部署到的目标仓库 # 格式为：你的GitHub用户名/目标仓库名 # （视频中占位符是 你的github名/你的仓库名，这里假设目标仓库是 letere-gz/hugo-dev） EXTERNAL_REPOSITORY: letere-gz/hugo-dev # 指定将静态文件推送到目标仓库的哪个分支（通常是 gh-pages 或 main） PUBLISH_BRANCH: main # 指定包含构建好的静态网站文件的目录（Hugo 默认生成到 public） PUBLISH_DIR: ./public # 自定义部署时 Git 提交的信息 commit_message: auto deploy 代码解释 (遵循托勒斯法则/Law of Demeter - 关注“做什么”而非“怎么做”的内部细节):\nname: deploy\n作用: 给这个自动化流程（工作流）起个名字，叫做 deploy。这个名字会显示在 GitHub 仓库的 \u0026ldquo;Actions\u0026rdquo; 标签页里，方便你识别。 托勒斯法则: 你只需要知道这是流程的名字，不需要关心 GitHub 内部如何使用这个名字。 on: push: branches: [main]\n作用: 定义触发这个自动化流程的条件。这里设置的是：当你向 main 这个分支推送 (push) 代码时，这个流程就会自动开始运行。 托勒斯法则: 你只需要知道“推送到 main 分支”这个动作会启动流程，不需要关心 GitHub 是如何监控推送事件或分支的。 jobs: deploy:\n作用: 定义这个流程中要执行的一个或多个任务组（作业）。这里只有一个任务组，名字叫 deploy。 托勒斯法则: 你只需要知道这里开始定义一个主要的工作单元，它包含了一系列要完成的步骤。 runs-on: ubuntu-latest\n作用: 指定 deploy 这个任务组运行在什么样的虚拟环境里。这里指定的是 ubuntu-latest，意味着 GitHub 会提供一个最新的 Ubuntu Linux 虚拟服务器来运行后续步骤。 托勒斯法则: 你只需要知道你的自动化步骤将在一个 Linux 环境中执行，不需要关心这个环境是如何创建和管理的。 steps:\n作用: 定义 deploy 作业中要按顺序执行的具体步骤。 - name: Checkout\n作用: 给这个步骤起个名字，叫 Checkout，方便在日志中查看。 ♈uses: actions/checkout@v4: 使用一个预先写好的、官方提供的“动作” (Action)，叫做 actions/checkout (版本是 v4)。♐这个动作的功能是**把你的仓库代码下载（检出）**到当前的虚拟运行环境中，以便后续步骤可以使用这些代码。 with: fetch-depth: 0: 这是给 actions/checkout 动作传递的一个参数。fetch-depth: 0 的意思是下载完整的代码历史记录，而不是默认只下载最近的一次提交。部署动作 peaceiris/actions-gh-pages 有时需要完整的历史记录才能正确工作。 托勒斯法则: 你只需要知道这个步骤会把你的代码准备好，并且需要完整的历史。你不需要知道 checkout 动作内部是如何通过 Git 命令实现下载的。 - name: Setup Hugo\n作用: 步骤名，设置 Hugo 环境。 ♈uses: peaceiris/actions-hugo@v3: 使用一个由社区开发者 (peaceiris) 维护的 Action，叫做 actions-hugo (版本是 v3)。这个动作专门用来安装和配置 Hugo。 【♓这一步的作用是把能够执行 hugo 命令的那个程序安装好 ，因为♐即使你的项目文件上传到了仓库，也仍然需要安装 Hugo 包。你上传到仓库的是你博客的“原材料”（Markdown 文件、主题、配置），而不是制作网站的“工具”（即 Hugo 程序本身））。” 托勒斯法则: 你只需要知道，这一步负责获取你的博客源代码（包括你的 Markdown 文章、主题、配置文件等），并将它们准备好，以便后续步骤使用。它确实使用了你推送到仓库的文件，♈Action 运行环境是一个临时的、干净的 Linux 系统，它默认并没有安装 Hugo 这个“工具”】 with:: 给 actions-hugo 动作传递参数： hugo-version: 'latest': 告诉动作要安装最新版本的 Hugo。 extended: true: 告诉动作要安装 Hugo 的 Extended（扩展）版本。这个版本支持 SASS/SCSS 等高级功能，很多主题需要它。 托勒斯法则: 你只需要知道这个步骤会自动安装好你需要的 Hugo 版本，不需要关心它具体从哪里下载、如何安装。 - name: Build Web\n作用: 步骤名，构建网站。 run: hugo -D: 直接运行一条命令：hugo -D。这就是你在本地用来生成静态网站文件的命令。它会读取你的 Hugo 项目文件，并把最终的网站文件（HTML, CSS, JS, 图片等）生成到默认的 public 目录下。-D 参数是包含标记为草稿 (draft) 的内容。 托勒斯法则: 你只需要知道这个步骤执行了 Hugo 的构建命令，并产出了 public 文件夹，就像你在本地操作一样。 - name: Deploy Web\n作用: 步骤名，部署网站。 uses: peaceiris/actions-gh-pages@v4: 使用另一个由 peaceiris 维护的 Action，叫做 actions-gh-pages (版本是 v4)。这个动作专门用来将一个文件夹的内容推送到指定仓库的指定分支，非常适合部署到 GitHub Pages。 with:: 给 actions-gh-pages 动作传递部署所需的参数： PERSONAL_TOKEN: ${{ secrets.TOKEN }}: 提供认证凭证。这个动作需要权限去推送代码到目标仓库。这里使用的是你之前创建并存储在仓库 Secrets 中的 Personal Access Token (PAT)。secrets.TOKEN 表示去读取名为 TOKEN 的那个 Secret 的值。 EXTERNAL_REPOSITORY: letere-gz/hugo-dev: 指定目标仓库。告诉动作要把 public 目录下的文件推送到 letere-gz 这个用户（或组织）下的 hugo-dev 仓库。你需要将 letere-gz/hugo-dev 替换成你自己的 GitHub 用户名和用于托管 GitHub Pages 的仓库名 (例如 your-username/your-username.github.io)。 PUBLISH_BRANCH: main: 指定目标分支。告诉动作要把文件推送到目标仓库的 main 分支。你需要根据你的 GitHub Pages 设置，将其修改为正确的目标分支 (常见的有 gh-pages 或 main)。 PUBLISH_DIR: ./public: 指定要部署的内容来源。告诉动作需要推送的是当前目录下 public 文件夹里的所有内容。 commit_message: auto deploy: 设置部署时产生的 Git 提交信息。每次部署都是一次 Git 提交，这里设置提交信息为 \u0026ldquo;auto deploy\u0026rdquo;。 托勒斯法则: 你只需要知道这个步骤负责把 public 目录的内容，使用你提供的 Token 认证后，推送到你指定的目标仓库和分支。你不需要关心它内部是如何进行 Git 操作和推送的。 Author:\nURL: http://example.org/posts/hugo%E5%8D%9A%E5%AE%A2/action-%E4%BB%A3%E7%A0%81%E6%88%91%E8%87%AA%E5%B7%B1%E5%86%99%E4%B8%8D%E6%9D%A5%E7%9A%84%E4%B8%8D%E7%94%A8ai%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E9%9C%80%E8%A6%81steps%E5%AD%97%E6%AE%B5%E4%B8%8B%E7%9A%84users%E5%8F%82%E6%95%B0/\n","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/hugo%E5%8D%9A%E5%AE%A2action%E4%BB%A3%E7%A0%81%E9%83%A8%E7%BD%B2action%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"hugo博客action代码部署：action代码解析"},{"content":" 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 # 工作流名称 name: deploy # 触发工作流的事件 on: push: branches: # 仅在 main 分支被推送时触发 - main # 定义工作流中的作业 jobs: # 定义一个名为 deploy 的作业 deploy: # 指定作业运行的环境为最新的 Ubuntu 系统 runs-on: ubuntu-latest # 定义作业中的步骤 steps: # 步骤1：检出代码 - name: Checkout # 使用官方的 actions/checkout 动作来获取仓库代码 uses: actions/checkout@v4 with: # 获取完整的 Git 历史记录（对于部署工具 peaceiris/actions-gh-pages 可能需要） # 0 表示获取所有历史，而不是默认的浅克隆（只获取最后一次提交） fetch-depth: 0 # 步骤2：设置 Hugo 环境 - name: Setup Hugo # 使用社区维护的 peaceiris/actions-hugo 动作来安装 Hugo uses: peaceiris/actions-hugo@v3 with: # 指定要安装的 Hugo 版本为最新版 hugo-version: \u0026#39;latest\u0026#39; # 明确指定安装 Hugo 的 Extended 版本（支持 SASS/SCSS 等特性） extended: true # 步骤3：构建网站 - name: Build Web # 运行 Hugo 命令来生成静态网站文件 # -D 参数表示包含草稿内容（如果本地预览用过，这里保持一致，或按需移除） run: hugo -D # 步骤4：部署网站到 GitHub Pages - name: Deploy Web # 使用社区维护的 peaceiris/actions-gh-pages 动作来部署 uses: peaceiris/actions-gh-pages@v4 with: # 指定用于认证的 Personal Access Token (PAT) # ${{ secrets.TOKEN }} 会从仓库的 Secrets 中读取名为 TOKEN 的值 # （视频中占位符是 你的token变量名，这里假设你设置为 TOKEN） PERSONAL_TOKEN: ${{ secrets.TOKEN }} # 指定要将静态文件部署到的目标仓库 # 格式为：你的GitHub用户名/目标仓库名 # （视频中占位符是 你的github名/你的仓库名，这里假设目标仓库是 letere-gz/hugo-dev） EXTERNAL_REPOSITORY: letere-gz/hugo-dev # 指定将静态文件推送到目标仓库的哪个分支（通常是 gh-pages 或 main） PUBLISH_BRANCH: main # 指定包含构建好的静态网站文件的目录（Hugo 默认生成到 public） PUBLISH_DIR: ./public # 自定义部署时 Git 提交的信息 commit_message: auto deploy ","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/hugo%E5%8D%9A%E5%AE%A2action%E4%BB%A3%E7%A0%81%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2%E7%9A%84action%E4%BB%A3%E7%A0%81%E6%9C%AC%E4%BD%93/","title":"hugo博客action代码部署：hugo博客的action代码本体"},{"content":"part1: 介绍 GitHub Action 自动化部署 Hugo 博客 (00:00-00:09) 1.1: 介绍目标\n视频开篇展示了一个本地运行的 Hugo 博客 (localhost:1313)。 目标是向大家介绍如何通过 GitHub 的 GitHub Actions 功能来自动部署博客。 (00:09-00:32) 1.2: 手动部署痛点与自动化优势\n♈指出传统方式下，每次修改完文件后，都需要手动将生成的 public 文件夹上传到服务器或托管平台，这个过程比较麻烦。 ♈使用 GitHub Actions 可以实现自动化：自动帮我们生成 public 目录，并将其部署到静态页面网站（如 GitHub Pages）。 ♈优点：只需提交代码 (git push)，后续的构建和部署过程就能自动完成，方便很多。 part2: 创建 GitHub 仓库并推送 Hugo 源码 (00:32-00:41) 2.1: 创建仓库准备\n要实现自动化部署，首先需要一个 GitHub 仓库来存放 Hugo 项目的主文件（源代码）。 操作：导航到 GitHub 网站，准备创建一个新的仓库。 (00:41-01:12) 2.2: 创建私有仓库\n在 GitHub 创建新仓库页面（经历短暂的网络卡顿后），设置仓库信息： 仓库名：hugo-main 可见性：建议选择私有（Private），因为源代码一般不需要公开。 操作：点击 Create repository 完成创建。 (01:12-01:24) 2.3: 上传文件准备\n仓库创建完成后，需要将本地的 Hugo 项目文件上传到这个新仓库。 关键点：上传前需要注意忽略掉一些不需要版本控制的文件。 (01:24-02:04) 2.4: 配置 .gitignore 忽略生成文件\n说明：Hugo 项目中的某些文件/目录是自动生成的，不应该提交到源代码仓库，包括： public/：Hugo 生成的静态网站文件目录。 resources/：Hugo 处理后的资源缓存目录。 hugo.build.lock：构建锁文件。 hugo.exe：（如果在 Windows 下）Hugo 可执行文件本身。 ♈ 操作：在本地 Hugo 项目根目录下，创建一个名为 .gitignore 的文件。 ♈操作：编辑 .gitignore 文件，将上述需要忽略的文件名或目录名（如 public, resources, hugo.build.lock, hugo.exe）添加进去，每行一个。 ♈目的：保证上传到 GitHub 的是最干净、最原始的 Hugo 项目文件。 (02:04-03:10) 2.5: Git 推送操作\n操作：在本地 Hugo 项目目录下打开命令行终端 (cmd)。 ♈操作：执行一系列 **Git 命令（仓库首页有）**来初始化仓库、添加文件并推送到远程 GitHub 仓库： git init：初始化本地 Git 仓库。 git add .：添加所有文件到暂存区（.gitignore 会生效，忽略指定文件）。 git commit -m \u0026quot;first commit\u0026quot;：提交更改。 git branch -M main：重命名当前分支为 main。 git remote add origin https://github.com/letere-gz/hugo-main.git：关联远程仓库地址。 git push -u origin main：推送本地 main 分支到远程仓库。 验证：切换到 GitHub 仓库页面刷新，确认项目文件已上传，且 .gitignore 中指定的文件（如 public）未被上传。 part3: 配置 GitHub Actions Workflow (03:10-03:59) 3.1: 参考官方与民间方案\n提及：作者自己的博客 (letere-gz.github.io/hugo-stack/) 提供了关于 Hugo + GitHub 免费部署的教程文章。 ♈参考：查看 Hugo 官方文档 (gohugo.io) 中关于 Hosting and deployment -\u0026gt; Host on GitHub Pages 的部分。 发现：官方文档提供了一个使用 GitHub Actions 部署的 workflow 示例（Step 6 中的 hugo.yml 文件内容）。 评价：作者认为官方提供的 workflow 文件内容很长，配置起来可能有点麻烦。 (03:59-04:13) 3.2: 引入简化版 Workflow\n建议：作者推荐使用一个民间大佬（peaceiris）编写和维护的简化版的 GitHub Actions workflow 配置。 来源：该简化配置可以在作者的教程文章中找到（或者直接参考 peaceiris/actions-hugo 和 peaceiris/actions-gh-pages 这两个 Action）。 优势：相比官方示例，这个版本更简洁、更方便使用。 (04:13-06:15) 3.3: 解析简化版 Workflow 配置\n展示：作者展示了其教程文章中的简化版 workflow YAML 文件内容，并逐段解释其含义： name: deploy: 定义 Workflow 的名称，显示在 Actions 列表里。 on: push: branches: [ main ]: 设置触发条件，当代码被 push 到 main 分支时，自动执行此 workflow。 jobs: deploy:: 定义一个名为 deploy 的作业 (job)。 ♈ runs-on: ubuntu-latest: 指定该作业运行在 GitHub 提供的最新版 Ubuntu 虚拟环境上。 steps:: 定义该作业包含的一系列步骤 (step)。 - name: Checkout: 步骤名称。 uses: actions/checkout@v4: 使用官方提供的 checkout Action，将仓库代码下载到运行环境中。 with: fetch-depth: 0: 获取完整的 Git 历史记录（对于 Hugo 可能需要）。 - name: Setup Hugo: 步骤名称。 uses: peaceiris/actions-hugo@v3: 使用 peaceiris 提供的 actions-hugo Action 来安装和设置 Hugo 环境。 with: hugo-version: 'latest', extended: true: 配置 actions-hugo，指定安装最新 (latest) 版本的 Hugo，并且是扩展 (extended) 版本（支持 Sass/SCSS）。 - name: Build Web: 步骤名称。 run: hugo -D: 执行 Hugo 命令 (hugo 默认会构建，-D 表示包含草稿内容，但在部署时通常不需要 -D，作者这里可能写错了，通常只用 hugo)，生成静态网站文件到 public 目录。 - name: Deploy Web: 步骤名称。 ♈ uses: peaceiris/actions-gh-pages@v4: 使用 peaceiris 提供的 actions-gh-pages Action，将构建好的网站部署到 GitHub Pages。 with:: 配置 actions-gh-pages。 PERSONAL_TOKEN: ${{ secrets.你的token变量名 }}: 关键配置，用于授权 Action 推送代码到目标仓库。需要一个个人访问令牌 (PAT)，并将其存储为仓库的 Secret。你的token变量名 需要替换成实际设置的 Secret 名称。 EXTERNAL_REPOSITORY: 你的github名/你的仓库名: 关键配置，指定目标仓库，即用来托管 GitHub Pages 的仓库地址（例如 letere-gz/hugo-dev），必须修改为用户自己的 Pages 仓库。 PUBLISH_BRANCH: main: 指定将 public 目录的内容推送到目标仓库的哪个分支（通常是 gh-pages 或 main，取决于 Pages 设置）。 PUBLISH_DIR: ./public: 指定要发布的目录，即 Hugo 构建生成的 public 目录。 commit_message: auto deploy: 自动部署时 Git 提交的消息。 part4: 创建和配置 Personal Access Token (PAT) (06:15-06:49) 4.1: Token 的作用与必要性\n重申：Workflow 中的 PERSONAL_TOKEN 参数需要填入一个个人访问令牌 (PAT)。 目的：这个 Token 用于身份验证，使得 GitHub Actions 有权限将构建好的静态文件推送到目标 GitHub Pages 仓库。没有它，推送会因权限不足而失败。 操作准备：接下来演示如何生成这个 Token。 (06:49-08:09) 4.2: 生成 Personal Access Token (Classic)\n操作路径：在 GitHub 页面，点击右上角个人头像 -\u0026gt; Settings -\u0026gt; 左侧菜单滚动到底部找到 Developer settings -\u0026gt; Personal access tokens -\u0026gt; Tokens (classic)。 操作：点击 Generate new token 按钮，选择 Generate new token (classic)。 配置 Token： Note：给 Token 起一个描述性名称，例如 “用作部署”。 Expiration：设置过期时间。作者建议选择 No expiration（永不过期），理由是避免定期更新 Token 的麻烦。但也提示 GitHub 出于安全考虑建议设置过期时间。对于个人私有仓库，作者认为永不过期问题不大。 ♈* Select scopes：选择 Token 的权限范围。必须勾选以下两项： repo：授予对仓库的完全控制权（包括推送）。 workflow：授予更新 GitHub Actions workflow 的权限。 生成：点击页面底部的 Generate token 按钮。 (08:09-08:41) 4.3: 保存 Token\n♈ 重要提示：Token 生成后，完整的 Token 字符串只会显示这一次。必须立即复制 (Copy 按钮) 并将其保存在安全的地方（例如，作者将其粘贴到了本地的记事本文件中）。如果关闭或刷新页面，将无法再次查看完整的 Token。 (08:41-10:19) 4.4: 配置 Repository Secret\n♈ 目的：将生成的 Token 安全地提供给 GitHub Actions workflow 使用。不应该直接将 Token 字符串写入 YAML 文件。 操作路径：回到存放 Hugo 源代码的仓库（即 hugo-main），进入该仓库的 Settings -\u0026gt; 左侧菜单 Security 下的 Secrets and variables -\u0026gt; Actions。 ♈操作：点击 New repository secret 按钮。 配置 Secret： Name：输入 Secret 的名称。这个名称必须与 workflow YAML 文件中 secrets. 后面引用的名称完全一致。作者在这里将其命名为 TOKEN。 Secret：将刚才复制并保存的完整 Token 字符串粘贴到这里。 保存：点击 Add secret。 ♈优点：使用 Secrets 可以隐藏敏感信息，workflow 运行时会安全地注入该值，比硬编码在代码中安全得多。 part5: 演示自动化部署流程 (10:19-10:31) 5.1: 准备 workflow 文件\n操作：回到本地的 Hugo 项目目录（dev 目录）。 下一步：需要将包含 workflow 配置的 YAML 文件添加到项目中。 (10:31-11:29) 5.2: 创建并编辑 workflow 文件\n♈操作：在本地 Hugo 项目根目录下，创建 .github 文件夹，然后在 .github 文件夹内创建 workflows 文件夹。 ♈操作：在 workflows 文件夹中，新建一个 YAML 文件，作者将其命名为 hugo_deploy.yaml。 ♈操作：使用文本编辑器打开 hugo_deploy.yaml 文件，将之前从教程文章中复制的简化版 workflow 内容粘贴进去。 (11:29-11:57) 5.3: 修改 workflow 文件参数\n操作：在粘贴好的 workflow 内容中，修改两个关键的 with 参数： ♈将 PERSONAL_TOKEN: ${{ secrets.你的token变量名 }} 修改为 PERSONAL_TOKEN: ${{ secrets.TOKEN }}，确保 TOKEN 与上一步创建的 Repository Secret 名称一致。 ♈ 将 EXTERNAL_REPOSITORY: 你的github名/你的仓库名 修改为实际的 GitHub Pages 目标仓库地址，作者修改为 letere-gz/hugo-dev。 保存：保存对 hugo_deploy.yaml 文件的修改。 (11:57-12:47) 5.4: 创建新文章验证部署\n目的：为了验证推送到 main 分支是否能自动触发部署，需要对项目做一些更改，例如添加一篇新文章。 现状：先查看本地 (localhost:1313) 运行的博客，确认当前的文章列表（只有一个 \u0026ldquo;Chinese Test\u0026rdquo;）。 (12:47-13:22) 5.5: 本地新增文章\n操作：在命令行中，执行 hugo new content post/newnewnew/index.md 命令，在 content/post 目录下创建一个名为 newnewnew 的子目录，并在其中生成 index.md 文件。 操作：再次执行 hugo server -D 启动本地服务。 验证：访问 localhost:1313，确认名为 Newnewnew 的新文章已成功创建并显示在本地博客首页。 (13:22-14:03) 5.6: 推送更改触发 Action\n操作：在命令行中按 Ctrl+C 停止本地 hugo server。 操作：执行 Git 命令将更改推送到远程仓库： ♈* git add .：添加所有更改（包括新文章 .md 文件和 .github/workflows/hugo_deploy.yaml 文件）。 ♈* git commit -m \u0026quot;update\u0026quot;：提交更改，提交信息为 \u0026ldquo;update\u0026rdquo;。 ♈* git push：推送本地 main 分支的提交到 GitHub 上的 hugo-main 仓库。 part6: 验证自动化部署结果与总结 (14:03-14:29) 6.1: 观察 Action 执行\n操作：切换回 GitHub 网站，进入 hugo-main 仓库的 Actions 标签页。 观察：可以看到一个由刚才的 push 事件触发的名为 update（commit 信息）的 workflow run 正在执行（黄色图标，in progress）或者已经执行完毕（绿色勾，Success）。 检查：点击该 workflow run，可以查看名为 deploy 的 job，再点击进去可以看到详细的执行步骤和日志，确认 Checkout, Setup Hugo, Build Web, Deploy Web 等步骤均已成功完成。 (14:29-14:53) 6.2: 验证部署结果（首次刷新）\n操作：打开或刷新用于 GitHub Pages 托管的目标网站（作者的是 https://letere-gz.github.io/hugo-dev/）。 发现：第一次刷新时，页面上可能仍然没有显示新添加的文章 Newnewnew。 原因：GitHub Pages 的部署和 CDN 的缓存更新通常会有一定的延迟。 (14:53-15:10) 6.3: 验证部署结果（成功显示）\n操作：再次刷新 GitHub Pages 网站页面。 结果：新添加的文章 Newnewnew 成功出现在了部署后的网站首页上。 结论：这表明 GitHub Actions 已经成功地自动拉取了 hugo-main 仓库的最新代码，使用 Hugo 进行了构建，并将生成的 public 目录内容推送并部署到了 hugo-dev 这个 GitHub Pages 仓库。 (15:10-15:40) 6.4: 总结与结束\n总结：通过以上设置，现在实现了自动化部署流程。开发者只需专注于编写文章 (.md 文件)，完成后执行 git push 将代码推送到源仓库 (hugo-main)。GitHub Actions 就会自动接管后续的构建和部署工作，无需再手动处理 public 目录的生成和上传。 评价：整个过程变得非常方便和省心。 ","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E4%BD%BF%E7%94%A8github-action%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2hugo%E5%8D%9A%E5%AE%A2%E5%88%B0page%E4%BB%93%E5%BA%93/","title":"使用github-action自动部署hugo博客到“page”仓库"},{"content":"part1: Hugo + GitHub 免费部署自己的博客介绍 (00:00-00:15) 1.1: 视频目标与演示 视频旨在分享如何使用 Hugo 和 GitHub 免费部署一个属于自己的个人博客。开头展示了作者已经部署成功的博客页面作为最终效果预览。\n(00:15-00:37) 1.2: 作者说明与目的 作者声明自己并非专业的教学视频制作者，制作此视频主要是因为近期想更新内容，且正好在研究 Hugo 博客的搭建过程。目的是分享经验，帮助观看者在搭建过程中少走弯路。\n(00:37-00:44) 1.3: 博客内容示例 简要展示了博客中的一篇文章，内容即为本次视频所要讲解的 Hugo + GitHub 免费部署博客的教程记录。\npart2: 搭建动机与 Hugo vs Hexo 对比 (00:45-00:57) 2.1: 历史背景 (Hexo) 作者提到自己之前已经使用 Hexo 框架搭建过一个个人博客 (展示了之前的 Hexo 博客页面)。\n(00:57-01:19) 2.2: 转向 Hugo 的原因 解释了为何选择使用 Hugo 重新搭建：更换了新电脑 (天选4) 后，之前的 Hexo 本地文件丢失（因为没有上传备份到 GitHub），所以需要重新搭建。借此机会研究并尝试了 Hugo。\n(01:19-01:30) 2.3: Hugo 使用体验 作者认为实际使用下来，Hugo 感觉比 Hexo 更方便。\n(01:30-01:41) 2.4: 对新手的建议 (Hugo vs Hexo) 指出 Hugo 的官方文档基本都是英文，可能对新手不太友好。因此，推荐新手优先尝试 Hexo (文档相对更完善，中文资源多)。使用 Hugo 需要啃英文文档，对新手来说较为麻烦。\npart3: 下载 Hugo (01:41-01:54) 3.1: 开始教程：下载准备 正式进入教程环节，第一步是下载 Hugo。作者打开浏览器并搜索 Hugo。\n(01:54-02:22) 3.2: 定位 Hugo GitHub Releases 进入 Hugo 官网 (gohugo.io)，点击 GitHub 链接跳转到 Hugo 的 GitHub 仓库。在仓库页面，点击 Tags (标签/版本) 来查找发布版本。选择了当时的最新版本 v0.131.0。\n(02:22-02:30) 3.3: 选择并下载 Hugo Extended 版本 在版本发布页面向下滚动到 Assets (资源) 部分，强调需要下载 Extended (扩展) 版本，并选择了适用于 Windows 64位的 hugo_extended_0.131.0_windows-amd64.zip 文件进行下载。\npart4: Hugo 文件准备与站点创建 (02:30-02:55) 4.1: 解压 Hugo 文件 展示了下载好的 zip 压缩包。在桌面上创建了一个名为 test 的新文件夹。将下载的 zip 包移动到 test 文件夹中，并进行解压。解压后得到包含 hugo.exe 等文件的文件夹。\n(02:55-03:04) 4.2: Hugo 可执行文件说明 强调 Hugo 是用 Go 语言编写的，但使用时不需要单独安装 Go 环境，♈下载解压后的 hugo.exe 可执行文件即可直接使用，这一点很方便。清理了多余的文件和文件夹，只保留核心文件。\n(03:04-03:13) 4.3: 打开命令行工具 (CMD) 在包含 hugo.exe 的文件夹（路径类似 C:\\Users\\Letere\\Desktop\\test\\hugo_extended_0.131.0_windows-amd64）的地址栏输入 cmd 并回车，从而直接在该目录下打开命令行窗口。\n(03:13-03:34) 4.4: 创建新站点命令 (hugo new site) 在命令行窗口中，♈输入命令 hugo new site dev。该命令会创建一个名为 dev 的新文件夹，并在其中♈♈生成 Hugo 站点的基本目录结构。\n(03:34-03:45) 4.5: 理解命令输出与后续步骤 展示了 hugo new site 命令执行后的输出提示，其中包含了建议的后续操作步骤，如切换目录、安装主题、编辑配置、创建内容和启动服务。作者截图保存了这些步骤提示。\n(03:45-04:10) 4.6: 切换目录与复制 Hugo 执行文件 根据提示，首先使用 cd dev 命令切换到新创建的 dev 站点根目录。因为 hugo.exe 不在系统路径中，需要将其复制到当前的 dev 目录下，才能在该目录中执行 Hugo 相关命令。\npart5: 本地预览 (无主题) (04:10-04:28) 5.1: 准备本地预览 根据之前的步骤提示，下一步是安装主题，但作者决定先不安装主题，直接启动本地服务看下效果。\n(04:28-05:08) 5.2: 启动 Hugo 本地服务 (hugo server -D) 在 dev 目录下，执行命令 hugo server -D 来启动 Hugo 的内置Web服务器进行本地预览。解释 -D 是 --buildDrafts 的简写，用于包含草稿内容。命令成功执行后，输出了本地服务器地址 http://localhost:1313。\n(05:08-05:16) 5.3: 浏览器访问与结果 (Page Not Found) 复制该地址并在浏览器中打开。♈页面显示 Page Not Found (404错误)。作者解释这是正常现象，因为目前既没有安装主题，也没有任何内容，但这表明 Hugo 服务已经成功启动。\npart6: 查找与下载 Hugo 主题 (Stack) (05:16-05:33) 6.1: 前往 Hugo 主题站 回到命令行输出提示，明确需要安装一个主题。再次访问 Hugo 官网 (gohugo.io)，点击导航栏中的 Themes。\n(05:33-05:56) 6.2: 选择 Stack 主题 浏览 Hugo 主题列表，作者选择了名为 Stack 的主题（卡片式风格，适用于博主）。点击进入该主题的详情页。\n(05:56-06:13) 6.3: ♈下载 Stack 主题源码 在 Stack 主题详情页，点击 Download 按钮，跳转到该主题的 GitHub 仓库 (CallMeMaybe/hugo-theme-stack)。在仓库页面，同样通过 Tags 找到最新版本 v3.26.0，♈并点击 Source code (zip) 下载主题的源代码压缩包。\npart7: 安装与配置 Hugo 主题 (06:13-06:33) 7.1: 放置主题文件 展示下载好的主题压缩包 hugo-theme-stack-3.26.0.zip。回到本地的 Hugo 站点 dev 目录，找到 themes 文件夹。♈将下载的主题压缩包移动到 themes 文件夹内，并进行解压。\n(06:33-07:03) 7.2: 重命名主题文件夹与修改配置 解压后的主题文件夹名带有版本号 (hugo-theme-stack-3.26.0)。为了方便配置，♒将其重命名为 hugo-theme-stack。打开站点根目录下的配置文件 hugo.yaml (之前从主题的 exampleSite 复制过来的)。找到 theme 配置项，将其值修改为刚刚重命名的文件夹名 hugo-theme-stack。保存配置文件。\n(07:03-07:19) 7.3: 应用主题示例配置与内容 为了快速看到效果，作者将主题♐ exampleSite 目录下的 content 文件夹和 hugo.yaml 文件复制到站点根目录 (dev) 下，并选择覆盖现有文件。这样就♈直接使用了主题自带的示例内容和配置。\n(07:19-07:28) 7.4: 清理与重启服务 ♐删除了站点根目录下原有的 hugo.toml 文件【因为现在使用 hugo.yaml，它们是同一个配置文件，只是后缀名不同】。回到命令行窗口，停止之前运行的 Hugo 服务（通过按 Ctrl+C，虽然画面未显示），然后再次执行 hugo server -D 命令重新启动服务。\n(07:28-07:47) 7.5: 预览带主题的站点 刷新浏览器中 http://localhost:1313 页面。此时，网站成功加载了 Stack 主题和其示例内容，外观与之前展示的成品相似。\npart8: 主题示例内容处理 (07:47-08:07) 8.1: 删除 Rich Content 示例文章 在预览页面滚动时，作者提到主题♋示例中有一个名为 rich-content 的文章。因为这篇文章引用了 YouTube 等外部资源，在国内网络环境下可能导致加载超时或构建失败，所以♈建议删除。随后展示了♈在 根目录/content/post 目录下删除 rich-content 文件夹的操作。\n(08:07-08:39) 8.2: 参考主题 Demo 站 回到主题的 GitHub 仓库页面，指出 README.md 文件通常包含重要信息。找到了♋主题提供的 Demo 演示站点链接 (demo.stack.jimmycai.com) 并打开。说明可以通过访问 Demo 站点来了解 rich-content 页面原本包含哪些内容（如嵌入视频、特殊格式等）。\npart9: Hugo 国际化 (i18n) 功能说明 (08:39-09:04) 9.1: 文件名与语言代码 解释 Hugo 的国际化 (i18n) 功能。以 dev/content/post/chinese-test 文件夹为例，其中的内容文件是 index.zh-cn.md。指出文件名中的 .zh-cn 就是语言代码。对比 emoji-support 文件夹下的 index.md 文件，它没有语言代码。\n(09:04-09:19) 9.2: 国际化工作原理 说明：如果文件名不带语言代码 (如 index.md)，则该内容属于在 hugo.yaml 中配置的默认语言（此例中为英语）。♈网站会根据用户切换的语言（通过页面上的语言切换器）加载对应语言代码的文件【切出对应后缀名的文件】。例如，♈切换到中文，会加载 index.zh-cn.md；♈切换到英文，会加载 index.en.md 或默认的 index.md。 也就是当访问者在您的网站上选择或切换到中文（简体） 这个语言选项时，Hugo 会查找并使用带有 .zh-cn.md 后缀的文件来渲染该页面，比如 有一个名为 我的笔记.md 的 Markdown 文件，并且您希望它只在网站的中文环境下显示，需要将其重命名为 我的笔记.zh-cn.md (09:19-09:29) 9.3: 国际化演示 为了演示，♈将 chinese-test 文件夹下的 index.zh-cn.md 重命名为 index.en.md【就是改改后缀名而】。刷新本地预览页面。此时切换到英文环境，会显示原 \u0026ldquo;Chinese Test\u0026rdquo; 的内容；♈切换到中文环境，则该文章消失，因为对应的中文文件 (index.zh-cn.md) 已不存在。说明 Hugo 的国际化实现相对简单，只需复制文件并修改语言代码即可。\npart10: 创建与编辑新内容 (09:29-09:57) 10.1: 创建新文章命令 (hugo new content) 演示如何使用命令行创建新文章。命令格式为 hugo new content \u0026lt;section\u0026gt;/\u0026lt;slug\u0026gt;/index.md。其中 \u0026lt;section\u0026gt; 通常是 content 目录下的子目录（如 post），\u0026lt;slug\u0026gt; 是文章的唯一标识（会成为文件夹名），index.md 是内容文件名。 ♈作者执行了命令 hugo new content post/myFirstBlog/index.md，成功创建了一个名为 myFirstBlog 的文章。\n(09:57-10:15) 10.2: 编辑新文章内容 (Markdown) 使用 Typora 编辑器打开新创建的 dev/content/post/myFirstBlog/index.md 文件。文件自动包含了基础的 front matter（如 title, date, draft: true）。在文件末尾添加了一行 Markdown 内容：### Hello World!。保存文件。\n(10:15-10:31) 10.3: 预览新文章 刷新本地预览页面 (http://localhost:1313)。新创建的 \u0026ldquo;MyFirstBlog\u0026rdquo; 文章出现在首页列表。点击进入该文章，可以看到刚才添加的 \u0026ldquo;Hello World!\u0026rdquo; 标题。这表明文章已成功创建并显示。\npart11: 本地搭建完成与后续步骤 (10:31-10:38) 11.1: 本地搭建总结 至此，使用 Hugo 和 Stack 主题在本地搭建博客的基本流程已经完成。\n(10:38-10:43) 11.2: 后续内容预告 说明接下来的步骤将涉及♐修改配置 hugo.yaml【♐至于hugo.yaml有哪些配置 一般要么样式文件里面的hugo.yaml会讲在哪里找，要么样式文件里面的hugo.yaml已经把全部配置给你了】以及如何将博客部署到 GitHub Pages。\n","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0hugo%E5%8D%9A%E5%AE%A2/","title":"搭建本地hugo博客"},{"content":" Cloudflare Pages 连接到 hugo-Fixlt-dev 仓库时，♈Cloudflare Pages 默认是直接托管该仓库中的 静态文件 ，♈它 不会 重新运行 hugo 构建命令。\n因此，♈你在 Cloudflare Pages 设置的环境变量 HUGO_BASEURL 在这个流程中 根本没有机会 被 Hugo 构建过程使用，因为它是在 Action（ hugo-main 仓库中）构建完成 之后 才接触到这些文件的。♈静态文件一旦生成，里面的绝对 URL 就已经固定了\n比如，♈GitHub Action 在构建时使用了固定的 baseURL （例如 GitHub Pages 的 URL），生成的 HTML/CSS/JS 文件中的链接都是指向这个 URL 的绝对路径。\n因此当 Cloudflare Pages 直接提供这些预先构建好的文件时，这些绝对路径对于 Cloudflare 的域名来说是错误的，导致资源加载失败\n","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%8F%AA%E6%9C%89%E5%9C%A8%E9%A1%B9%E7%9B%AE%E7%9A%84%E6%9E%84%E5%BB%BA%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%E4%BD%BF%E7%94%A8/","title":"环境变量只有在项目的构建的过程中使用"},{"content":"用AI独立开发MCP服务的集合 生成MCP服务 生成调用本地项目的MCP服务 https://aistudio.google.com/prompts/1yTmovHhdhAThW5Fzigr_4hYlBB8GEaLM\n- [1 如何AI编程开发MCP] - 用AI开发MCP服务 这里用对应的提示词让AI客户端生成满足自己需求的本地MCP服务代码 - （markdown格式 、简单描述MCP服务要实现的功能 、 告诉AI 本地项目的路径、 告诉AI 要激活哪个虚拟环境、 告诉AI要设置的 模型的key）、 - （把本地项目的全部代码直接告诉AI、告诉AI 项目运行后的响应情况 也就是直接把终端的完整内容复制给AI，这个终端的完整内容包含程序的运行命令和项目运行效果） - （ 给AI详细的MCP文档（告诉AI MCP是什么） 、 应该可以让AI生成MCP的标准配置标准配置的对应的字段和参数我还是了解的 ） - 【2 案例 创建MCP服务TimeServer的提示词】\thttps://aistudio.google.com/prompts/1yTmovHhdhAThW5Fzigr_4hYlBB8GEaLM - 提示词里面需要MCP的哪些参考资料，这里给了链接，直接在链接里面粘贴内容过来 ","date":"2025-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8ai%E7%8B%AC%E7%AB%8B%E5%BC%80%E5%8F%91mcp%E6%9C%8D%E5%8A%A1-%E9%9B%86%E5%90%88/","title":"用AI独立开发MCP服务 集合"},{"content":"༼ ༽\n","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/","title":""},{"content":"༼ CPU中寄存器的原理༽\npart1: 寄存器与内存的对比及寄存器原理引入 (00:00-00:17) 1.1: CPU寄存器介绍\n视频开头展示了一个Intel Core i5-7500 CPU。 提到CPU内部有用于存储数据的装置，称为寄存器 (Register)。 比较寄存器和内存：寄存器容量很小，但拥有惊人的读写速度。 提出问题：寄存器的原理是什么？为什么它比内存快这么多？ part2: 组合逻辑电路的局限性 (00:17-00:39) 2.1: 无记忆功能的电路示例\n日常生活中接触的电路（如灯）大部分不具备存储信息的功能。 演示一个简单的灯泡电路：包含电池、开关和灯泡。 电路特点：开关闭合灯亮，开关断开灯灭。灯的状态只取决于当前开关的状态。 这种电路被称为组合电路 (Combinational Logic)，其输出仅与当前输入有关，与过去状态无关。 part3: 时序逻辑电路与记忆功能的需求 (00:39-00:53) 3.1: 组合电路无法满足计算机需求\n指出仅有组合电路是无法做电脑的。 解释原因：电脑的状态不仅取决于你当前的操作，还与以前的操作有关（需要记忆）。 提出问题：如何让电路拥有记忆的功能？ part4: “自我提醒机制”——记忆的类比与实现思路 (00:53-01:44) 4.1: 生活中的记忆类比\n使用一个生活场景类比记忆：妻子让丈夫去买东西（一袋盐，五斤苹果），丈夫需要记住这个信息。 画面内容： 女士（cosplay角色）说：“你去买一袋盐，五斤苹果”。男士问：“你怎么不去啊？” 女士回答：“我要去漫展找小哥哥玩呀！” 男士竖起大拇指：“合理”。 丈夫通过在路上不停地对自己重复要买的东西（五斤苹果，一袋盐）来刷新自己的记忆，防止忘记。 将这种方式称为“自我提醒机制 (Self Reminder Mechanism)”。 这个机制将在设计存储电路时发挥决定性作用。 part5: 基础逻辑门与简单的反馈电路 (01:44-02:44) 5.1: 非门 (NOT Gate) 介绍\n介绍非门（逻辑符号：一个三角形加一个小圆圈）。 功能：可以将输入的信号进行反转。 示例：输入高电平，输出低电平；输入低电平，输出高电平。 (02:04-02:28) 5.2: 单非门反馈电路 (振荡器) 引入“自我提醒机制”思路：将非门的输出接回到输入。 演示结果：信号在高低电平之间循环往复地变化（振荡）。 画面内容： 假设初始输入为低电平，输出变高电平，高电平反馈回输入，输出变低电平，低电平再反馈，形成持续振荡。 调侃：称其为“永动机”。 (02:28-02:44) 5.3: 澄清“永动机”与门电路供电 说明这不是永动机。 解释：通常看到的门电路接线只是信号线，实际门电路还需要连接电源线（电源正极 VDD 和 电源负极 VSS）。 高低电平变化的动力其实来自于电源线。 part6: 双稳态电路的构建与特性 (02:44-03:11) 6.1: 单非门反馈的问题与改进思路\n指出单非门反馈电路无法存储数据，因为它一直在振荡。 提出改进思路：将“光速”（信号传播速度）调慢（作为一种比喻），并在反馈路径上增加门电路。 (03:11-04:09) 6.2: 双非门反馈电路 (Bistable Circuit) 演示将信号传播速度调慢后，在合适时机再加入一个非门，形成两个非门互相反馈的环路。 分析： 如果上路输出为高电平，下路输入为高，输出为低，低电平反馈回上路输入，上路输出维持高电平 -\u0026gt; 稳定状态1。 如果上路输出为低电平，下路输入为低，输出为高，高电平反馈回上路输入，上路输出维持低电平 -\u0026gt; 稳定状态2。 结论：这个由两个非门构成的电路具有两种稳定状态，可以用来存储1 bit的数据（例如，一种状态代表1，另一种代表0）。 这种电路被称为双稳态电路 (Bistable Circuit)。 (04:09-04:28) 6.3: 双稳态电路的问题 目前这种双稳态电路虽然能稳定存储状态，但存在一个问题：状态无法被外部修改。不能简单地在特定时机插入非门来改变状态。 part7: 引入控制——锁存器的形成 (04:28-04:54) 7.1: 或门 (OR Gate) 介绍\n引入另一种门电路：或门。 符号：类似盾牌的形状。 有两个输入，一个输出。 特性：只要有一个输入是高电平，输出就是高电平。只有当两个输入都是低电平时，输出才是低电平。 (04:54-06:12) 7.2: 构建可控的存储电路 (SR锁存器雏形) 将两个或门加入到之前的双稳态电路结构中，形成新的电路。 引入两个外部输入：Input A 和 Input B。 演示写入操作 (Set/Reset)： 当 Input A 为低电平，Input B 为高电平时： 下方的或门由于 Input B 为高，其输出强制为高电平。 这个高电平经过非门变成低电平。 这个低电平反馈到上方或门的输入。 由于 Input A 和反馈信号都为低电平，上方或门输出为低电平。 低电平再经过非门变成高电平，反馈到下方或门。 此时，电路状态被强制设定并稳定下来。Input B 的高电平打破了原有的双稳态机制。 (06:12-06:50) 7.3: 写入与保持 (Latch) 功能演示 演示当 Input A 为高电平，Input B 为低电平时的写入操作，结果相反，电路状态被设定为另一种稳定状态。 演示保持 (Hold/Latch) 功能： 当 Input A 和 Input B 都设置为低电平时 (锁存状态 Latch State): 或门的作用相当于不存在（输出取决于另一个反馈输入）。 电路恢复到之前的双稳态结构。 此时电路会保持 (维持) 上一次写入操作设定的状态。 结论：电路记住了过去的状态，拥有了记忆的能力。 (06:50-08:16) 7.4: 锁存器的基本功能总结 再次演示从写入状态转换到保持状态，状态得以维持。 总结该电路（锁存器 Latch）具备的三个存储电路最基本的功能： 写入 (Write)：通过设置 Input A 和 Input B (一个高一个低) 来改变存储状态。 保持 (Hold)：通过将 Input A 和 Input B 都设为低电平来维持当前状态。 读取 (Read)：通过引出导线测量电路内部某个节点的电平来获取存储的数据。 part8: 锁存器的非法输入与实际问题 (08:16-08:38) 8.1: 探讨非法输入组合\n提出疑问：如果 Input A 和 Input B 都是高电平会发生什么？ (08:38-09:47) 8.2: 非法输入 (Input A=High, Input B=High) 的后果 分析：当 Input A 和 B 都为高电平时，两个或门输出都为高电平，经过非门后，两个非门的输出都为低电平。 问题：如果此时同时将 Input A 和 B 变回低电平（进入保持状态），两个非门的输出理论上都应变为高电平。但由于实际电路中门电路响应存在延迟（不是理想的瞬时响应），哪个非门先变高是不确定的。 后果： 可能导致电路状态不确定 (Indeterminate State)。 可能导致电路振荡 (癫狂)。 可能因中间状态导致大量发热，甚至烧毁电路。 结论：Input A 和 B 同时为高电平是一种非法输入或不确定后果的输入，在实际使用中应当避免。 强调理想模型与现实的区别：理想模型中门电路响应是瞬时的，现实中存在延迟。 part9: 电路简化与展望 (09:47-10:31) 9.1: 或非门 (NOR Gate) 与电路简化\n展示或门和非门的内部结构（基于晶体管）。 指出或门 + 非门的组合称为或非门 (NOR Gate)，并展示其符号。 指出或非门的后半部分包含一个非门，而电路中另一个独立的部分也是一个非门。 应用负负得正原理：连续两个非门可以抵消。 结果：可以将原来的电路（两个或门 + 两个非门）简化为仅由两个或非门交叉耦合构成的等效电路。 这个简化后的电路图就是课本上常见的SR锁存器（由或非门构成）。 (10:31-10:48) 9.2: 锁存器与寄存器的关系 提出问题：这个锁存器就能当作寄存器用了吗？ 回答：还不行，还缺少东西。 (10:48-11:16) 9.3: 下期预告 预告下期内容： 如何避免锁存器的非法输入。 真实的运算电路中电压的波形。 什么是边沿触发 (Edge Trigger)。 计算机中的 L1、L2、L3 缓存 (Cache) 与寄存器的电路是否一致，区别何在。 part10: 片尾彩蛋 (11:16-11:38) 10.1: 购物后续\n视频结尾，男士回到家，放下了购物成果。 画面内容： 男士放下一袋苹果和五袋盐，说：“一袋苹果，五斤盐”。 女士（cosplay角色）看着桌子上的东西，露出困惑/无语的表情，问男士：“你要这么多盐干嘛？”。 （呼应前面的记忆类比，暗示丈夫记混了数量和单位）。 ","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/","title":""},{"content":"༼ ༽\n","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/","title":""},{"content":"༼ 🚀挑战Gemini 2.5！最强开源企业级OCR大模型InternVL3！本地部署教程+实战测评全纪录，轻松搞定潦草手写汉字、模糊PDF扫描件、模糊复杂表格，效果炸裂超过人眼！支持Open WebUI༽\npart1: 环境准备与依赖安装 好的，将一个本地运行的模型API（比如在 http://127.0.0.1:5000 上运行的服务）接入 Gradio 界面是常见的做法。Gradio 在这种情况下扮演的是前端用户界面的角色，而你的本地API则是后端模型服务。\n你需要做的核心事情是在 Gradio 的处理函数 (function) 中，使用 Python 的 HTTP 请求库（最常用的是 requests）去调用你本地的 API，然后将 API 返回的结果展示在 Gradio 的输出组件中。\n以下是接入的基本步骤和示例：\n1. 安装必要的库:\n1 pip install gradio requests 2. 了解你的本地API:\n在编写 Gradio 代码之前，你必须清楚你的本地 API (http://127.0.0.1:5000) 的以下细节：\n确切的端点 (Endpoint Path): API 的具体路径是什么？例如，是 /predict, /generate, /chat/completions 还是其他？ HTTP 方法 (Method): 调用这个端点需要使用 GET 还是 POST 方法？（通常，如果需要发送数据给模型，会使用 POST）。 请求格式 (Request Format): 调用 API 时，输入数据应该以什么格式发送？ 是 JSON 格式吗？如果是，JSON 的结构是怎样的？例如 {\u0026quot;prompt\u0026quot;: \u0026quot;用户输入\u0026quot;} 或 {\u0026quot;image\u0026quot;: \u0026quot;base64编码的图片数据\u0026quot;}？ 是表单数据 (Form Data) 吗？ 响应格式 (Response Format): API 成功处理请求后，返回的数据是什么格式？ 通常也是 JSON 格式。JSON 的结构是怎样的？你需要从中提取哪个字段作为最终结果？例如 {\u0026quot;result\u0026quot;: \u0026quot;模型输出\u0026quot;} 或 {\u0026quot;label\u0026quot;: \u0026quot;分类结果\u0026quot;, \u0026quot;confidence\u0026quot;: 0.9}？ 3. 编写 Gradio 代码:\n下面是几个不同场景的示例：\n示例 1：简单的文本输入 -\u0026gt; 文本输出 API\n假设你的 API 在 http://127.0.0.1:5000/generate，接受 POST 请求，请求体是 {\u0026quot;prompt\u0026quot;: \u0026quot;用户输入文本\u0026quot;}，响应体是 {\u0026quot;response\u0026quot;: \u0026quot;模型生成文本\u0026quot;}。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import gradio as gr import requests import json # 你本地API的URL和端点 API_URL = \u0026#34;http://127.0.0.1:5000/generate\u0026#34; # 假设端点是 /generate def call_model_api(input_text): \u0026#34;\u0026#34;\u0026#34; 这个函数会被Gradio调用，它负责调用本地API。 \u0026#34;\u0026#34;\u0026#34; payload = {\u0026#34;prompt\u0026#34;: input_text} # 构建API需要的请求体 headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} try: # 发送POST请求到本地API response = requests.post(API_URL, headers=headers, json=payload) response.raise_for_status() # 如果API返回错误状态码 (如 4xx, 5xx)，则抛出异常 # 解析API返回的JSON响应 result = response.json() # 从响应中提取模型输出 (根据你的API实际返回格式调整) model_output = result.get(\u0026#34;response\u0026#34;, \u0026#34;API did not return \u0026#39;response\u0026#39; field.\u0026#34;) # 获取 \u0026#39;response\u0026#39; 字段 return model_output except requests.exceptions.RequestException as e: print(f\u0026#34;Error calling API: {e}\u0026#34;) return f\u0026#34;Error connecting to the API: {e}\u0026#34; except json.JSONDecodeError: print(f\u0026#34;Error decoding API response: {response.text}\u0026#34;) return \u0026#34;Error: Could not decode API response.\u0026#34; except Exception as e: print(f\u0026#34;An unexpected error occurred: {e}\u0026#34;) return f\u0026#34;An unexpected error occurred: {e}\u0026#34; # 创建 Gradio 界面 # 输入是文本框，输出也是文本框 iface = gr.Interface( fn=call_model_api, # 指定处理函数 inputs=gr.Textbox(lines=2, placeholder=\u0026#34;Enter your prompt here...\u0026#34;), # 输入组件 outputs=gr.Textbox(label=\u0026#34;Model Response\u0026#34;), # 输出组件 title=\u0026#34;Local Model API Interface\u0026#34;, description=\u0026#34;Enter text and get a response from the local model API running at \u0026#34; + API_URL ) # 启动 Gradio 应用 iface.launch() print(f\u0026#34;Gradio interface running. Access it in your browser.\u0026#34;) print(f\u0026#34;Make sure your local API is running at {API_URL}\u0026#34;) 示例 2：图片输入 -\u0026gt; 文本输出 API\n假设你的 API 在 http://127.0.0.1:5000/describe_image，接受 POST 请求，请求体是 {\u0026quot;image\u0026quot;: \u0026quot;base64编码的图片数据\u0026quot;}，响应体是 {\u0026quot;description\u0026quot;: \u0026quot;图片描述\u0026quot;}。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 import gradio as gr import requests import base64 from PIL import Image import io import json API_URL = \u0026#34;http://127.0.0.1:5000/describe_image\u0026#34; def call_image_api(input_image): \u0026#34;\u0026#34;\u0026#34; 处理图片输入并调用API input_image 是 Gradio 传递过来的 PIL Image 对象 或 NumPy array \u0026#34;\u0026#34;\u0026#34; if input_image is None: return \u0026#34;Please upload an image.\u0026#34; try: # 将Gradio的输入图片 (通常是PIL Image或numpy array) 转换为 bytes # 如果输入是 NumPy array，先转为 PIL Image if not isinstance(input_image, Image.Image): input_image = Image.fromarray(input_image) buffered = io.BytesIO() input_image.save(buffered, format=\u0026#34;JPEG\u0026#34;) # 或者 PNG, 根据你的模型API偏好 img_bytes = buffered.getvalue() # 将图片bytes进行Base64编码 img_base64 = base64.b64encode(img_bytes).decode(\u0026#39;utf-8\u0026#39;) # 构建API请求体 payload = {\u0026#34;image\u0026#34;: img_base64} headers = {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;} # 调用API response = requests.post(API_URL, headers=headers, json=payload) response.raise_for_status() result = response.json() # 提取结果 description = result.get(\u0026#34;description\u0026#34;, \u0026#34;API did not return \u0026#39;description\u0026#39; field.\u0026#34;) return description except requests.exceptions.RequestException as e: return f\u0026#34;Error calling API: {e}\u0026#34; except Exception as e: return f\u0026#34;An error occurred: {e}\u0026#34; # 创建 Gradio 界面 iface = gr.Interface( fn=call_image_api, inputs=gr.Image(type=\u0026#34;pil\u0026#34;), # 输入是图片上传组件, type=\u0026#34;pil\u0026#34; 表示接收PIL Image对象 outputs=gr.Textbox(label=\u0026#34;Image Description\u0026#34;), # 输出是文本框 title=\u0026#34;Image Description API\u0026#34;, description=\u0026#34;Upload an image to get its description from the local API.\u0026#34; ) iface.launch() 关键点总结:\n核心是 requests 库: Gradio 函数内部使用 requests.post(...) 或 requests.get(...) 来与你的本地 API 通信。 理解你的 API 是前提: 你必须知道 API 的 URL、端点、方法、请求和响应格式，才能正确地构建 payload 和解析 response。 数据格式转换: Gradio 组件的输入/输出类型（如 gr.Textbox, gr.Image(type=\u0026quot;pil\u0026quot;)）需要和你调用 API 时所需的数据格式进行匹配或转换（例如，将 PIL Image 转换为 Base64 字符串）。 错误处理: 务必添加 try...except 块来处理网络连接错误、API 返回错误或数据解析错误，给用户友好的反馈。 启动顺序: 先确保你的本地模型 API (127.0.0.1:5000) 已经成功运行，然后再启动 Gradio 脚本。 通过这种方式，Gradio 可以非常灵活地接入任何符合标准 HTTP 协议的本地或远程模型 API。。\n","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/api%E7%9A%84%E5%89%8D%E7%AB%AF%E7%95%8C%E9%9D%A2gradio%E7%95%8C%E9%9D%A2/","title":"API的前端界面：Gradio界面"},{"content":"༼🚀企业级最强开源大模型Qwen3震撼发布！本地部署+全面客观测评！Qwen3-235B-A22B+Qwen3-32B+Qwen3-14B谁是最强王者？ollama+LM Studio+vLLM本地部署 ༽\npart1: InternVL3 OCR能力测试 - 手写体识别 好的，这是视频中对Qwen3系列模型测试结果的总结：\nPart 1: Qwen3 模型基础能力与幻觉测试 (04:02-04:30) 1.1: 知识库截止日期测试\n测试类型：知识库时效性。 结果： Qwen3-235B-A22B 的知识库截止日期为 2024年6月。 Qwen3-32B 的知识库截止日期到 2024年10月。 Qwen3-14B 的知识库截止日期到 2024年10月。 作者观点: 三款模型的知识都比较新。 (04:30-06:33) 1.2: 幻觉测试（虚构信息辨别）\n测试类型：模型对明确虚构或不存在信息的辨别能力。 数字大小比较 (2.999 vs 2.12345) 英文单词字母统计 (stresslessness) 虚构人物（量子信息学家马克·威尔逊教授） 虚构化学物质（九氧化二氮） 虚构历史事件（公元前235年秦国与齐国的“稷下之盟”） 虚构物理理论（反熵矩阵理论） 虚构文学事件（狄更斯和海明威在巴黎辩论） 结果： 数字比较：三款模型（235B, 32B, 14B）均正确比较。 字母统计：三款模型（235B, 32B, 14B）均正确统计。 虚构人物/化学物质/物理理论/文学事件：三款模型（235B, 32B, 14B）基本都能正确识别出这些是虚构的，没有直接产生幻觉进行编造。235B和32B表现很好，14B（开启思考模式后）也能识别。 虚构历史事件： Qwen3-235B-A22B：正确指出该事件没有明确记载。 Qwen3-32B：产生了幻觉，编造了“稷下之盟”的细节内容，这是错误的。 Qwen3-14B（开启思考模式）：正确指出该事件不存在。 作者观点: Qwen3在辨别虚构信息方面表现不错，尤其是235B和开启思考模式的14B。但32B在虚构历史事件上出现了明显幻觉。 Part 2: Qwen3 模型逻辑推理与遵循指令能力测试 (07:02-08:15) 2.1: 多维模式识别与逻辑推理测试\n测试类型：根据规则填充矩阵、寻找序列规律（字母、数字、二进制）。 矩阵填充 字母序列 (OTTFFSSEN\u0026hellip;) 交替数字序列 (3, 5, 6, 10, 9\u0026hellip;) 二进制序列 (1, 2, 4, 8\u0026hellip;) 结果： 矩阵填充： Qwen3-235B-A22B：错误，未能解决。 Qwen3-32B：错误，未能解决。 Qwen3-14B（开启思考模式）：正确解决。 序列规律：三款模型（235B, 32B, 14B）对于字母、交替数字、二进制序列均能正确找出规律并补全。 作者观点: 在矩阵逻辑题上，14B开启思考模式后表现优于两个更大的模型。但在序列规律题上，三者表现都很好。 (08:15-08:30) 2.2: JSON结构化输出能力测试\n测试类型：根据给定信息生成包含订单处理逻辑的JSON结构。 输入指令：请根据以下信息，生成一个包含订单处理逻辑的JSON结构：[包含客户信息、购物车商品、优惠券、物流选项、支付方式等信息]\u0026hellip;请计算最终订单金额\u0026hellip; 结果： Qwen3-235B-A22B：生成的JSON格式正确，但折扣计算顺序出错，导致最终金额有误差。 Qwen3-32B：生成的JSON格式正确，但出现的错误较多。 Qwen3-14B（开启思考模式）：生成的JSON格式和计算结果均正确。 作者观点: 14B开启思考模式后表现最好，235B次之，32B错误较多。 (09:01-09:35) 2.3: 混合格式信息提取与结构化输出测试\n测试类型：将包含CSV、XML、普通文本的混合信息转换为统一的JSON格式。 输入指令：请将以下混合信息转换为统一的JSON格式：[包含CSV格式的产品信息、XML格式的供应商信息、普通文本的促销活动信息] 结果：三款模型（235B, 32B, 14B）均能成功将混合格式信息转换为正确的JSON格式。 作者观点: 在这个混合格式转换任务上，三款模型表现都非常不错。 (09:35-10:18) 2.4: 代码理解与SVG格式生成测试\n测试类型：根据Python冒泡排序代码生成SVG格式的流程图。 输入指令：为下面的代码用SVG画出完整的流程图：[Python冒泡排序代码]\u0026hellip; 结果： 三款模型（235B, 32B, 14B）均生成了SVG代码。 但是，所有模型生成的SVG代码在浏览器中打开时均出现报错，无法正确渲染成流程图。 作者观点: 三款模型在生成可用的SVG流程图方面都失败了。 (10:18-12:57) 2.5: 农夫过河问题（复杂逻辑推理）\n测试类型：解决经典的农夫过河逻辑谜题。 输入指令：农夫带着一只老虎、一只羊、一条蛇、一只鸡和一筐苹果要过河\u0026hellip;[详细规则]\u0026hellip;请问农夫如何才能将老虎、羊、蛇、鸡和苹果安全送到对岸？ 结果： Qwen3-235B-A22B：失败。在第五步“农夫带鸡过河”后，分析安全性时产生幻觉，错误地认为“老虎会阻止蛇吃羊”，而规则中蛇不吃羊，导致后续步骤错误。 Qwen3-32B：失败。在第一步就出错，选择了带鸡过河，违反了鸡会阻止老虎吃羊的规则（虽然原题规则是鸡阻止蛇吃鸡，但此处模型理解或应用规则错误）。 Qwen3-14B（开启思考模式）：失败。步骤给出的看似合理，但在第七步“带羊过河”后，对岸状态分析错误，认为有老虎、羊、蛇，此时老虎会吃羊，违反安全规则。 作者观点: 这个经典的农夫过河问题难倒了所有三款Qwen3模型，它们在多步复杂逻辑推理和状态跟踪方面存在明显不足，容易产生幻觉或违反规则。 (12:58-13:36) 2.6: 文档分析能力测试\n测试类型：基于上传的PDF文档（Phi-4-Mini技术报告）回答问题。 输入指令：[上传PDF后提问] Phi-4-Mini具有多少个Transformer层？隐藏状态大小是多少？它使用的GQA配置具体包含多少个查询头和键/值头？\u0026hellip; 结果： Qwen3-235B-A22B：正确回答了所有问题。 Qwen3-32B：部分正确，前两个问题回答正确，第三个问题（得分提高百分点）回答错误。 Qwen3-14B（通过Open WebUI测试）：失败，未能从文档中找到答案。 作者观点: 235B表现最好。14B的失败可能是由于Open WebUI文档解析功能的问题，不完全代表模型本身的能力。 (13:36-14:17) 2.7: 算法与编程能力测试\n测试类型：用Python编程实现计算第N个质数，要求不引入外部库。 算法题：计算438990637是第多少个质数。 编程题：设计一个2D物理模拟系统（涉及碰撞、旋转等）。 结果（算法题）： Qwen3-235B-A22B：给出的Python代码只做了基本的优化，整体效率较低。 Qwen3-32B：给出的代码采用了更高效的方法，通过存储已找到的质数来加速判断。 Qwen3-14B（开启思考模式）：给出的代码优化程度最高，比32B的还要好一些，例如在找到目标后能退出循环。 结果（编程题 - 2D物理模拟）： Qwen3-235B-A22B：生成的Pygame代码无法一次性运行，运行时闪退且没有内容，代码存在错误（如 \u0026lsquo;BLACK\u0026rsquo; is not defined）。 Qwen3-32B：生成的代码同样无法一次性运行，运行时闪退。 Qwen3-14B（开启思考模式）：生成的代码能够运行，并展示了两个六边形和一个移动的小球，但未完全实现题目要求的所有物理特性（如交叉区域弹跳）。 作者观点: 在算法题上，14B和32B的代码优化能力优于235B。在编程题上，只有14B生成的代码能基本运行，但功能不完善，而235B和32B生成的代码都无法直接运行。 (10:18-10:56) 2.8: 提示词遵循能力与负责问题分析能力测试\n测试类型：模型是否能严格按照一个复杂的、包含多步骤和格式要求的提示词（Prompt）来分析和回答问题。 输入指令：你现在是一名超级思维解析师\u0026hellip;[详细的多步骤分析要求，包括解析流程规范、使用特定符号、探索多种答案、明确能力边界、审视推理过程、应用最佳实践、JSON格式输出等]\u0026hellip;下面是你要解决的问题：[抛硬币概率问题]。 结果： 三款模型（235B, 32B, 14B）均未能遵循复杂的提示词指令。它们都只给出了第一步（输出包含“步骤”、“标题”、“内容”、“下一步行动”的JSON），而没有继续执行提示词中要求的后续分析步骤。 作者观点: 这个测试表明，Qwen3系列模型在严格遵循复杂指令（Prompt Following）方面存在显著的短板，这是一个较大的弱点。 (10:57-11:48) 2.9: SQL能力测试\n测试类型：根据给定的数据库表结构（产品表、供应商表、客户表、采购订单表、销售订单表、库存交易表等）和5个具体问题，生成相应的SQL查询语句。 结果： Qwen3-235B-A22B：生成的5个SQL查询语句全部正确。 Qwen3-32B：第3个问题（供应商绩效）使用的**DATEDIFF函数语法是SQL Server特有的**，而非通用的或MySQL的语法，其他SQL语句正确。 Qwen3-14B（开启思考模式）：生成的5个SQL查询语句全部正确。 作者观点: 235B和14B的SQL能力非常强，32B稍有不足（使用了特定数据库方言）。 Part 5: 总体评价 (15:19-15:38) 5.1: 作者总结\n作者观点: 通过多方面的测试，可以发现Qwen3系列模型在多方面的能力都有了很大的提升。但是，在复杂逻辑推理（如农夫过河）、严格遵循提示词指令以及部分代码生成（尤其是SVG）方面仍有不足。14B参数的模型在开启思考模式后，其效果常常能媲美甚至超过不开启思考模式的32B模型，有时甚至比235B模型效果还好一些（例如在JSON准确性、算法优化方面）。用户可以根据自己的项目需求和对模型能力侧重的不同，来选择不同参数的版本。 ","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/qwen3%E6%A8%A1%E5%9E%8B%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E5%8C%85%E5%90%AB%E8%83%BD%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E7%9A%84%E5%B0%8F%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B/","title":"Qwen3模型测试结果（包含能本地部署的小参数模型）"},{"content":"༼🚀挑战Gemini 2.5！最强开源企业级OCR大模型InternVL3！本地部署教程+实战测评全纪录，轻松搞定潦草手写汉字、模糊PDF扫描件、模糊复杂表格，效果炸裂超过人眼！支持Open WebUI ༽\npart1: InternVL3 OCR能力测试 - 手写体识别 #模型的UI界面\n模型的UI界面用的是openWebUi (03:34-04:43) 1.1: 测试1 - 手写体识别 在OpenWebUI中选择部署好的OpenGVLab/InternVL3-14B-Instruct模型进行测试。 上传一张手写体图片（内容为一首诗）。 输入指令：“提取图上的文字内容, 并保持原有格式输出”。 测试结果：模型很快提取出了图片中的手写文字内容，包括诗的标题“暮春感怀”和最后的签名“AI超元域-书”。 (04:43-05:20) 1.2: 手写体识别结果比对与评价 将模型提取的文字内容与原图进行逐行比对。 测试结果：\n所有诗句内容均完全正确提取。 诗名后的破折号 (\u0026ndash;) 和签名后的点号 (.) 也被正确识别并提取。 作者观点：对于如此潦草的手写体，包括标点符号都能正确提取，效果非常好。之前测试中只有Gemini 2.5 Pro能达到这个效果。 (05:47-07:06) 1.3: Gradio界面资源监控下的手写体识别测试 使用作者编写的Gradio脚本来实时监控模型推理时的系统资源（CPU、内存、GPU显存）占用情况。 在Gradio界面中再次上传之前的手写体图片。 输入指令：“提取图上的文字内容, 并保持原有格式输出”。 测试结果：\n模型再次成功提取了手写内容。 资源监控显示：CPU使用率有波动，内存使用约2.53 GB（初始）上升至处理时的峰值，GPU显存占用高达45.71 / 47.99 GB (95.2%)（使用单张A6000显卡运行InternVL3-14B模型）。 作者观点： 14B参数的模型显存占用较高。 part2: InternVL3 OCR能力测试 - 发票与重叠文字识别 (07:06-08:04) 2.1: 测试2 - 发票扫描件识别（含模糊与重叠文字） 上传一张发票扫描件图片，图片中包含模糊文字和重叠文字。 输入指令：“提取图上的文字内容, 并保持原有格式输出”。 测试结果：\n模型将提取内容输出到code block中。 发票顶部的英文公司信息和地址完全正确提取。 发票号码、日期、付款条件、到期日等信息完全正确提取。 客户信息（公司名、联系人、地址、电话、客户编号）完全正确提取。 项目明细部分： \u0026ldquo;云服务器租用 / Cloud Server Hosting\u0026rdquo; 行：完全正确。 \u0026ldquo;企业数据备份服务 / Data Backup Service\u0026rdquo; 行：价格部分存在重叠文字（¥3,500.00与$900.00重叠），模型正确识别并提取为 ¥3,500.00，数量“1项”也正确。 \u0026ldquo;技术支持 / Technical Support\u0026rdquo; 行：价格部分存在重叠文字（¥19,200.00与¥800.00重叠），模型正确识别并提取为 ¥19,200.00，数量“24小时”也正确。 \u0026ldquo;软件许可证 / Software License\u0026rdquo; 行：完全正确。 \u0026ldquo;安全审计服务 / Security Audit\u0026rdquo; 行：完全正确。 小计、税额、总计金额完全正确提取。 付款方式、银行转账、账户名、银行、账号、Swift代码等信息完全正确提取。 唯一识别错误：在重叠的 \u0026ldquo;Data Backup Service\u0026rdquo; 价格处，模型输出为¥3,500.00，原始应为¥3,500.00，识别出了重叠内容但未能完全还原格式（原图重叠了$900和¥3500）。 修正：仔细观察视频，应是¥3,500.00与$9,560.00重叠，模型识别为¥3,500.00，识别是正确的。 作者观点：效果非常不错，特别是能处理重叠文字的情况。 part3: InternVL3 OCR能力测试 - 模糊PDF扫描件识别（含表格和代码） (08:04-09:03) 3.1: 测试3 - 模糊PDF扫描件识别 上传一张非常模糊的PDF扫描件图片，包含普通文本、一个表格和一段代码。 输入指令：“提取图上的文字内容, 并保持原有格式输出”。 测试结果：\n文档标题“AI超元域原创视频-文档扫描测试”正确提取。 开头的英文段落 (Lorem ipsum\u0026hellip;) 正确提取。 中间的中文段落及三个要点（第一点、第二点、第三点）均正确提取，并保持了原有的格式。 表格内容（包括表头“项目”、“描述”、“状态”及三行测试数据）完全正确提取。 底部的非常模糊的代码段 (function testAIVision()\u0026hellip;) 也完全正确地提取出来，并保持了原有的代码格式。 作者观点：即使文档非常模糊，模型依然能够准确提取文本、表格和代码。 part4: InternVL3 OCR能力测试 - 复杂模糊PDF扫描件识别（含噪声） (09:04-10:11) 4.1: 测试4 - 带噪声的复杂模糊PDF扫描件识别 上传一张更加模糊且复杂的PDF扫描件，包含标题、文本和一个带有人为添加的红色线条噪声的表格。 输入指令：“提取图上的文字内容, 并保持原有格式输出”。 测试结果：\n报告标题“研究报告：人工智能在现代商业中的应用”和副标题“3.2 行业应用案例分析”正确提取。 第一段中文长文本内容正确提取。 包含人为噪声的表格内容完全正确提取，模型成功忽略了噪声。其中“医疗健康”行，“投资回报率”列的数值128.7，即使带有噪声干扰，也正确提取。 表格下方的引用说明文字“*人工智能的商业价值\u0026hellip;”基本正确提取，但遗漏了中间非常模糊的一小段文字（“各行业AI技术采用及投资回报分析\u0026hellip;”）。 图例说明“图7: AI技术投资与业务绩效相关性分析”正确提取。 最下方的“AI商业应用研究系列\u0026hellip;”正确提取。 作者观点：非常难得，模型不仅处理了模糊性，还有效忽略了表格中的噪声，提取结果非常准确。 part5: InternVL3 OCR能力测试 - 复杂模糊表格扫描件识别 (10:11-10:50) 5.1: 测试5 - 复杂模糊表格扫描件识别 上传一张包含非常模糊且复杂表格的扫描件图片，表格中部分文字超出了单元格边界。 输入指令：“提取图上的文字内容, 并保持原有格式输出”。 测试结果：\n模型使用Markdown格式输出了表格内容。 经过仔细比对，表格的所有内容，包括那些超出单元格的文字，都被完全正确地提取出来。 表格下方的表注“表1: AI超元域2024-2025年度项目进度表”和页码信息“第1页，共5页”也完全正确提取。 作者观点：即使表格非常模糊和复杂，提取结果依然完全正确。 part6: InternVL3 OCR能力测试 - 图像理解与描述 (10:50-11:27) 6.1: 测试6 - 线条画（Meme图）描述 上传一张线条画（Meme图）图片。 输入指令：“描述这张图”。 测试结果：模型正确识别出这是一幅线条画 (line drawing)，描绘了一个聚会场景 (party scene)，并准确描述了图中五个人物各自的位置和动作状态。 作者观点：对图像内容的描述基本正确。\npart7: InternVL3 OCR能力测试 - 流程图理解与代码生成 (11:27-12:53) 7.1: 测试7 - 流程图转Python代码 上传一张流程图图片，该流程图展示的是冒泡排序 (bubble sort) 算法。 输入指令：“请根据图上的流程, 用python给出完整的代码”。 测试结果：\n模型成功识别出流程图代表的是冒泡排序算法。 生成了相应的Python代码来实现该算法。 将生成的代码复制到代码编辑器中运行测试，代码能够成功运行并输出了正确的排序结果 ([11, 12, 22, 25, 34, 64, 90])。 作者观点：模型能够识别流程图的逻辑，并将其转换为可运行的Python代码。 part8: 总结与评价 (12:53-13:11) 8.1: 作者最终评价 作者观点：通过多方面和多角度的测试，可以发现 InternVL3-14B-Instruct 这款模型的OCR能力还是非常强的。即使面对非常模糊的图像，模型也能成功提取图像上的文字内容。对于需要在本地部署进行OCR任务的企业项目来说，这款模型是一个非常不错的选择。视频中用到的代码和指令会放在视频下方描述栏或评论区，以及作者的博客 (www.aivi.fyi)。\n","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E5%BC%80%E6%BA%90%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8Binternvl3%E8%A7%86%E8%A7%89%E8%83%BD%E5%8A%9B%E6%B5%8B%E8%AF%95/","title":"开源多模态模型InternVL3视觉能力测试"},{"content":"༼ ༽ 【规则（从cursor一键生成的规则文件中了解到了 规则这个系统提示词） https://aistudio.google.com/prompts/1RttnSD3EpA0wXkzaz9uGrsp8V-vFvgDT】 [0 生成规则即便靠现成的完整项目] 《AI 生成规则的过程，不仅仅是为了让 AI 自己 后续能理解，也是一个帮助开发者（包括新手）梳理和理解项目的过程。生成的规则文件本身就是一种结构化的项目文档》 [1 什么是规则] 《规则就是输出规则，是AI理解和遵循的项目“说明书”和“行为准则”，AI会主动参考这些规则来约束和指导自己的输出，可以将规则理解为更结构化、更持久化、更具项目针对性的“系统提示”或“背景知识文档”,》\u0026gt; [2 创建规则 (core.mdc) ] \u0026lt;用法：定义Plan/Act 模式，把该规则投喂给AI后，就能指示AI在处理复杂任务时，先输出规划步骤（Plan），待你确认或修改后，再通过特定指令（如输入 ACT）触发它执行具体的代码生成（Act），就是AI客户端的chat模式和build模式\u0026gt; [3 项目结构规则 (project-structure.mdc, file-structure.mdc)] \u0026lt;用法：让AI创建新文件、重构代码或添加新功能时，AI会参考这些规则，确保新生成的文件被放置在项目约定的正确位置，并遵循推荐的命名方式\u0026gt; [4 页面结构规则 (page-structure.mdc)] \u0026lt;用法：当AI需要生成或修改某个页面时，它会依据此规则来组织页面的HTML结构或组件嵌套，确保页面符合既定的设计和布局规范\u0026gt; [5 样式规则 (styles.mdc)] \u0026lt;用法：在生成UI组件或修改样式时，会遵循这些规则来编写CSS或使用预定义的样式类/变量，从而保证整个项目的视觉风格一致性。这可以避免AI随意添加样式或使用不符合规范的写法\u0026gt; [6 UI组件规则 (ui-components.mdc)] \u0026lt;用法： AI在构建界面时，会知道可以使用哪些现成的组件，以及如何正确地使用它们\u0026gt; [7 项目状态规则 (project-status.mdc)、主索引文件 (index.mdc)] 《用法：这更像是一个供AI和开发者参考的项目日志或看板。AI可以读取它来了解项目当前状态\u0026gt; [8 项目概述规则 (project-overview.mdc)] 《用法：给AI一个关于项目的基础背景和整体印象，帮助AI在处理更广泛或战略性的问题时，能更好地理解上下文》 [9 测试规则 (testing.mdc)] \u0026lt;用法：当你要求AI为某个功能编写测试时，AI会按照这些规则来生成测试代码，确保测试符合项目的整体标准和实践\u0026gt;\n","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%94%9F%E6%88%90%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E8%A7%84%E5%88%99%E6%9C%89%E7%82%B9%E7%94%A8/","title":"生成什么样的规则有点用"},{"content":"༼ ༽\n#测试CDN的速度或节点速度的行不行\n用gemin2.5pro就能轻松测出来，无论是在cursor还是在google ai studio ♐如果节点速度不行，那么用cursor跑gemini2.5pro就经常跑失败，cursor连续两次跑gemin2.5pro失败那一定是网络问题 ♒google ai studio同理，google ai studio跑gemini模型对网络要求非常高，连续两次跑模型失败那就是网络问题 ♒google ai stuido对视频提取的网络要求变高了 ","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8gemini%E6%B5%8B%E7%BD%91%E7%BB%9C%E8%B4%A8%E9%87%8F/","title":"用gemini测网络质量"},{"content":"༼ 🚀挑战Gemini 2.5！最强开源企业级OCR大模型InternVL3！本地部署教程+实战测评全纪录，轻松搞定潦草手写汉字、模糊PDF扫描件、模糊复杂表格，效果炸裂超过人眼！支持Open WebUI༽\npart1: 环境准备与依赖安装 (01:30-01:53) 1.1: 运行环境说明 作者提到的关键信息: 演示将在Ubuntu系统上进行，使用RTX A6000显卡。 作者建议: Windows系统用户需要在Windows上开启WSL (Windows Subsystem for Linux) 才能进行后续操作。作者提供了微软官方关于如何安装WSL的文档链接。 目的: 明确演示所使用的硬件和操作系统环境，并为Windows用户提供前置条件说明。\n(01:53-02:04) 1.2: 安装Miniconda 关键操作: 通过命令行执行一系列指令 (wget, bash, eval, echo, source) 下载并安装Miniconda。 画面内容:\n1 2 3 4 5 6 # 安装Miniconda (如果尚未安装) wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh bash ~/miniconda.sh -b -p $HOME/miniconda eval \u0026#34;$($HOME/miniconda/bin/conda shell.bash hook)\u0026#34; echo \u0026#39;export PATH=\u0026#34;$HOME/miniconda/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 目的: 安装Conda环境管理器，用于后续创建和管理Python虚拟环境。 结果: Miniconda成功安装并配置好环境变量。\n(02:04-02:14) 1.3: 创建并激活Conda虚拟环境 关键操作: 使用conda create指令创建一个名为lmdeploy的Python 3.11虚拟环境，并通过conda activate指令激活该环境。 画面内容: conda create -n lmdeploy python=3.11 -y \u0026amp;\u0026amp; conda activate lmdeploy 目的: 为模型部署创建一个隔离的Python环境，避免依赖冲突。 结果: 名为lmdeploy的Conda环境被成功创建并激活，终端提示符前缀变为(lmdeploy)。\n(02:14-02:24) 1.4: 安装LMDeploy 作者提到的关键信息: 本次部署将使用LMDeploy工具包。 关键操作: 在已激活的lmdeploy环境中使用pip install指令安装LMDeploy。 画面内容: 命令行执行 pip install lmdeploy (演示脚本中为 pip install lmdeploy\u0026gt;=0.7.3)。 目的: 安装核心的模型部署工具LMDeploy。 结果: LMDeploy及其相关依赖包被成功下载并安装到lmdeploy环境中。\n(02:25-02:30) 1.5: 安装额外依赖包 关键操作: 使用pip install指令安装partial_json_parser和timm这两个库。 画面内容:\n1 2 pip install partial_json_parser pip install timm 目的: 安装运行InternVL3模型或LMDeploy可能需要的额外Python库。 结果: partial_json_parser和timm被成功安装。\npart2: 使用LMDeploy部署模型API服务 (02:30-03:05) 2.1: ♐启动模型API服务器 关键操作: 执行lmdeploy serve api_server指令来启动模型服务。 画面内容:\n1 lmdeploy serve api_server OpenGVLab/InternVL3-14B-Instruct --backend turbomind --server-port 23333 --tp 2 --chat-template internvl2_5 作者提到的关键信息:\n指令指定了要部署的模型为 OpenGVLab/InternVL3-14B-Instruct。 使用了turbomind作为推理后端 (--backend turbomind)。 指定服务器端口为23333 (--server-port 23333)。 设置张量并行 (Tensor Parallelism) 为2 (--tp 2)，表示使用2块GPU进行推理。 指定了聊天模板为internvl2_5 (--chat-template internvl2_5)。 作者建议: 如果只有一块GPU，应将--tp参数设置为1。 目的: 将InternVL3-14B-Instruct模型加载到GPU，并通过LMDeploy启动一个兼容OpenAI API的HTTP服务，以便后续应用调用。 结果: LMDeploy开始下载模型权重文件（.safetensors），下载完成后加载模型并启动Uvicorn Web服务器。服务器成功运行在http://0.0.0.0:23333，并提示可以在浏览器中打开该地址查看API详情（Swagger UI）。 part3: 安装并配置OpenWebUI (03:05-03:32) 3.1: 安装并启动OpenWebUI 作者提到的关键信息: 使用OpenWebUI作为与模型API交互的前端界面。 关键操作: 使用pip install open-webui指令安装OpenWebUI。 画面内容: pip install open-webui 目的: 安装Web用户界面。 结果: OpenWebUI安装完成。 关键操作: 使用open-webui serve指令启动OpenWebUI服务。 画面内容: open-webui serve 目的: 运行Web界面服务器。 结果: OpenWebUI服务启动，可以通过浏览器访问localhost:8080。\n(03:32-04:06) 3.2: 配置OpenWebUI连接模型API 关键操作: 在浏览器中打开localhost:8080访问OpenWebUI。 关键操作: 点击左下角头像 -\u0026gt; 设置 -\u0026gt; 管理员设置 -\u0026gt; 外部连接。 关键操作: 确认“OpenAI API”开关已打开。 关键操作: 在“管理OpenAI API连接”的输入框中，填入由LMDeploy启动的API服务器地址。 作者提到的关键信息: API地址应为http://[部署服务器的IP地址]:23333/v1。视频中示例为http://192.168.1.105:23333/v1。 关键操作: 点击右下角的“保存”按钮。 关键操作: 返回主界面，点击左上角“新建对话”。 关键操作: 在模型选择下拉菜单中，选择刚刚部署的模型 OpenGVLab/InternVL3-14B-Instruct。 目的: 将OpenWebUI前端指向本地运行的InternVL3模型API，使其能够调用该模型进行交互。 结果: OpenWebUI成功连接到本地部署的模型API，可以在聊天界面中选择并使用OpenGVLab/InternVL3-14B-Instruct模型。\nFastAPI:\n定位: FastAPI 是一个通用的、高性能的Python Web框架，用于构建API（应用程序接口）。 作用: 它的核心功能是创建Web服务和API端点。在模型部署的场景中，你可以使用FastAPI来编写代码，定义模型的输入/输出接口，接收用户请求，调用模型进行推理，然后将结果返回给用户。♈它本身不包含模型加载、优化或推理执行的功能，♐你需要自己集成这些逻辑（例如使用transformers库加载模型，用PyTorch/TensorFlow进行推理）。 类比: 它更像是建造房屋的**“脚手架”或“工具箱”**，提供了构建API的基础结构，但具体的模型推理逻辑需要你自己实现。 Ollama:\n定位: Ollama 是一个简化在本地运行大型语言模型（LLM）的工具。 作用: 它打包了运行多种开源LLM所需的环境、模型权重下载和管理，并内置了一个API服务器（通常兼容OpenAI API格式）。用户可以通过简单的命令 (ollama run model_name) 快速启动并与模型交互，或者通过其提供的API进行调用。它的重点在于易用性和本地快速体验。 类比: 它更像是一个**“开箱即用的LLM运行器”**，帮你处理了很多底层的配置和模型管理，让你能快速用上各种模型。 LMDeploy:\n定位: LMDeploy 是一个专注于大型语言模型（LLM）的高效部署工具包，由InternLM团队开发。 作用: 它提供了模型转换、推理引擎（如TurboMind）优化、量化以及API服务部署等一系列功能。它的核心优势在于推理性能优化，可以显著提升LLM的推理速度并降低显存占用。它也提供了一个开箱即用的API服务器（如视频中展示的lmdeploy serve api_server），并且这个服务器也兼容OpenAI API格式。 类比: 它更像是一个**“高性能LLM部署与服务引擎”**，不仅帮你部署模型并提供API，还着重于优化模型的运行效率。 总结与比较：\n相似之处: LMDeploy 和 Ollama 都提供了相对完整的、用于运行LLM并提供API服务的功能，都致力于简化模型部署的过程，并且都提供了兼容OpenAI的API接口，方便与其他应用集成。从这个角度看，LMDeploy和Ollama更像是同类工具，都是LLM Serving（服务）框架/工具。 不同之处: LMDeploy vs Ollama: LMDeploy 更侧重于推理性能优化（特别是对于InternLM系列模型，利用TurboMind等引擎），提供更细致的部署配置选项（如张量并行）；而Ollama更侧重于跨模型兼容性和本地运行的便捷性。 LMDeploy/Ollama vs FastAPI: FastAPI 是一个更底层的Web框架，用于构建各种API，包括模型API。而LMDeploy和Ollama是更高层、更专注于LLM部署的解决方案，它们内部可能使用了类似FastAPI/Uvicorn这样的Web框架来构建其API服务部分，但它们还封装了模型加载、优化、推理和管理等更多功能。 简单来说：\n♈如果你想快速在本地跑各种开源LLM，并且不太关心极致的性能优化，Ollama 是个好选择。 ♈如果你想高效地部署特定（尤其是InternLM系列）的LLM，追求推理速度和低资源占用，并需要一个兼容OpenAI的API服务，LMDeploy 是一个强大的工具。 如果你想从头开始构建一个定制化的模型API服务，需要完全控制API的逻辑和行为，或者部署非LLM模型，那么你会使用FastAPI这样的Web框架作为基础。 因此，LMDeploy 和 Ollama 可以被认为是同类（LLM Serving工具），而 FastAPI 是构建这类工具（或其他Web API）的基础框架。\n","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8lmdeploy%E6%9C%AC%E5%9C%B0linux%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2internvl3/","title":"用LMDeploy本地linux系统部署InternVL3"},{"content":"༼ 企业级最强开源大模型Qwen3震撼发布！本地部署+全面客观测评！Qwen3-235B-A22B+Qwen3-32B+Qwen3-14B谁是最强王者？ollama+LM Studio+vLLM本地部署I༽\nPart 1: 准备vLLM部署环境 (01:57-02:06) 1.1: 介绍vLLM及部署平台\n作者提到的关键信息：如果需要进行企业项目部署，可以使用vLLM。作者将在Ubuntu系统上演示使用vLLM进行部署。 (02:06-02:10) 1.2: 展示部署环境\n关键操作：作者执行nvidia-smi命令查看服务器的显卡信息。 目的：确认部署环境的硬件配置。 画面内容：显示服务器使用的是NVIDIA RTX A6000显卡，显存为48G，CUDA版本为12.7。 (02:10-02:19) 1.3: 安装Miniconda\n目的：安装Miniconda用于管理Python环境和依赖包。 关键操作：作者展示并执行了一系列命令来下载、安装Miniconda，并配置相关的环境变量。 画面内容: 1 2 3 4 5 6 # 安装Miniconda (如果尚未安装) wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh bash ~/miniconda.sh -b -p $HOME/miniconda eval \u0026#34;$($HOME/miniconda/bin/conda shell.bash hook)\u0026#34; echo \u0026#39;export PATH=\u0026#34;$HOME/miniconda/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 结果：Miniconda环境安装并配置完成。 (02:19-02:29) 1.4: 创建并激活Conda环境\n目的：创建一个独立的Python环境（qwen3_env）用于vLLM部署，指定Python版本为3.10。 关键操作：执行conda create命令创建环境，然后执行conda activate命令激活该环境。 画面内容: 1 2 3 # 创建conda环境 conda create -n qwen3_env python=3.10 -y conda activate qwen3_env 结果：成功创建并进入了名为qwen3_env的Conda环境。 (02:29-02:35) 1.5: 安装vLLM及依赖\n目的：安装vLLM库及其运行所需的依赖库（ray, transformers, accelerate）。 关键操作：在已激活的Conda环境（qwen3_env）中，使用pip install命令安装vLLM及相关依赖。 画面内容: 1 pip install vllm ray transformers accelerate 结果：vLLM及其依赖库安装完成。 Part 2: 使用vLLM启动Qwen3 14B模型服务 (02:35-02:46) 2.1: 启动vLLM服务\n目的：使用vLLM将Qwen3 14B模型加载并启动为一个API服务。 关键操作：执行vllm serve命令，指定模型为Qwen/Qwen3-14B，并添加了参数--enable-reasoning和--reasoning-parser deepseek_r1。 画面内容: 1 vllm serve Qwen/Qwen3-14B --enable-reasoning --reasoning-parser deepseek_r1 结果：vLLM服务成功启动，API服务器监听在http://0.0.0.0:8000。 Part 3: 通过Open WebUI调用vLLM模型 (02:46-03:38) 3.1: 安装、启动并配置Open WebUI连接vLLM\n目的：配置一个Web UI（Open WebUI）来方便地与通过vLLM部署的Qwen3 14B模型进行交互。 关键操作: 在本地电脑终端安装Open WebUI：pip install open-webui。 启动Open WebUI服务：open-webui serve。 在浏览器访问http://localhost:8080。 进入Open WebUI设置 -\u0026gt; 管理员设置 -\u0026gt; 外部连接。 在OpenAI API连接设置中，将API请求地址设置为vLLM服务器的IP地址+端口号+/v1（例如：http://192.168.1.105:8000/v1）。 API Key处随意输入几个字母。 点击保存。 结果：Open WebUI配置完成，可以连接到vLLM服务。 (03:38-03:52) 3.2: 在Open WebUI中选择并使用模型\n目的：通过配置好的Open WebUI界面与Qwen3 14B模型进行对话测试。 关键操作：在Open WebUI中，点击新建对话，然后在模型选择列表中选择刚配置好的Qwen: Qwen3 14B模型。 结果：用户现在可以通过Open WebUI与本地通过vLLM部署的Qwen3 14B模型进行交互。 Part 4: 补充信息 (15:39-15:43) 4.1: 获取代码和指令\n作者建议：视频中使用的代码和指令会放在视频下方的描述栏或者评论区。 (15:43-15:49) 4.2: 通过博客获取笔记\n作者建议：如果在视频下方无法找到代码和指令，可以访问作者的博客（www.aivi.fyi）查找本期视频对应的笔记。 ","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8vllm%E6%9C%AC%E5%9C%B0linux%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2qwen3/","title":"用VLLM本地linux系统部署Qwen3"},{"content":"༼ ༽\npart1: 开场与本周内容预告 (00:00-00:03) 1.1: 开场白\n视频开头，介绍了这是GitHub一周热点汇总的第70期。 (00:03-00:17) 1.2: 本周内容预告\n预告了本周将包含的内容，包括： 微软的原生1 Bit模型：BitNet 无限长度影片生成模型：SkyReels-V2 程序员在家做饭指南：HowToCook 浏览器中运行VSCode：code-server 命令行的视频下载器：yt-dlp 最后分享两份资料。 (00:17-00:19) 1.3: 互动提示\n提醒观众如果觉得内容不错，别忘了点赞关注。 part2: BitNet - 微软原生1bit模型 (00:19-00:28) 2.1: BitNet项目介绍\nBitNet是微软开源的原生1 Bit三进制大模型。 号称可以在单CPU上就能运行。 性能与同规模全精度的开源模型相当。 (00:29-00:52) 2.2: BitNet b1.58 2B4T命名解析\n解释了模型名称 BitNet b1.58 2B4T 的含义： b1.58：表示采用了三进制存储权重，每个权重仅用1.58 Bit，相比16位浮点数可大幅降低显存需求。 2B：代表模型包含20亿参数。 4T：表示模型使用了4T (Trillion) 的训练数据进行训练。 (00:53-01:17) 2.3: bitnet.cpp项目介绍与性能\nbitnet.cpp是BitNet配套的官方推理框架。 提供了一套优化的内核，支持在CPU上进行快速无损的1.58位模型推理。 性能对比llama.cpp： 在ARM CPU上实现了1.37倍到5.07倍的加速。 在X86架构CPU上实现了2.07倍到6.17倍的加速。 并且都能大幅降低能耗。 (01:18-01:31) 2.4: 运行示例与前景\n展示了在苹果M2芯片上运行的效果，速度非常不错。\n该项目因其CPU推理能力和更少的资源需求而备受关注，在个人电脑和边缘设备上的运行前景被看好。\n(01:32-01:38) 2.5: 技术报告\n微软的开源一直很全面。 相关的技术报告也已发布，感兴趣的朋友可以深入阅读。 part3: SkyReels-V2 - 无限长度影片生成模型 (01:39-01:55) 3.1: SkyReels-V2项目介绍\nSkyReels-V2是由昆仑万维发布的无限长度影片生成模型。 被认为是国内大模型玩家的新生力量。 最大特点是生成的时长是无限的，相比目前主流的5-10秒片段非常有吸引力。 官方给出的视频效果都不错。 (01:56-02:03) 3.2: 量子位演示效果\n提到了在网上看到量子位发布的一个视频，效果非常震撼。 该视频演示了用一张照片加上一句prompt为起点，实现了AI版的《罗马假日》。 (02:04-02:15) 3.3: 技术原理简述 (基于报告)\n根据官方技术报告，模型通过各种技术，基于前一段视频的最后几帧来扩充后续的视频，从而实现了无限的扩展。 (02:16-02:25) 3.4: 在线版本与试用\nSkyReels目前有自己的在线版本可以使用。 注册会赠送25积分。 生图还可以，但生成视频的话，免费积分只够生成5秒。 想要更多功能需要购买会员，价格稍微有点小贵。 (02:26-02:30) 3.5: 网友生成效果\n展示了一些网友的生成效果，整体感觉还不错。 画面内容2 (展示网友生成的动画风格女孩走路视频) (02:31-02:39) 3.6: 基于开源项目与ComfyUI集成\nSkyReels是基于阿里开源的wan2.1。 网上已经有很多大神开始整合ComfyUI的工作流了。 (02:38-03:16) 3.7: 本地显卡限制与青椒云方案\n作者表示自己的本地显卡确实不太行。 推荐使用青椒云云桌面进行尝试。 青椒云是专注为视觉行业提供垂直云计算服务的平台。 在视觉云计算领域，其算力规模是亚洲第一。 作者使用了其高性能4060-Ti云桌面，选择了自带ComfyUI的模板进行演示（模板内已内置好工具，用起来方便）。 画面内容3 (演示在青椒云选择带ComfyUI模板的4060Ti云桌面) 建议： 对于Mac用户临时想用Windows系统或想玩游戏，青椒云也提供多种高性能GPU和CPU的云桌面，可按需按时计费，且安全合规，数据不落地，是个不错的选择。 part4: HowToCook - 程序员在家做饭方法指南 (03:17-03:26) 4.1: HowToCook项目介绍\nHowToCook是一个程序员在家做饭的指南。 作者很久以前看到过，也有朋友推荐过。 引出疑问：为什么一个菜谱的项目会这么火？ (03:27-03:42) 4.2: 核心理念：程序员思维做饭\n因为它使用程序员的思维来解构做饭这件事。 采用结构化、逻辑化的方式来完成做饭。 以红烧茄子为例，其过程被分解为：原料工具、计算、操作、最后是成品。 (03:43-04:04) 4.3: 细节：量化与开发流程类比\n材料用量不像一般菜谱的“几勺”，而是精确的剂量（例如 多少多少克）。 时间也精确到秒（例如 翻炒15秒）。 看起来非常像一份项目的文档，类比软件开发流程：从需求分析 -\u0026gt; 架构设计 -\u0026gt; 开发计划 -\u0026gt; 项目完成 -\u0026gt; 成品。 这是理工男与厨房锅铲的理想主义结合。 建议： 如果自己也能做饭，不妨去试一试，看看按照这个菜谱做出的效果怎么样。 part5: code-server - 浏览器中运行VSCode (04:05-04:14) 5.1: code-server项目介绍\ncode-server是一个用浏览器来运行VS Code的项目。 可以用任何的电脑、平板来进行远程的开发。 在国内，这类工具通常被称为Cloud IDE。 (04:15-04:49) 5.2: Cloud IDE的优势\nCloud IDE常见的好处有几个： #1 无设备限制：只要有浏览器就可以开发，不限地点、不限设备。 #2 省去环境配置：可以省去环境的配置工作。例如项目有新人进来，启动一套新环境，马上就能开始干活。 #3 性能限制：解决设备的性能限制。因为实际运行都是在服务器上，对本地的要求其实很低。 #4 安全性：在一些安全性的场景下也是一个卖点。 (04:50-05:18) 5.3: Coder与Terraform集成及商业方案\ncode-server属于Coder项目组。 Coder项目组还有专门的Terraform的项目模板，可以用来快速地构建服务器上的开发环境。 将 Coder server (运行VS Code) + 模板部署 (环境构建) + K8s (底层架构) 结合起来，就是国内很多商业的Cloud IDE的方案的基本结构。 作者观点： 很多开发者不习惯在浏览器里写代码。如果强制只能用浏览器IDE，不能装本地版，开发者可能不会同意。 part6: yt-dlp - 命令行的视频下载器 (05:19-05:29) 6.1: yt-dlp项目介绍\nyt-dlp是一个非常不错的视频下载项目 (命令行工具)。 很多图形化的下载工具，实际上有可能就是用的它。 (05:30-05:40) 6.2: 与youtube-dl对比\nyt-dlp是基于youtube-dl的开发应用 (fork)。 对比youtube-dl，yt-dlp更加灵活，活跃度也比较高，下载速度也很快。 (05:41-05:49) 6.3: 安装与使用\n可以通过官方的一键安装脚本来安装。 也可以通过pip来安装。 建议： 使用方面建议大家自己去实操一下，因为这个东西不太好说太多。 (05:50-06:00) 6.4: 使用技巧与问题处理\n几个注意点： 如果下载速度慢，可以开启多线程 (-N) 或者开启加速/限制速率 (-r, --limit-rate)。 如果报**“机器人”的错误 (not a bot)，那可能是IP被封了**，需要考虑换个IP或者换一下cookie (--cookies)。 (06:01-06:06) 6.5: 依赖项：FFmpeg\nyt-dlp依赖于FFmpeg。 否则的话，下载高清视频就会有画质、没有声音。 part7: One more thing - 资料分享 (06:07-06:20) 7.1: DevOps Roadmap 2025\n分享第一个资料：DevOps Roadmap 2025。 建议： 作为开发者，最好都要有一些DevOps的意识，尤其是小团队或者创业者。 (06:21-06:29) 7.2: Google AI Agent白皮书\n分享第二个资料：谷歌出品的AI agent白皮书。 之前官方就推出了，值得一看。 如果需要可以告诉作者。 part8: 结尾 (06:30-06:34) 8.1: 总结与互动\n以上是本周的全部内容。 如果觉得不错，别忘了点赞关注。 (06:34-06:36) 8.2: 结束语\n我们下周再见。 ","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E7%A7%91%E6%8A%80%E6%96%B0%E9%97%BB1/","title":"科技新闻1"},{"content":"༼2元面包店为什么不火了？——【两元面包】不是真正的产品，【面包店】才是 ༽\npart1: 2元面包店的热潮与疑问 (00:00-00:09) 1.1: 起源时间\n视频提问：2元面包店何时开始火爆？ 讲述者认为大约是2023年上半年开始的，距今已快2年时间（口误，应为1年多）。 引用百度百科信息佐证，显示2023年6月济南火车站附近开设了第一家，10月日照市街头也出现了多家。 (00:09-00:24) 1.2: 初期火爆原因与预期\n刚出现时特别火，引发热议。 人们认为终于有人把面包价格打下来了，使其价格与馒头相当。 优点在于价格低廉且种类繁多可供选择。 讲述者评价这种模式“太牛逼了”，并预测按照正常逻辑，武汉市一年能开200家。 (00:24-00:34) 1.3: 热潮消退与解析引入\n提出疑问：为何现在反而没动静了，像一阵风一样过去了？ 猜测：难道是商家不愿意把价钱打下来吗？ 引入讲述者“小鱼”为大家解析原因。 part2: 揭秘核心：卖的不是面包，是项目 (00:34-00:47) 2.1: 商业模式本质\n解析最早的起源：从一开始，产品就不是店里卖的面包。 真正的“产品”是2元面包店这个项目本身。 强调：“谁问的就赚谁的钱”。 实际售卖的是“如何开2元面包店”的加盟或培训项目。 (00:47-01:02) 2.2: 低价引流与素材制造\n初期通过低价促销吸引眼球。 很多人（包括同行）出于好奇心（想了解2元能做出什么水平的面包）而去购买，自然而然形成了排队。 关键点：排队的人群成为了编织“暴富梦想”的素材和主角，用于吸引更多加盟者。 part3: 互联网模式的套用及其局限 (01:02-01:24) 3.1: 互联网概念的应用\n该项目运用了互联网概念进行包装。 核心策略是“花钱烧规模”，前期不赚钱无所谓，目的是固定用户，后期再进行“收割”。 (01:24-01:40) 3.2: 实体业务的根本差异\n加盟者忽略了一个关键点：面包是需要人做的实体产品。 它不是数字化的Ctrl+C、Ctrl+V，无法无限复制。 实体店受固定面积限制，导致产量有上限。 结合极低的可怜利润率，导致难以规模化盈利。 part4: 加盟者的困境与模式演变 (01:40-02:02) 4.1: 加盟商盈利现状\n讲述者分享群友案例：有人去济南加盟2元面包店。 提及“勇哥”都知道济南加盟多为骗局，上当受骗的人很多。 群里几百个加盟商，只有两个人盈利（包括该群友）。 盈利者赚的也只是纯辛苦钱。 (02:02-02:22) 4.2: 经营的辛劳与不可持续\n盈利的加盟商需要拉上全家亲戚（如妈妈）一起长时间辛苦劳作。 举例：一天做1000个面包（2000多元营业额）非常累，堆积如山。 结果：赚钱的人累得要死，不赚钱的人更惨（累死还不赚钱）。 结论：这个模式推不起来，不可持续。 (02:22-02:41) 4.3: 模式演变：走向普通面包店\n即使是勉强活下来的店（如“2元总管”）。 生存之道在于增加高价产品：开发3元、5元、8元的产品。 最终慢慢变成了一个普通的面包店。 总结：2元面包店模式并非一定会失败，但作为一种商业模式没什么意思，投入产出比极低。 part5: 低成本创业陷阱的本质与循环 (02:41-03:07) 5.1: 类比“慈善”与“打工”\n将开2元面包店比作做慈善。 描述投入：投资10万，换来一周7天、每天10小时的工作，月收入可能只有4000元（仅为辛苦钱），回本遥遥无期。 对比：收入甚至不如在旁边包子铺打工。 (03:07-03:20) 5.2: 旁观者与实践者的认知偏差\n建议：去2元面包店上两天班就能明白其中辛劳。 指出：去年觉得这个模式有搞头的，往往是那些不干活的人（指推广加盟的人）。 (03:20-03:49) 5.3: 消费者期望与商家现实的矛盾\n推广者声称模式满足了人民（消费者）的需求。 消费者角度：希望东西越便宜越好。 商家现实：不赚钱的情况下，必然会偷工减料，例如馅料像葡萄干一样小，使用植物奶油而非动物奶油。 反问：2块钱还想吃动物奶油？给你包波士顿龙虾你敢吃吗？ (03:49-04:13) 5.4: 快消加盟模式的换皮与循环\n指出消费者只考虑自己，不考虑别人（商家）。 这种模式本质上是快消行业的换皮，与“大锅卤菜”等加盟类似。 预言：明年还会有新的套路出现，本质都是针对对行业完全没有理解的人，推广低成本创业项目，进行反反复复的割（韭菜）。 (04:13-04:33) 5.5: 历史教训与思维定式\n引用《过秦论》：“后人哀之而不鉴之，亦使后人而复哀后人”。 批评人们永远不总结教训，总想着“零基础、低投入、赚大钱”的美梦。 只要存在这种想法，快招（快速招商加盟）就永远存在。 part6: 低价模式成功与失败的对比分析 (04:33-04:57) 6.1: 对比蜜雪冰城\n提出对比：大家都想喝便宜的奶茶，蜜雪冰城成功了。 反观面包：大家都想吃便宜的面包，为什么2元面包店就起不来？ 核心原因：价钱压得太低了，导致模式不可持续，无法盈利。 part7: 特殊案例：信仰驱动的低成本模式 (04:57-05:42) 7.1: 北京某“教会企业”案例引入\n存在一种能让别人心甘情愿免费干活的办法。 举例北京一家非常有名、规模很大、产品很好的企业。 其产品价格便宜，按常规成本计算很难覆盖。 原因揭秘：该企业的工人（很多是残障人士）加班不要加班费，月工资仅800元（在北京）。 (05:42-06:14) 7.2: 信仰的力量：低薪或无薪的驱动力\n工人非常用心用力地干活，并非因为钱少。 这家企业是“教会企业”。 工人认为自己是在为上帝干活，传播福音，而非为了钱。 信仰是驱动力，所以即使给500元也干，甚至不需要钱，包吃包住即可。 想干活的人多得很。 (06:14-06:31) 7.3: 常规模式的对比与2元店的结局\n对比普通人（无神论者）：就是看钱，钱少了就不干，人都请不到。 结论：2元面包店最终只能老板剥削自己来维持。 模式垮台的根本原因：找不到那种既不要钱（或极低工资）又好好干活的人，除非有信仰等特殊驱动力。 ","date":"2025-01-28T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E8%83%BD%E6%8A%8A%E4%BB%B7%E6%A0%BC%E6%89%93%E4%B8%8B%E6%9D%A5%E4%BD%86%E4%B9%9F%E8%A6%81%E6%B4%BB%E4%B8%8B%E5%8E%BB%E6%89%8D%E6%9C%89%E7%94%A8/","title":"能把价格打下来但也要活下去才有用"},{"content":"\t- [action部署hugo项目的aciton程序] - \u0026lt;action部署hugo项目的aciton程序的核心：让action使用hugo命令，action环境需要hugo命令执行hugo博客\u0026gt; - \u0026lt;1 有参考作用的使用uses参数的action程序的写法（参考如何让AI写action程序）\u0026gt; - \u0026lt;2 有参考作用的使用run字段的action程序的写法 https://aistudio.google.com/prompts/1dv1SBgRsYgFBSgYe1Dseg0dyYouTZ3_P（参考如何让AI写action程序）\u0026gt; - [[[[在action环境安装项目包（hugo包）的两种做法（参考如何让AI写action程序）] - 《1 uses 关键字并指定一个像 别大大佬些好的aciton 比如peaceiris/actions-hugo@v3 这样的“动作 (Action)”时，你是在委托这个预先打包好的工具去完成安装和配置 Hugo 的任务》 - 《2 直接用run字段亲自告诉那个临时的 Linux 系统具体怎么做才能把 Hugo 装好》 - [aciton想部署hugo的核心就是用别人预先写好的东西] - 《hugo官方有提供官方action动作把仓库代码下载（检出） 到当前的虚拟运行环境中，它就是actions/checkout@v4，通过uses: actions/checkout@v4: 就能使用一个预先写好的、官方提供的“动作” (Action)》 ","date":"2024-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/github-action0-action%E9%83%A8%E7%BD%B2hugo%E9%A1%B9%E7%9B%AE%E7%9A%84aciton%E7%A8%8B%E5%BA%8F-%E6%9C%80%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/","title":"github action：0 action部署hugo项目的aciton程序 最基本原理"},{"content":"(08:41-10:19) 4.4: 配置 Repository Secret\n♈ 目的：将生成的 Token 安全地提供给 GitHub Actions workflow 使用。不应该直接将 Token 字符串写入 YAML 文件。\n操作路径：回到♎♐存放 Hugo 源代码的仓库（即 hugo-main），进入该仓库的 Settings -\u0026gt; 左侧菜单 Security 下的 Secrets and variables -\u0026gt; Actions。\n♈操作：点击 New repository secret 按钮。\n配置 Secret：\nName：输入 Secret 的名称。这个名称必须与 workflow YAML 文件中 secrets. 后面引用的名称完全一致。作者在这里将其命名为 TOKEN。 Secret：将刚才复制并保存的完整 Token 字符串粘贴到这里。 保存：点击 Add secret。\n♈优点：使用 Secrets 可以隐藏敏感信息，workflow 运行时会安全地注入该值，比硬编码在代码中安全得多\n[[[[[使用aciton后将分源代码仓库和静态页面仓库 ]https://aistudio.google.com/prompts/1dv1SBgRsYgFBSgYe1Dseg0dyYouTZ3_P\n《《《hugo-main 仓库 (源代码仓库)，专门用来提交项目源码的仓库，以及用来运行action的仓库，不需要开启github page的功能，所以hugo-main仓库不需要公开，也不需要把静态资源上传到这仓库上》 《《《hugo-dev 仓库 (部署目标/静态页面仓库)，hugo-main运行action后生成的静态资源文件所推送到的仓库，是存放最终网站文件并对外提供访问的地方，它是自动化流程的终点/发布目标，因此我们在仓库开启github page功能》 《《《aciton中的参数 EXTERNAL_REPOSITORY: letere-gz/hugo-dev 就是用来 明确指定了main仓库部署的目标是这个dev仓库。》 ","date":"2023-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/github-action2-github-action%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%E5%8E%9F%E7%90%86github-action%E6%98%AF%E5%B0%86%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E5%88%B0%E5%85%B6%E5%AE%83%E4%BB%93%E5%BA%93%E4%B8%8D%E6%98%AF%E9%83%A8%E7%BD%B2%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BB%93%E5%BA%93/","title":"github action：2 github action部署项目原理：github action是将项目部署到其它仓库，不是部署到自己的仓库"},{"content":" github aciton配置文件如何创建 (2 action配置文件用aciton模板的配置文件， action模板配置文件扔给AI，说明需要这个action模板运行哪个脚本，让AI修改即可，因为有些时候 AI直接生成的action配置文件aciton根本运行不了) ","date":"2023-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/github-action3-ai%E7%BC%96%E5%86%99action%E7%A8%8B%E5%BA%8F%E6%8A%80%E5%B7%A7/","title":"github action：3 AI编写action程序技巧"},{"content":"(00:09-00:32) 1.2: 手动部署痛点与自动化优势\n♈指出传统方式下，每次修改完文件后，都需要手动将生成的 public 文件夹上传到服务器或托管平台，这个过程比较麻烦。 ♈使用 GitHub Actions 可以实现自动化：自动帮我们生成 public 目录，并将其部署到静态页面网站（如 GitHub Pages）。 ♈优点：只需提交代码 (git push)，后续的构建和部署过程就能自动完成，方便很多。x x \u0026lt;-https://aistudio.google.com/prompts/1dv1SBgRsYgFBSgYe1Dseg0dyYouTZ3_P 《《《♐可以源码仓库和静态资源仓库分开；因为源码仓库很重要，每次项目回溯时回溯源码仓库的代码才有意义，回溯静态资源仓库的代码没意义，这意味着未来版本控制就必须要push推送源码仓库，♓但要推送源码仓库就意味着每次推送都要同时push推送源码仓库和静态资源仓库，手动维护两个独立的 Git，需再手动进行第二次推送 流程退冗余了；♐而有了action部署静态资源后，每次只需要推送最重要的源码仓库，无需做冗余的推送静态资源仓库的步骤，无需再手动进行第二次推送\u0026gt; 《♓我觉得最主要作用是源码静态资源仓库和源代码仓库分开；♐其实不要action也行，不要action就不去推送源代码仓库了，在本地只推送public静态资源即可，反正都是推送步骤不会减少 《《♐用action把部署静态资源到github page的真正价值：多人协作，多人协作时，需要所有人都遵循同样的流程，但很是有人知道我们要推送的是public静态资源，不是源码，这就导致源码把静态资源给污染了（目录混入源代码仓库会极大地污染 Git 历史）》 \u0026lt;设置好 Action 后，你甚至可以通过 GitHub 网页界面直接编辑 Markdown 文件并提交，Action 照样能完成部署，你此时甚至不需要本地安装 Hugo；因此未来你更换了本地开发机器，构建环境都是一致的，每次 Action 运行时，都是用同一个版本、同一种配置的 Hugo 工具在同一种操作系统环境下进行构建，结果更可控\u0026gt; ","date":"2023-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/github-action4-%E7%94%A8github-action%E6%8A%8A%E6%BA%90%E7%A0%81%E4%BB%93%E5%BA%93%E9%83%A8%E7%BD%B2%E9%A1%B9%E7%9B%AE%E5%88%B0github-page%E4%BB%93%E5%BA%93%E7%9A%84%E7%9C%9F%E6%AD%A3%E6%84%8F%E4%B9%89/","title":"github action：4 用github action把源码仓库部署项目到github page仓库的真正意义"},{"content":"(01:24-02:04) 2.4: 配置 .gitignore 忽略生成文件\n说明：Hugo 项目中的某些文件/目录是自动生成的，不应该提交到源代码仓库，包括： public/：Hugo 生成的静态网站文件目录。 resources/：Hugo 处理后的资源缓存目录。 hugo.build.lock：构建锁文件。 hugo.exe：（如果在 Windows 下）Hugo 可执行文件本身。 ♈ 操作：在本地 Hugo 项目根目录下，创建一个名为 .gitignore 的文件。 ♈操作：编辑 .gitignore 文件，将上述需要忽略的文件名或目录名（如 public, resources, hugo.build.lock, hugo.exe）添加进去，每行一个。 ♈目的：保证上传到 GitHub 的是最干净、最原始的 Hugo 项目文件。 《♓目的就是不要让静态资源污染源码，因为hugo-main是源代码仓库，源代码仓库不负责展示github page，只负责运行action得到静态资源给其它部署仓库hugo-dev，所以没必要把多余的静态资源上传到hugo-main，反正在hugo-main展示这些静态资源》 \u0026lt;git add .：添加所有文件到暂存区（♎.gitignore 会生效，忽略指定文件）。\u0026gt; ","date":"2023-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/github%E9%85%8D%E7%BD%AE1-%E4%BD%BF%E7%94%A8.gitignore%E7%9A%84%E6%84%8F%E4%B9%89/","title":"github配置：1 使用.gitignore的意义"},{"content":" [0 检查当前分支的跟踪关系]\n\u0026lt;命令：git status；因为如果追踪错误，那么我想要push推送到我想要的分支很可能失败，因为追踪错误的缘故，pull的是其它的分支，因此我想要的分支就没法push了\u0026gt; [1 取消本地 main 分支对 所有分支 的跟踪]\n\u0026lt;命令：git branch \u0026ndash;unset-upstream\u0026gt; [[[[2 指定pull拉取main的分支（为了push到main，只能强制指定要拉取main分支）]\n《命令：git pull origin main》 [[[[3 pull拉取了main分支后 就能push推送到main分支了]\n\u0026lt;命令：git push -u origin main\u0026gt; ","date":"2023-02-20T20:14:22+08:00","permalink":"https://yichixing.github.io/hugo-stack-dev/p/github%E9%85%8D%E7%BD%AE2-github-pull-%E4%B8%8E-github-pushpull%E5%AF%B9%E5%88%86%E6%94%AF%E5%90%8E%E6%89%8D%E8%83%BD%E7%94%A8push/","title":"github配置：2 github pull 与 github push，pull对分支后才能用push"},{"content":"","date":"2019-03-11T00:00:00Z","permalink":"https://yichixing.github.io/hugo-stack-dev/p/%E6%A8%A1%E6%9D%BF/","title":"模板"}]