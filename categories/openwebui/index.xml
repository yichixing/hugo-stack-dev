<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>OpenWebUI on 个人笔记博客</title>
        <link>https://yichixing.github.io/hugo-stack-dev/categories/openwebui/</link>
        <description>Recent content in OpenWebUI on 个人笔记博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Tue, 28 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://yichixing.github.io/hugo-stack-dev/categories/openwebui/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>用LMDeploy本地linux系统部署InternVL3</title>
        <link>https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8lmdeploy%E6%9C%AC%E5%9C%B0linux%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2internvl3/</link>
        <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8lmdeploy%E6%9C%AC%E5%9C%B0linux%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2internvl3/</guid>
        <description>&lt;p&gt;༼ 🚀挑战Gemini 2.5！最强开源企业级OCR大模型InternVL3！本地部署教程+实战测评全纪录，轻松搞定潦草手写汉字、模糊PDF扫描件、模糊复杂表格，效果炸裂超过人眼！支持Open WebUI༽&lt;/p&gt;
&lt;h1 id=&#34;part1-环境准备与依赖安装&#34;&gt;part1: 环境准备与依赖安装
&lt;/h1&gt;&lt;p&gt;(01:30-01:53)
&lt;em&gt;&lt;strong&gt;1.1: 运行环境说明&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;作者提到的关键信息&lt;/strong&gt;: 演示将在&lt;strong&gt;Ubuntu系统&lt;/strong&gt;上进行，使用&lt;strong&gt;RTX A6000&lt;/strong&gt;显卡。
&lt;strong&gt;作者建议&lt;/strong&gt;: Windows系统用户需要在Windows上&lt;strong&gt;开启WSL (Windows Subsystem for Linux)&lt;/strong&gt; 才能进行后续操作。作者提供了微软官方关于如何安装WSL的文档链接。
&lt;strong&gt;目的&lt;/strong&gt;: 明确演示所使用的硬件和操作系统环境，并为Windows用户提供前置条件说明。&lt;/p&gt;
&lt;p&gt;(01:53-02:04)
&lt;em&gt;&lt;strong&gt;1.2: 安装Miniconda&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;关键操作&lt;/strong&gt;: 通过命令行执行一系列指令 (&lt;code&gt;wget&lt;/code&gt;, &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;eval&lt;/code&gt;, &lt;code&gt;echo&lt;/code&gt;, &lt;code&gt;source&lt;/code&gt;) 下载并安装&lt;strong&gt;Miniconda&lt;/strong&gt;。
&lt;strong&gt;画面内容&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装Miniconda (如果尚未安装)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bash ~/miniconda.sh -b -p &lt;span class=&#34;nv&#34;&gt;$HOME&lt;/span&gt;/miniconda
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOME&lt;/span&gt;/miniconda/bin/conda shell.bash hook&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;export PATH=&amp;#34;$HOME/miniconda/bin:$PATH&amp;#34;&amp;#39;&lt;/span&gt; &amp;gt;&amp;gt; ~/.bashrc
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt; ~/.bashrc
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;: &lt;strong&gt;安装Conda环境管理器&lt;/strong&gt;，用于后续创建和管理Python虚拟环境。
&lt;strong&gt;结果&lt;/strong&gt;: &lt;strong&gt;Miniconda&lt;/strong&gt;成功安装并配置好环境变量。&lt;/p&gt;
&lt;p&gt;(02:04-02:14)
&lt;em&gt;&lt;strong&gt;1.3: 创建并激活Conda虚拟环境&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;关键操作&lt;/strong&gt;: 使用&lt;code&gt;conda create&lt;/code&gt;指令创建一个名为&lt;code&gt;lmdeploy&lt;/code&gt;的&lt;strong&gt;Python 3.11&lt;/strong&gt;虚拟环境，并通过&lt;code&gt;conda activate&lt;/code&gt;指令激活该环境。
&lt;strong&gt;画面内容&lt;/strong&gt;: &lt;code&gt;conda create -n lmdeploy python=3.11 -y &amp;amp;&amp;amp; conda activate lmdeploy&lt;/code&gt;
&lt;strong&gt;目的&lt;/strong&gt;: 为模型部署创建一个&lt;strong&gt;隔离的Python环境&lt;/strong&gt;，避免依赖冲突。
&lt;strong&gt;结果&lt;/strong&gt;: 名为&lt;code&gt;lmdeploy&lt;/code&gt;的Conda环境被成功创建并激活，终端提示符前缀变为&lt;code&gt;(lmdeploy)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;(02:14-02:24)
&lt;em&gt;&lt;strong&gt;1.4: 安装LMDeploy&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;作者提到的关键信息&lt;/strong&gt;: 本次部署将使用&lt;strong&gt;LMDeploy&lt;/strong&gt;工具包。
&lt;strong&gt;关键操作&lt;/strong&gt;: 在已激活的&lt;code&gt;lmdeploy&lt;/code&gt;环境中使用&lt;code&gt;pip install&lt;/code&gt;指令安装&lt;strong&gt;LMDeploy&lt;/strong&gt;。
&lt;strong&gt;画面内容&lt;/strong&gt;: 命令行执行 &lt;code&gt;pip install lmdeploy&lt;/code&gt; (演示脚本中为 &lt;code&gt;pip install lmdeploy&amp;gt;=0.7.3&lt;/code&gt;)。
&lt;strong&gt;目的&lt;/strong&gt;: 安装核心的模型部署工具&lt;strong&gt;LMDeploy&lt;/strong&gt;。
&lt;strong&gt;结果&lt;/strong&gt;: &lt;strong&gt;LMDeploy&lt;/strong&gt;及其相关依赖包被成功下载并安装到&lt;code&gt;lmdeploy&lt;/code&gt;环境中。&lt;/p&gt;
&lt;p&gt;(02:25-02:30)
&lt;em&gt;&lt;strong&gt;1.5: 安装额外依赖包&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;关键操作&lt;/strong&gt;: 使用&lt;code&gt;pip install&lt;/code&gt;指令安装&lt;code&gt;partial_json_parser&lt;/code&gt;和&lt;code&gt;timm&lt;/code&gt;这两个库。
&lt;strong&gt;画面内容&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install partial_json_parser
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install timm
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;目的&lt;/strong&gt;: 安装运行&lt;strong&gt;InternVL3&lt;/strong&gt;模型或&lt;strong&gt;LMDeploy&lt;/strong&gt;可能需要的&lt;strong&gt;额外Python库&lt;/strong&gt;。
&lt;strong&gt;结果&lt;/strong&gt;: &lt;code&gt;partial_json_parser&lt;/code&gt;和&lt;code&gt;timm&lt;/code&gt;被成功安装。&lt;/p&gt;
&lt;h1 id=&#34;part2-使用lmdeploy部署模型api服务&#34;&gt;part2: 使用LMDeploy部署模型API服务
&lt;/h1&gt;&lt;p&gt;(02:30-03:05)
&lt;em&gt;&lt;strong&gt;2.1: ♐启动模型API服务器&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;关键操作&lt;/strong&gt;: 执行&lt;code&gt;lmdeploy serve api_server&lt;/code&gt;指令来启动模型服务。
&lt;strong&gt;画面内容&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;lmdeploy serve api_server OpenGVLab/InternVL3-14B-Instruct --backend turbomind --server-port &lt;span class=&#34;m&#34;&gt;23333&lt;/span&gt; --tp &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; --chat-template internvl2_5
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;作者提到的关键信息&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指令指定了要部署的模型为 &lt;code&gt;OpenGVLab/InternVL3-14B-Instruct&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;使用了&lt;code&gt;turbomind&lt;/code&gt;作为推理后端 (&lt;code&gt;--backend turbomind&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;指定服务器端口为&lt;code&gt;23333&lt;/code&gt; (&lt;code&gt;--server-port 23333&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;设置&lt;strong&gt;张量并行 (Tensor Parallelism)&lt;/strong&gt; 为2 (&lt;code&gt;--tp 2&lt;/code&gt;)，表示使用2块GPU进行推理。&lt;/li&gt;
&lt;li&gt;指定了聊天模板为&lt;code&gt;internvl2_5&lt;/code&gt; (&lt;code&gt;--chat-template internvl2_5&lt;/code&gt;)。
&lt;strong&gt;作者建议&lt;/strong&gt;: 如果只有一块GPU，应将&lt;code&gt;--tp&lt;/code&gt;参数设置为&lt;code&gt;1&lt;/code&gt;。
&lt;strong&gt;目的&lt;/strong&gt;: 将&lt;strong&gt;InternVL3-14B-Instruct&lt;/strong&gt;模型加载到GPU，并通过&lt;strong&gt;LMDeploy&lt;/strong&gt;启动一个&lt;strong&gt;兼容OpenAI API&lt;/strong&gt;的HTTP服务，以便后续应用调用。
&lt;strong&gt;结果&lt;/strong&gt;: &lt;strong&gt;LMDeploy&lt;/strong&gt;开始下载模型权重文件（&lt;code&gt;.safetensors&lt;/code&gt;），下载完成后加载模型并启动&lt;strong&gt;Uvicorn Web服务器&lt;/strong&gt;。服务器成功运行在&lt;code&gt;http://0.0.0.0:23333&lt;/code&gt;，并提示可以在浏览器中打开该地址查看API详情（Swagger UI）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;part3-安装并配置openwebui&#34;&gt;part3: 安装并配置OpenWebUI
&lt;/h1&gt;&lt;p&gt;(03:05-03:32)
&lt;em&gt;&lt;strong&gt;3.1: 安装并启动OpenWebUI&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;作者提到的关键信息&lt;/strong&gt;: 使用&lt;strong&gt;OpenWebUI&lt;/strong&gt;作为与模型API交互的前端界面。
&lt;strong&gt;关键操作&lt;/strong&gt;: 使用&lt;code&gt;pip install open-webui&lt;/code&gt;指令安装&lt;strong&gt;OpenWebUI&lt;/strong&gt;。
&lt;strong&gt;画面内容&lt;/strong&gt;: &lt;code&gt;pip install open-webui&lt;/code&gt;
&lt;strong&gt;目的&lt;/strong&gt;: 安装Web用户界面。
&lt;strong&gt;结果&lt;/strong&gt;: &lt;strong&gt;OpenWebUI&lt;/strong&gt;安装完成。
&lt;strong&gt;关键操作&lt;/strong&gt;: 使用&lt;code&gt;open-webui serve&lt;/code&gt;指令启动&lt;strong&gt;OpenWebUI&lt;/strong&gt;服务。
&lt;strong&gt;画面内容&lt;/strong&gt;: &lt;code&gt;open-webui serve&lt;/code&gt;
&lt;strong&gt;目的&lt;/strong&gt;: 运行Web界面服务器。
&lt;strong&gt;结果&lt;/strong&gt;: &lt;strong&gt;OpenWebUI&lt;/strong&gt;服务启动，可以通过浏览器访问&lt;code&gt;localhost:8080&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;(03:32-04:06)
&lt;em&gt;&lt;strong&gt;3.2: 配置OpenWebUI连接模型API&lt;/strong&gt;&lt;/em&gt;
&lt;strong&gt;关键操作&lt;/strong&gt;: 在浏览器中打开&lt;code&gt;localhost:8080&lt;/code&gt;访问&lt;strong&gt;OpenWebUI&lt;/strong&gt;。
&lt;strong&gt;关键操作&lt;/strong&gt;: 点击左下角头像 -&amp;gt; 设置 -&amp;gt; 管理员设置 -&amp;gt; 外部连接。
&lt;strong&gt;关键操作&lt;/strong&gt;: 确认“&lt;strong&gt;OpenAI API&lt;/strong&gt;”开关已打开。
&lt;strong&gt;关键操作&lt;/strong&gt;: 在“管理OpenAI API连接”的输入框中，填入由&lt;strong&gt;LMDeploy&lt;/strong&gt;启动的API服务器地址。
&lt;strong&gt;作者提到的关键信息&lt;/strong&gt;: API地址应为&lt;code&gt;http://[部署服务器的IP地址]:23333/v1&lt;/code&gt;。视频中示例为&lt;code&gt;http://192.168.1.105:23333/v1&lt;/code&gt;。
&lt;strong&gt;关键操作&lt;/strong&gt;: 点击右下角的“保存”按钮。
&lt;strong&gt;关键操作&lt;/strong&gt;: 返回主界面，点击左上角“新建对话”。
&lt;strong&gt;关键操作&lt;/strong&gt;: 在模型选择下拉菜单中，选择刚刚部署的模型 &lt;code&gt;OpenGVLab/InternVL3-14B-Instruct&lt;/code&gt;。
&lt;strong&gt;目的&lt;/strong&gt;: 将&lt;strong&gt;OpenWebUI&lt;/strong&gt;前端指向本地运行的&lt;strong&gt;InternVL3模型API&lt;/strong&gt;，使其能够调用该模型进行交互。
&lt;strong&gt;结果&lt;/strong&gt;: &lt;strong&gt;OpenWebUI&lt;/strong&gt;成功连接到本地部署的模型API，可以在聊天界面中选择并使用&lt;code&gt;OpenGVLab/InternVL3-14B-Instruct&lt;/code&gt;模型。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FastAPI:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定位:&lt;/strong&gt; FastAPI 是一个&lt;strong&gt;通用的、高性能的Python Web框架&lt;/strong&gt;，用于构建API（应用程序接口）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用:&lt;/strong&gt; 它的核心功能是&lt;strong&gt;创建Web服务和API端点&lt;/strong&gt;。在模型部署的场景中，你可以使用FastAPI来编写代码，定义模型的输入/输出接口，接收用户请求，调用模型进行推理，然后将结果返回给用户。♈它本身&lt;strong&gt;不包含&lt;/strong&gt;模型加载、优化或推理执行的功能，♐你需要自己集成这些逻辑（例如使用&lt;code&gt;transformers&lt;/code&gt;库加载模型，用PyTorch/TensorFlow进行推理）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类比:&lt;/strong&gt; 它更像是建造房屋的**“脚手架”或“工具箱”**，提供了构建API的基础结构，但具体的模型推理逻辑需要你自己实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ollama:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定位:&lt;/strong&gt; Ollama 是一个&lt;strong&gt;简化在本地运行大型语言模型（LLM）的工具&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用:&lt;/strong&gt; 它&lt;strong&gt;打包了&lt;/strong&gt;运行多种开源LLM所需的环境、模型权重下载和管理，并&lt;strong&gt;内置了一个API服务器&lt;/strong&gt;（通常兼容OpenAI API格式）。用户可以通过简单的命令 (&lt;code&gt;ollama run model_name&lt;/code&gt;) 快速启动并与模型交互，或者通过其提供的API进行调用。它的&lt;strong&gt;重点在于易用性和本地快速体验&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类比:&lt;/strong&gt; 它更像是一个**“开箱即用的LLM运行器”**，帮你处理了很多底层的配置和模型管理，让你能快速用上各种模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;LMDeploy:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;定位:&lt;/strong&gt; LMDeploy 是一个专注于&lt;strong&gt;大型语言模型（LLM）的高效部署工具包&lt;/strong&gt;，由InternLM团队开发。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;作用:&lt;/strong&gt; 它提供了&lt;strong&gt;模型转换、推理引擎（如TurboMind）优化、量化&lt;/strong&gt;以及&lt;strong&gt;API服务部署&lt;/strong&gt;等一系列功能。它的&lt;strong&gt;核心优势在于推理性能优化&lt;/strong&gt;，可以显著提升LLM的推理速度并降低显存占用。它也提供了一个&lt;strong&gt;开箱即用的API服务器&lt;/strong&gt;（如视频中展示的&lt;code&gt;lmdeploy serve api_server&lt;/code&gt;），并且这个服务器也&lt;strong&gt;兼容OpenAI API格式&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类比:&lt;/strong&gt; 它更像是一个**“高性能LLM部署与服务引擎”**，不仅帮你部署模型并提供API，还着重于优化模型的运行效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;总结与比较：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;相似之处:&lt;/strong&gt; LMDeploy 和 Ollama 都提供了&lt;strong&gt;相对完整的、用于运行LLM并提供API服务&lt;/strong&gt;的功能，都致力于简化模型部署的过程，并且都提供了兼容OpenAI的API接口，方便与其他应用集成。从这个角度看，LMDeploy和Ollama更像是同类工具，都是&lt;strong&gt;LLM Serving（服务）框架/工具&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;不同之处:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LMDeploy vs Ollama:&lt;/strong&gt; LMDeploy 更侧重于&lt;strong&gt;推理性能优化&lt;/strong&gt;（特别是对于InternLM系列模型，利用TurboMind等引擎），提供更细致的部署配置选项（如张量并行）；而Ollama更侧重于&lt;strong&gt;跨模型兼容性和本地运行的便捷性&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LMDeploy/Ollama vs FastAPI:&lt;/strong&gt; FastAPI 是一个&lt;strong&gt;更底层的Web框架&lt;/strong&gt;，用于构建各种API，包括模型API。而LMDeploy和Ollama是&lt;strong&gt;更高层、更专注于LLM部署的解决方案&lt;/strong&gt;，它们内部可能使用了类似FastAPI/Uvicorn这样的Web框架来构建其API服务部分，但它们还封装了模型加载、优化、推理和管理等更多功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;简单来说：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;♈如果你想&lt;strong&gt;快速在本地跑各种开源LLM&lt;/strong&gt;，并且不太关心极致的性能优化，&lt;strong&gt;Ollama&lt;/strong&gt; 是个好选择。&lt;/li&gt;
&lt;li&gt;♈如果你想&lt;strong&gt;高效地部署特定（尤其是InternLM系列）的LLM&lt;/strong&gt;，追求&lt;strong&gt;推理速度和低资源占用&lt;/strong&gt;，并需要一个兼容OpenAI的API服务，&lt;strong&gt;LMDeploy&lt;/strong&gt; 是一个强大的工具。&lt;/li&gt;
&lt;li&gt;如果你想&lt;strong&gt;从头开始构建一个定制化的模型API服务&lt;/strong&gt;，需要完全控制API的逻辑和行为，或者部署非LLM模型，那么你会&lt;strong&gt;使用FastAPI&lt;/strong&gt;这样的Web框架作为基础。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，LMDeploy 和 Ollama 可以被认为是同类（LLM Serving工具），而 FastAPI 是构建这类工具（或其他Web API）的基础框架。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>用VLLM本地linux系统部署Qwen3</title>
        <link>https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8vllm%E6%9C%AC%E5%9C%B0linux%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2qwen3/</link>
        <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://yichixing.github.io/hugo-stack-dev/p/%E7%94%A8vllm%E6%9C%AC%E5%9C%B0linux%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2qwen3/</guid>
        <description>&lt;p&gt;༼ 企业级最强开源大模型Qwen3震撼发布！本地部署+全面客观测评！Qwen3-235B-A22B+Qwen3-32B+Qwen3-14B谁是最强王者？ollama+LM Studio+vLLM本地部署I༽&lt;/p&gt;
&lt;h1 id=&#34;part-1-准备vllm部署环境&#34;&gt;Part 1: 准备vLLM部署环境
&lt;/h1&gt;&lt;p&gt;(01:57-02:06) &lt;em&gt;&lt;strong&gt;1.1: 介绍vLLM及部署平台&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作者提到的关键信息&lt;/strong&gt;：如果需要进行&lt;strong&gt;企业项目&lt;/strong&gt;部署，可以使用&lt;strong&gt;vLLM&lt;/strong&gt;。作者将在&lt;strong&gt;Ubuntu系统&lt;/strong&gt;上演示使用&lt;strong&gt;vLLM&lt;/strong&gt;进行部署。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(02:06-02:10) &lt;em&gt;&lt;strong&gt;1.2: 展示部署环境&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;关键操作&lt;/strong&gt;：作者执行&lt;code&gt;nvidia-smi&lt;/code&gt;命令查看服务器的显卡信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：确认部署环境的硬件配置。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;画面内容&lt;/strong&gt;：显示服务器使用的是&lt;strong&gt;NVIDIA RTX A6000&lt;/strong&gt;显卡，显存为&lt;strong&gt;48G&lt;/strong&gt;，CUDA版本为12.7。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(02:10-02:19) &lt;em&gt;&lt;strong&gt;1.3: 安装Miniconda&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：安装&lt;strong&gt;Miniconda&lt;/strong&gt;用于管理Python环境和依赖包。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键操作&lt;/strong&gt;：作者展示并执行了一系列命令来下载、安装&lt;strong&gt;Miniconda&lt;/strong&gt;，并配置相关的环境变量。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;画面内容&lt;/strong&gt;:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 安装Miniconda (如果尚未安装)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bash ~/miniconda.sh -b -p &lt;span class=&#34;nv&#34;&gt;$HOME&lt;/span&gt;/miniconda
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;eval&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOME&lt;/span&gt;/miniconda/bin/conda shell.bash hook&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;export PATH=&amp;#34;$HOME/miniconda/bin:$PATH&amp;#34;&amp;#39;&lt;/span&gt; &amp;gt;&amp;gt; ~/.bashrc
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;source&lt;/span&gt; ~/.bashrc
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果&lt;/strong&gt;：&lt;strong&gt;Miniconda&lt;/strong&gt;环境安装并配置完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(02:19-02:29) &lt;em&gt;&lt;strong&gt;1.4: 创建并激活Conda环境&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：创建一个独立的Python环境（&lt;code&gt;qwen3_env&lt;/code&gt;）用于&lt;strong&gt;vLLM&lt;/strong&gt;部署，指定&lt;strong&gt;Python版本为3.10&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键操作&lt;/strong&gt;：执行&lt;code&gt;conda create&lt;/code&gt;命令创建环境，然后执行&lt;code&gt;conda activate&lt;/code&gt;命令激活该环境。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;画面内容&lt;/strong&gt;:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 创建conda环境&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda create -n qwen3_env &lt;span class=&#34;nv&#34;&gt;python&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;3.10 -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conda activate qwen3_env
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果&lt;/strong&gt;：成功创建并进入了名为&lt;code&gt;qwen3_env&lt;/code&gt;的&lt;strong&gt;Conda&lt;/strong&gt;环境。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(02:29-02:35) &lt;em&gt;&lt;strong&gt;1.5: 安装vLLM及依赖&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：安装&lt;strong&gt;vLLM&lt;/strong&gt;库及其运行所需的依赖库（&lt;strong&gt;ray&lt;/strong&gt;, &lt;strong&gt;transformers&lt;/strong&gt;, &lt;strong&gt;accelerate&lt;/strong&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键操作&lt;/strong&gt;：在已激活的&lt;strong&gt;Conda&lt;/strong&gt;环境（&lt;code&gt;qwen3_env&lt;/code&gt;）中，使用&lt;code&gt;pip install&lt;/code&gt;命令安装&lt;strong&gt;vLLM&lt;/strong&gt;及相关依赖。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;画面内容&lt;/strong&gt;:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install vllm ray transformers accelerate
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果&lt;/strong&gt;：&lt;strong&gt;vLLM&lt;/strong&gt;及其依赖库安装完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;part-2-使用vllm启动qwen3-14b模型服务&#34;&gt;Part 2: 使用vLLM启动Qwen3 14B模型服务
&lt;/h1&gt;&lt;p&gt;(02:35-02:46) &lt;em&gt;&lt;strong&gt;2.1: 启动vLLM服务&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：使用&lt;strong&gt;vLLM&lt;/strong&gt;将&lt;strong&gt;Qwen3 14B&lt;/strong&gt;模型加载并启动为一个API服务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键操作&lt;/strong&gt;：执行&lt;code&gt;vllm serve&lt;/code&gt;命令，指定模型为&lt;strong&gt;Qwen/Qwen3-14B&lt;/strong&gt;，并添加了参数&lt;code&gt;--enable-reasoning&lt;/code&gt;和&lt;code&gt;--reasoning-parser deepseek_r1&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;画面内容&lt;/strong&gt;:
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vllm serve Qwen/Qwen3-14B --enable-reasoning --reasoning-parser deepseek_r1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果&lt;/strong&gt;：&lt;strong&gt;vLLM&lt;/strong&gt;服务成功启动，API服务器监听在&lt;code&gt;http://0.0.0.0:8000&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;part-3-通过open-webui调用vllm模型&#34;&gt;Part 3: 通过Open WebUI调用vLLM模型
&lt;/h1&gt;&lt;p&gt;(02:46-03:38) &lt;em&gt;&lt;strong&gt;3.1: 安装、启动并配置Open WebUI连接vLLM&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：配置一个Web UI（&lt;strong&gt;Open WebUI&lt;/strong&gt;）来方便地与通过&lt;strong&gt;vLLM&lt;/strong&gt;部署的&lt;strong&gt;Qwen3 14B&lt;/strong&gt;模型进行交互。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键操作&lt;/strong&gt;:
&lt;ol&gt;
&lt;li&gt;在本地电脑终端安装&lt;strong&gt;Open WebUI&lt;/strong&gt;：&lt;code&gt;pip install open-webui&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;启动&lt;strong&gt;Open WebUI&lt;/strong&gt;服务：&lt;code&gt;open-webui serve&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;在浏览器访问&lt;code&gt;http://localhost:8080&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;进入&lt;strong&gt;Open WebUI&lt;/strong&gt;设置 -&amp;gt; 管理员设置 -&amp;gt; 外部连接。&lt;/li&gt;
&lt;li&gt;在&lt;strong&gt;OpenAI API连接&lt;/strong&gt;设置中，将&lt;strong&gt;API请求地址&lt;/strong&gt;设置为&lt;strong&gt;vLLM&lt;/strong&gt;服务器的&lt;strong&gt;IP地址+端口号+/v1&lt;/strong&gt;（例如：&lt;code&gt;http://192.168.1.105:8000/v1&lt;/code&gt;）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;API Key&lt;/strong&gt;处随意输入几个字母。&lt;/li&gt;
&lt;li&gt;点击保存。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果&lt;/strong&gt;：&lt;strong&gt;Open WebUI&lt;/strong&gt;配置完成，可以连接到&lt;strong&gt;vLLM&lt;/strong&gt;服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(03:38-03:52) &lt;em&gt;&lt;strong&gt;3.2: 在Open WebUI中选择并使用模型&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：通过配置好的&lt;strong&gt;Open WebUI&lt;/strong&gt;界面与&lt;strong&gt;Qwen3 14B&lt;/strong&gt;模型进行对话测试。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键操作&lt;/strong&gt;：在&lt;strong&gt;Open WebUI&lt;/strong&gt;中，点击&lt;strong&gt;新建对话&lt;/strong&gt;，然后在模型选择列表中选择刚配置好的&lt;strong&gt;Qwen: Qwen3 14B&lt;/strong&gt;模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果&lt;/strong&gt;：用户现在可以通过&lt;strong&gt;Open WebUI&lt;/strong&gt;与本地通过&lt;strong&gt;vLLM&lt;/strong&gt;部署的&lt;strong&gt;Qwen3 14B&lt;/strong&gt;模型进行交互。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;part-4-补充信息&#34;&gt;Part 4: 补充信息
&lt;/h1&gt;&lt;p&gt;(15:39-15:43) &lt;em&gt;&lt;strong&gt;4.1: 获取代码和指令&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作者建议&lt;/strong&gt;：视频中使用的代码和指令会放在视频下方的描述栏或者评论区。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(15:43-15:49) &lt;em&gt;&lt;strong&gt;4.2: 通过博客获取笔记&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;作者建议&lt;/strong&gt;：如果在视频下方无法找到代码和指令，可以访问作者的博客（&lt;strong&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.aivi.fyi&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;www.aivi.fyi&lt;/a&gt;&lt;/strong&gt;）查找本期视频对应的笔记。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
